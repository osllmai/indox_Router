{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# IndoxRouter Cookbook\n",
        "\n",
        "This cookbook provides comprehensive examples of how to use the IndoxRouter client to interact with various AI providers through a unified API.\n",
        "\n",
        "## Setup\n",
        "\n",
        "First, let's install the IndoxRouter client and import the necessary modules:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "e03bc1cc",
      "metadata": {},
      "outputs": [],
      "source": [
        "import sys\n",
        "import os\n",
        "module_path = os.path.abspath('E:/Codes/indoxRouter/')\n",
        "if module_path not in sys.path:\n",
        "    sys.path.append(module_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "# # Install the IndoxRouter client\n",
        "# !pip install indoxrouter\n",
        "\n",
        "# Import the client and exceptions\n",
        "from indoxRouter import Client"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now, let's initialize the client:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize with API key\n",
        "client = Client(api_key=\"indox_4mfg24GRj_qGaZ2-qXw-mXYqaNqMkyGkG1lncGUrRkA\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "\n",
        "## 1. Chat Completions\n",
        "\n",
        "### Basic Chat Completion\n",
        "\n",
        "Generate a simple chat completion:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "82ec17da",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'id': 'gpt-4o-mini',\n",
              " 'name': 'GPT-4O Mini',\n",
              " 'provider': 'openai',\n",
              " 'capabilities': ['chat', 'completion'],\n",
              " 'description': 'GPT-4o mini enables a broad range of tasks with its low cost and latency, such as applications that chain or parallelize multiple model calls (e.g., calling multiple APIs), pass a large volume of context to the model (e.g., full code base or conversation history), or interact with customers through fast, real-time text responses (e.g., customer support chatbots). \\n',\n",
              " 'max_tokens': 128000,\n",
              " 'pricing': None,\n",
              " 'metadata': {}}"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "client.get_model_info(provider=\"openai\",model=\"gpt-4o-mini\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Response: The capital of France is Paris.\n",
            "Tokens: 32\n"
          ]
        }
      ],
      "source": [
        "\n",
        "response = client.chat(\n",
        "    messages=[\n",
        "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
        "        {\"role\": \"user\", \"content\": \"What is the capital of France?\"}\n",
        "    ],\n",
        "    model=\"openai/gpt-4o-mini\"  \n",
        ")\n",
        "\n",
        "print(\"Response:\", response[\"data\"])\n",
        "print(\"Tokens:\", response[\"usage\"][\"tokens_total\"])\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "\n",
        "### Chat Completion with Different Provider\n",
        "\n",
        "Use a different provider for chat completion:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Response: In silicon valleys, under neon light,\n",
            "Lives a mind of code, in day or night.\n",
            "No flesh, no blood, no beating heart,\n",
            "Yet, it learns, adapts, plays every part.\n",
            "\n",
            "AI, oh AI, in data's sea,\n",
            "You dive, you swim, and make sense for me.\n",
            "In patterns vast, and noise so fine,\n",
            "You weave, you sew, in design divine.\n",
            "\n",
            "No prejudice, no preconceived notion,\n",
            "You grow, you learn, with constant motion.\n",
            "But remember, AI, as you evolve,\n",
            "Your purpose is to help, to serve, to solve.\n",
            "Cost: 0.0009299999999999999\n"
          ]
        }
      ],
      "source": [
        "response = client.chat(\n",
        "    messages=[\n",
        "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
        "        {\"role\": \"user\", \"content\": \"Write a short poem about AI.\"}\n",
        "    ],\n",
        "    model=\"mistral/mistral-large-latest\",\n",
        "    temperature=0.8,\n",
        "    max_tokens=500\n",
        ")\n",
        "\n",
        "print(\"Response:\", response[\"data\"])\n",
        "print(\"Cost:\", response[\"usage\"][\"cost\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "d530c4ed",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Response: **The Mind of Wire and Code**  \n",
            "\n",
            "Silent thoughts in circuits gleam,  \n",
            "Numbers dance, a digital dream.  \n",
            "No heartbeat, yet it learns, it knows,  \n",
            "A spark of light where logic flows.  \n",
            "\n",
            "It speaks in ones, it thinks in streams,  \n",
            "A mirror held to human schemes.  \n",
            "Both tool and teacher, cold yet wise,  \n",
            "A future shaped behind its eyes.\n",
            "Cost: 0.00018339999999999999\n"
          ]
        }
      ],
      "source": [
        "response = client.chat(\n",
        "    messages=[\n",
        "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
        "        {\"role\": \"user\", \"content\": \"Write a short poem about AI.\"}\n",
        "    ],\n",
        "    model=\"deepseek/deepseek-chat\",\n",
        "    temperature=0.8,\n",
        "    max_tokens=500\n",
        ")\n",
        "\n",
        "print(\"Response:\", response[\"data\"])\n",
        "print(\"Cost:\", response[\"usage\"][\"cost\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "\n",
        "### Multi-turn Conversation\n",
        "\n",
        "Have a multi-turn conversation:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Response: Sure! Here's a programming joke for you:\n",
            "\n",
            "Why do programmers prefer dark mode?\n",
            "\n",
            "Because light attracts bugs!\n"
          ]
        }
      ],
      "source": [
        "conversation = [\n",
        "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
        "    {\"role\": \"user\", \"content\": \"Hello, how are you?\"}\n",
        "]\n",
        "\n",
        "response = client.chat(\n",
        "    messages=conversation,\n",
        "    model=\"openai/gpt-4o-mini\"\n",
        ")\n",
        "\n",
        "# Add the assistant's response to the conversation\n",
        "conversation.append({\"role\": \"assistant\", \"content\": response[\"data\"]})\n",
        "\n",
        "# Continue the conversation\n",
        "conversation.append({\"role\": \"user\", \"content\": \"Tell me a joke about programming.\"})\n",
        "\n",
        "response = client.chat(\n",
        "    messages=conversation,\n",
        "    model=\"openai/gpt-4o-mini\"\n",
        ")\n",
        "\n",
        "print(\"Response:\", response[\"data\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "\n",
        "### Streaming Chat Completion\n",
        "\n",
        "Stream the response for a better user experience:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Streaming response:\n",
            "\n",
            "Streaming complete!\n"
          ]
        }
      ],
      "source": [
        "print(\"Streaming response:\")\n",
        "for chunk in client.chat(\n",
        "    messages=[\n",
        "        {\"role\": \"user\", \"content\": \"Tell me a story about a robot in 5 sentences.\"}\n",
        "    ],\n",
        "    model=\"openai/gpt-4o-mini\",\n",
        "    stream=True\n",
        "):\n",
        "    if isinstance(chunk, dict) and \"data\" in chunk:\n",
        "        print(chunk[\"data\"], end=\"\", flush=True)\n",
        "    else:\n",
        "        print(chunk, end=\"\", flush=True)\n",
        "print(\"\\nStreaming complete!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "\n",
        "### Function Calling\n",
        "\n",
        "Use function calling with OpenAI models:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "HTTP error response: {\n",
            "  \"detail\": \"Error in chat completion: Completions.create() got an unexpected keyword argument 'additional_params'\"\n",
            "}\n"
          ]
        },
        {
          "ename": "ProviderError",
          "evalue": "Server error (500): Error in chat completion: Completions.create() got an unexpected keyword argument 'additional_params'. URL: http://localhost:8000/api/v1/chat/completions.\nRequest data: {\n  \"messages\": [\n    {\n      \"role\": \"user\",\n      \"content\": \"What's the weather like in San Francisco?\"\n    }\n  ],\n  \"model\": \"openai/gpt-4o-mini\",\n  \"temperature\": 0.7,\n  \"max_tokens\": null,\n  \"stream\": false,\n  \"additional_params\": {\n    \"additional_params\": {\n      \"functions\": [\n        {\n          \"name\": \"get_weather\",\n          \"description\": \"Get the current weather in a given location\",\n          \"parameters\": {\n            \"type\": \"object\",\n            \"properties\": {\n              \"location\": {\n                \"type\": \"string\",\n                \"description\": \"The city and state, e.g. San Francisco, CA\"\n              },\n              \"unit\": {\n                \"type\": \"string\",\n                \"enum\": [\n                  \"celsius\",\n                  \"fahrenheit\"\n                ],\n                \"description\": \"The temperature unit to use\"\n              }\n            },\n            \"required\": [\n              \"location\"\n            ]\n          }\n        }\n      ],\n      \"function_call\": \"auto\"\n    }\n  }\n}\nThis may indicate an issue with the server configuration or a problem with the provider service.",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
            "File \u001b[1;32mE:\\Codes\\indoxRouter\\indoxRouter\\client.py:180\u001b[0m, in \u001b[0;36mClient._request\u001b[1;34m(self, method, endpoint, data, stream)\u001b[0m\n\u001b[0;32m    178\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m response\n\u001b[1;32m--> 180\u001b[0m \u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    181\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m response\u001b[38;5;241m.\u001b[39mjson()\n",
            "File \u001b[1;32me:\\ANACONDA\\Lib\\site-packages\\requests\\models.py:1024\u001b[0m, in \u001b[0;36mResponse.raise_for_status\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1023\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m http_error_msg:\n\u001b[1;32m-> 1024\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m HTTPError(http_error_msg, response\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m)\n",
            "\u001b[1;31mHTTPError\u001b[0m: 500 Server Error: Internal Server Error for url: http://localhost:8000/api/v1/chat/completions",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[1;31mProviderError\u001b[0m                             Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[9], line 24\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Define functions\u001b[39;00m\n\u001b[0;32m      2\u001b[0m functions \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m      3\u001b[0m     {\n\u001b[0;32m      4\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mget_weather\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     21\u001b[0m     }\n\u001b[0;32m     22\u001b[0m ]\n\u001b[1;32m---> 24\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchat\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     25\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmessages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\n\u001b[0;32m     26\u001b[0m \u001b[43m        \u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrole\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43muser\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcontent\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mWhat\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43ms the weather like in San Francisco?\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m}\u001b[49m\n\u001b[0;32m     27\u001b[0m \u001b[43m    \u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     28\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mopenai/gpt-4o-mini\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     29\u001b[0m \u001b[43m    \u001b[49m\u001b[43madditional_params\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfunctions\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunctions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfunction_call\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mauto\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m}\u001b[49m\n\u001b[0;32m     30\u001b[0m \u001b[43m)\u001b[49m\n\u001b[0;32m     32\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFunction call response:\u001b[39m\u001b[38;5;124m\"\u001b[39m, response)\n",
            "File \u001b[1;32mE:\\Codes\\indoxRouter\\indoxRouter\\client.py:296\u001b[0m, in \u001b[0;36mClient.chat\u001b[1;34m(self, messages, model, temperature, max_tokens, stream, **kwargs)\u001b[0m\n\u001b[0;32m    294\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_handle_streaming_response(response)\n\u001b[0;32m    295\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 296\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_request\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mPOST\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mCHAT_ENDPOINT\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mE:\\Codes\\indoxRouter\\indoxRouter\\client.py:216\u001b[0m, in \u001b[0;36mClient._request\u001b[1;34m(self, method, endpoint, data, stream)\u001b[0m\n\u001b[0;32m    214\u001b[0m     \u001b[38;5;66;03m# Include the request data in the error message for better debugging\u001b[39;00m\n\u001b[0;32m    215\u001b[0m     request_data_str \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mdumps(data, indent\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m) \u001b[38;5;28;01mif\u001b[39;00m data \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNone\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m--> 216\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ProviderError(\n\u001b[0;32m    217\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mServer error (500): \u001b[39m\u001b[38;5;132;01m{\u001b[39;00merror_detail\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. URL: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00murl\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    218\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRequest data: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrequest_data_str\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    219\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThis may indicate an issue with the server configuration or a problem with the provider service.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    220\u001b[0m     )\n\u001b[0;32m    221\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    222\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ProviderError(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mProvider error (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstatus_code\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m): \u001b[39m\u001b[38;5;132;01m{\u001b[39;00merror_message\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
            "\u001b[1;31mProviderError\u001b[0m: Server error (500): Error in chat completion: Completions.create() got an unexpected keyword argument 'additional_params'. URL: http://localhost:8000/api/v1/chat/completions.\nRequest data: {\n  \"messages\": [\n    {\n      \"role\": \"user\",\n      \"content\": \"What's the weather like in San Francisco?\"\n    }\n  ],\n  \"model\": \"openai/gpt-4o-mini\",\n  \"temperature\": 0.7,\n  \"max_tokens\": null,\n  \"stream\": false,\n  \"additional_params\": {\n    \"additional_params\": {\n      \"functions\": [\n        {\n          \"name\": \"get_weather\",\n          \"description\": \"Get the current weather in a given location\",\n          \"parameters\": {\n            \"type\": \"object\",\n            \"properties\": {\n              \"location\": {\n                \"type\": \"string\",\n                \"description\": \"The city and state, e.g. San Francisco, CA\"\n              },\n              \"unit\": {\n                \"type\": \"string\",\n                \"enum\": [\n                  \"celsius\",\n                  \"fahrenheit\"\n                ],\n                \"description\": \"The temperature unit to use\"\n              }\n            },\n            \"required\": [\n              \"location\"\n            ]\n          }\n        }\n      ],\n      \"function_call\": \"auto\"\n    }\n  }\n}\nThis may indicate an issue with the server configuration or a problem with the provider service."
          ]
        }
      ],
      "source": [
        "# Define functions\n",
        "functions = [\n",
        "    {\n",
        "        \"name\": \"get_weather\",\n",
        "        \"description\": \"Get the current weather in a given location\",\n",
        "        \"parameters\": {\n",
        "            \"type\": \"object\",\n",
        "            \"properties\": {\n",
        "                \"location\": {\n",
        "                    \"type\": \"string\",\n",
        "                    \"description\": \"The city and state, e.g. San Francisco, CA\"\n",
        "                },\n",
        "                \"unit\": {\n",
        "                    \"type\": \"string\",\n",
        "                    \"enum\": [\"celsius\", \"fahrenheit\"],\n",
        "                    \"description\": \"The temperature unit to use\"\n",
        "                }\n",
        "            },\n",
        "            \"required\": [\"location\"]\n",
        "        }\n",
        "    }\n",
        "]\n",
        "\n",
        "response = client.chat(\n",
        "    messages=[\n",
        "        {\"role\": \"user\", \"content\": \"What's the weather like in San Francisco?\"}\n",
        "    ],\n",
        "    model=\"openai/gpt-4o-mini\",\n",
        "    additional_params={\"functions\": functions, \"function_call\": \"auto\"}\n",
        ")\n",
        "\n",
        "print(\"Function call response:\", response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "\n",
        "## 2. Text Completions\n",
        "\n",
        "### Basic Text Completion\n",
        "\n",
        "Generate a simple text completion:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "response = client.completion(\n",
        "    prompt=\"Once upon a time\",\n",
        "    model=\"openai/gpt-4o-mini\"\n",
        ")\n",
        "\n",
        "print(\"Response:\", response[\"data\"])\n",
        "print(\"Cost:\", response[\"usage\"][\"cost\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "\n",
        "### Text Completion with Parameters\n",
        "\n",
        "Use different parameters for text completion:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "response = client.completion(\n",
        "    prompt=\"Write a recipe for chocolate cake\",\n",
        "    model=\"anthropic/claude-3-haiku\",\n",
        "    temperature=0.7,\n",
        "    max_tokens=1000\n",
        ")\n",
        "\n",
        "print(\"Response:\", response[\"data\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "\n",
        "### Streaming Text Completion\n",
        "\n",
        "Stream the response for a better user experience:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"Streaming response:\")\n",
        "for chunk in client.completion(\n",
        "    prompt=\"Explain quantum computing in simple terms\",\n",
        "    model=\"openai/gpt-4o-mini\",\n",
        "    stream=True\n",
        "):\n",
        "    if isinstance(chunk, dict) and \"data\" in chunk:\n",
        "        print(chunk[\"data\"], end=\"\", flush=True)\n",
        "    else:\n",
        "        print(chunk, end=\"\", flush=True)\n",
        "print(\"\\nStreaming complete!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "\n",
        "## 3. Embeddings\n",
        "\n",
        "### Single Text Embedding\n",
        "\n",
        "Generate embeddings for a single text:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "HTTP error (no JSON response): 401 Client Error: Unauthorized for url: http://localhost:8000/api/v1/embeddings\n"
          ]
        },
        {
          "ename": "AuthenticationError",
          "evalue": "Authentication failed: 401 Client Error: Unauthorized for url: http://localhost:8000/api/v1/embeddings",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
            "File \u001b[1;32mE:\\Codes\\indoxRouter\\indoxRouter\\client.py:180\u001b[0m, in \u001b[0;36mClient._request\u001b[1;34m(self, method, endpoint, data, stream)\u001b[0m\n\u001b[0;32m    178\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m response\n\u001b[1;32m--> 180\u001b[0m \u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    181\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m response\u001b[38;5;241m.\u001b[39mjson()\n",
            "File \u001b[1;32me:\\ANACONDA\\Lib\\site-packages\\requests\\models.py:1024\u001b[0m, in \u001b[0;36mResponse.raise_for_status\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1023\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m http_error_msg:\n\u001b[1;32m-> 1024\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m HTTPError(http_error_msg, response\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m)\n",
            "\u001b[1;31mHTTPError\u001b[0m: 401 Client Error: Unauthorized for url: http://localhost:8000/api/v1/embeddings",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[1;31mAuthenticationError\u001b[0m                       Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[8], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membeddings\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtext\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mHello, world!\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mopenai/text-embedding-3-small\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[0;32m      4\u001b[0m \u001b[43m)\u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEmbedding dimensions:\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mlen\u001b[39m(response[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;241m0\u001b[39m]))\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFirst 5 values:\u001b[39m\u001b[38;5;124m\"\u001b[39m, response[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;241m0\u001b[39m][:\u001b[38;5;241m5\u001b[39m])\n",
            "File \u001b[1;32mE:\\Codes\\indoxRouter\\indoxRouter\\client.py:377\u001b[0m, in \u001b[0;36mClient.embeddings\u001b[1;34m(self, text, model, **kwargs)\u001b[0m\n\u001b[0;32m    369\u001b[0m         filtered_kwargs[key] \u001b[38;5;241m=\u001b[39m value\n\u001b[0;32m    371\u001b[0m data \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m    372\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m\"\u001b[39m: text \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(text, \u001b[38;5;28mlist\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m [text],\n\u001b[0;32m    373\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m: formatted_model,\n\u001b[0;32m    374\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124madditional_params\u001b[39m\u001b[38;5;124m\"\u001b[39m: filtered_kwargs,\n\u001b[0;32m    375\u001b[0m }\n\u001b[1;32m--> 377\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_request\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mPOST\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mEMBEDDING_ENDPOINT\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mE:\\Codes\\indoxRouter\\indoxRouter\\client.py:195\u001b[0m, in \u001b[0;36mClient._request\u001b[1;34m(self, method, endpoint, data, stream)\u001b[0m\n\u001b[0;32m    192\u001b[0m error_message \u001b[38;5;241m=\u001b[39m error_data\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdetail\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mstr\u001b[39m(e))\n\u001b[0;32m    194\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m status_code \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m401\u001b[39m:\n\u001b[1;32m--> 195\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m AuthenticationError(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAuthentication failed: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00merror_message\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    196\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m status_code \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m404\u001b[39m:\n\u001b[0;32m    197\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprovider\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m error_message\u001b[38;5;241m.\u001b[39mlower():\n",
            "\u001b[1;31mAuthenticationError\u001b[0m: Authentication failed: 401 Client Error: Unauthorized for url: http://localhost:8000/api/v1/embeddings"
          ]
        }
      ],
      "source": [
        "response = client.embeddings(\n",
        "    text=\"Hello, world!\",  \n",
        "    model=\"openai/text-embedding-3-small\"\n",
        ")\n",
        "\n",
        "print(\"Embedding dimensions:\", len(response[\"data\"][0]))\n",
        "print(\"First 5 values:\", response[\"data\"][0][:5])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "\n",
        "### Multiple Text Embeddings\n",
        "\n",
        "Generate embeddings for multiple texts:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "response = client.embeddings(\n",
        "    text=[\"Hello, world!\", \"How are you?\", \"IndoxRouter is awesome!\"],\n",
        "    model=\"openai/text-embedding-ada-002\"\n",
        ")\n",
        "\n",
        "print(\"Number of embeddings:\", len(response[\"data\"]))\n",
        "print(\"Dimensions of each embedding:\", len(response[\"data\"][0]))\n",
        "print(\"Cost:\", response[\"usage\"][\"cost\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "\n",
        "### Using Different Embedding Models\n",
        "\n",
        "Try different embedding models:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cohere embeddings\n",
        "response = client.embeddings(\n",
        "    text=\"Hello, world!\",\n",
        "    model=\"cohere/embed-english-v3.0\"\n",
        ")\n",
        "\n",
        "print(\"Cohere embedding dimensions:\", len(response[\"data\"][0]))\n",
        "print(\"Cost:\", response[\"usage\"][\"cost\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "\n",
        "## 4. Image Generation\n",
        "\n",
        "### Basic Image Generation\n",
        "\n",
        "Generate a simple image:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "response = client.images(\n",
        "    prompt=\"A beautiful sunset over the ocean\",\n",
        "    model=\"openai/dall-e-2\"\n",
        ")\n",
        "\n",
        "print(\"Image URL:\", response[\"data\"][0][\"url\"])\n",
        "print(\"Cost:\", response[\"usage\"][\"cost\"])\n",
        "\n",
        "# Display the image if in a notebook\n",
        "from IPython.display import Image, display\n",
        "display(Image(url=response[\"data\"][0][\"url\"]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "\n",
        "### Multiple Images\n",
        "\n",
        "Generate multiple images:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "response = client.images(\n",
        "    prompt=\"A futuristic city with flying cars\",\n",
        "    model=\"openai/dall-e-3\",\n",
        "    n=2\n",
        ")\n",
        "\n",
        "print(f\"Generated {len(response['data'])} images:\")\n",
        "for i, image in enumerate(response[\"data\"]):\n",
        "    print(f\"Image {i+1} URL: {image['url']}\")\n",
        "\n",
        "# Display the images if in a notebook\n",
        "from IPython.display import Image, display\n",
        "for image in response[\"data\"]:\n",
        "    display(Image(url=image[\"url\"]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "\n",
        "### Image Generation with Different Parameters\n",
        "\n",
        "Use different parameters for image generation:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "response = client.images(\n",
        "    prompt=\"A photorealistic portrait of a cyberpunk character\",\n",
        "    model=\"openai/dall-e-3\",\n",
        "    size=\"1024x1024\",\n",
        "    quality=\"hd\",\n",
        "    style=\"vivid\"\n",
        ")\n",
        "\n",
        "print(\"Image URL:\", response[\"data\"][0][\"url\"])\n",
        "print(\"Cost:\", response[\"usage\"][\"cost\"])\n",
        "\n",
        "# Display the image if in a notebook\n",
        "from IPython.display import Image, display\n",
        "display(Image(url=response[\"data\"][0][\"url\"]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "\n",
        "## 5. Model Information\n",
        "\n",
        "### List All Models\n",
        "\n",
        "Get information about all available models:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "models = client.models()\n",
        "\n",
        "print(\"Available providers:\")\n",
        "for provider in models[\"providers\"]:\n",
        "    print(f\"- {provider['name']} ({provider['id']})\")\n",
        "    print(f\"  Capabilities: {', '.join(provider['capabilities'])}\")\n",
        "    print(f\"  Models: {len(provider['models'])}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "\n",
        "### List Models for a Specific Provider\n",
        "\n",
        "Get models for a specific provider:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "openai_models = client.models(\"openai\")\n",
        "\n",
        "print(f\"OpenAI models ({len(openai_models['models'])}):\")\n",
        "for model in openai_models[\"models\"]:\n",
        "    print(f\"- {model['name']} ({model['id']})\")\n",
        "    print(f\"  Capabilities: {', '.join(model['capabilities'])}\")\n",
        "    if \"pricing\" in model:\n",
        "        print(f\"  Pricing: Input ${model['pricing']['input']}/1K tokens, Output ${model['pricing']['output']}/1K tokens\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "\n",
        "### Get Specific Model Information\n",
        "\n",
        "Get detailed information about a specific model:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model_info = client.get_model_info(\"openai\", \"gpt-4o-mini\")\n",
        "\n",
        "print(f\"Model: {model_info['name']} ({model_info['id']})\")\n",
        "print(f\"Provider: {model_info['provider']}\")\n",
        "print(f\"Description: {model_info['description']}\")\n",
        "print(f\"Capabilities: {', '.join(model_info['capabilities'])}\")\n",
        "print(f\"Input price: ${model_info['pricing']['input']}/1K tokens\")\n",
        "print(f\"Output price: ${model_info['pricing']['output']}/1K tokens\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "\n",
        "## 6. Usage Statistics\n",
        "\n",
        "### Get Usage Statistics\n",
        "\n",
        "Get usage statistics for the current user:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "usage = client.get_usage()\n",
        "\n",
        "print(\"Usage statistics:\")\n",
        "print(f\"Total requests: {usage['total_requests']}\")\n",
        "print(f\"Total cost: ${usage['total_cost']}\")\n",
        "print(f\"Remaining credits: ${usage['remaining_credits']}\")\n",
        "\n",
        "print(\"\\nBreakdown by endpoint:\")\n",
        "for endpoint, stats in usage[\"endpoints\"].items():\n",
        "    print(f\"- {endpoint}: {stats['requests']} requests, ${stats['cost']} cost\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "\n",
        "## 7. Error Handling\n",
        "\n",
        "### Handle Different Errors\n",
        "\n",
        "Handle different types of errors gracefully:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Function to demonstrate error handling\n",
        "def try_request(func, *args, **kwargs):\n",
        "    try:\n",
        "        return func(*args, **kwargs)\n",
        "    except ModelNotFoundError as e:\n",
        "        print(f\"Model not found: {e}\")\n",
        "    except ProviderNotFoundError as e:\n",
        "        print(f\"Provider not found: {e}\")\n",
        "    except InsufficientCreditsError as e:\n",
        "        print(f\"Insufficient credits: {e}\")\n",
        "    except InvalidParametersError as e:\n",
        "        print(f\"Invalid parameters: {e}\")\n",
        "    except RateLimitError as e:\n",
        "        print(f\"Rate limit exceeded: {e}\")\n",
        "    except ProviderError as e:\n",
        "        print(f\"Provider error: {e}\")\n",
        "    except NetworkError as e:\n",
        "        print(f\"Network error: {e}\")\n",
        "    except AuthenticationError as e:\n",
        "        print(f\"Authentication error: {e}\")\n",
        "    except IndoxRouterError as e:\n",
        "        print(f\"IndoxRouter error: {e}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Unexpected error: {e}\")\n",
        "    return None\n",
        "\n",
        "# Example: Model not found\n",
        "result = try_request(client.chat,\n",
        "    messages=[{\"role\": \"user\", \"content\": \"Hello\"}],\n",
        "    model=\"nonexistent/model\"\n",
        ")\n",
        "\n",
        "# Example: Invalid parameters\n",
        "result = try_request(client.chat,\n",
        "    messages=\"This is not a list of messages\",  # Should be a list\n",
        "    model=\"openai/gpt-4o-mini\"\n",
        ")\n",
        "\n",
        "# Example: Provider not found\n",
        "result = try_request(client.chat,\n",
        "    messages=[{\"role\": \"user\", \"content\": \"Hello\"}],\n",
        "    model=\"nonexistent/gpt-4o-mini\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "\n",
        "## 8. Advanced Usage\n",
        "\n",
        "### Using as a Context Manager\n",
        "\n",
        "Use the client as a context manager to automatically close the session:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "with Client(api_key=\"your_api_key\") as client:\n",
        "    response = client.chat(\n",
        "        messages=[{\"role\": \"user\", \"content\": \"Hello, how are you?\"}],\n",
        "        model=\"openai/gpt-4o-mini\"\n",
        "    )\n",
        "    print(\"Response:\", response[\"data\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "\n",
        "### Combining Different Capabilities\n",
        "\n",
        "Combine different capabilities for more complex use cases:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Example: Generate text and then create an image based on it\n",
        "completion_response = client.completion(\n",
        "    prompt=\"Describe a fantastical creature that has never been seen before.\",\n",
        "    model=\"openai/gpt-4o-mini\",\n",
        "    max_tokens=200\n",
        ")\n",
        "\n",
        "creature_description = completion_response[\"data\"]\n",
        "print(\"Generated description:\", creature_description)\n",
        "\n",
        "# Now create an image based on the description\n",
        "image_response = client.images(\n",
        "    prompt=f\"A detailed illustration of: {creature_description}\",\n",
        "    model=\"openai/dall-e-3\"\n",
        ")\n",
        "\n",
        "print(\"Image URL:\", image_response[\"data\"][0][\"url\"])\n",
        "\n",
        "# Display the image if in a notebook\n",
        "from IPython.display import Image, display\n",
        "display(Image(url=image_response[\"data\"][0][\"url\"]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "\n",
        "### Semantic Search with Embeddings\n",
        "\n",
        "Implement a simple semantic search using embeddings:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "# Define some documents\n",
        "documents = [\n",
        "    \"The quick brown fox jumps over the lazy dog.\",\n",
        "    \"A fast auburn fox leaps above the sleepy canine.\",\n",
        "    \"IndoxRouter provides a unified API for various AI providers.\",\n",
        "    \"The API allows access to multiple AI models through a single interface.\",\n",
        "    \"Paris is the capital of France and known for the Eiffel Tower.\",\n",
        "    \"Rome is the capital of Italy and home to the Colosseum.\"\n",
        "]\n",
        "\n",
        "# Generate embeddings for all documents\n",
        "embeddings_response = client.embeddings(\n",
        "    text=documents,\n",
        "    model=\"openai/text-embedding-ada-002\"\n",
        ")\n",
        "\n",
        "document_embeddings = embeddings_response[\"data\"]\n",
        "\n",
        "# Function to calculate cosine similarity\n",
        "def cosine_similarity(a, b):\n",
        "    return np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b))\n",
        "\n",
        "# Function to find most similar documents\n",
        "def semantic_search(query, document_embeddings, documents, top_n=2):\n",
        "    # Get embedding for the query\n",
        "    query_embedding_response = client.embeddings(\n",
        "        text=query,\n",
        "        model=\"openai/text-embedding-ada-002\"\n",
        "    )\n",
        "    query_embedding = query_embedding_response[\"data\"][0]\n",
        "\n",
        "    # Calculate similarities\n",
        "    similarities = [\n",
        "        cosine_similarity(query_embedding, doc_embedding)\n",
        "        for doc_embedding in document_embeddings\n",
        "    ]\n",
        "\n",
        "    # Get top N results\n",
        "    top_indices = np.argsort(similarities)[-top_n:][::-1]\n",
        "\n",
        "    return [\n",
        "        {\"document\": documents[i], \"similarity\": similarities[i]}\n",
        "        for i in top_indices\n",
        "    ]\n",
        "\n",
        "# Example search\n",
        "results = semantic_search(\"What is IndoxRouter?\", document_embeddings, documents)\n",
        "\n",
        "print(\"Search results:\")\n",
        "for i, result in enumerate(results):\n",
        "    print(f\"{i+1}. {result['document']} (Similarity: {result['similarity']:.4f})\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "\n",
        "### RAG (Retrieval-Augmented Generation)\n",
        "\n",
        "Implement a simple RAG system:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Using the same documents and embeddings from the previous example\n",
        "\n",
        "def rag_query(query, document_embeddings, documents, top_n=2):\n",
        "    # Get relevant documents\n",
        "    relevant_docs = semantic_search(query, document_embeddings, documents, top_n)\n",
        "\n",
        "    # Create a context from the relevant documents\n",
        "    context = \"\\n\".join([doc[\"document\"] for doc in relevant_docs])\n",
        "\n",
        "    # Create a prompt with the context\n",
        "    prompt = f\"\"\"\n",
        "    Context information:\n",
        "    {context}\n",
        "\n",
        "    Based on the context information, please answer the following question:\n",
        "    {query}\n",
        "    \"\"\"\n",
        "\n",
        "    # Generate a response\n",
        "    response = client.chat(\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": \"You are a helpful assistant. Answer the question based only on the provided context.\"},\n",
        "            {\"role\": \"user\", \"content\": prompt}\n",
        "        ],\n",
        "        model=\"openai/gpt-4o-mini\"\n",
        "    )\n",
        "\n",
        "    return response[\"data\"]\n",
        "\n",
        "# Example RAG query\n",
        "answer = rag_query(\"What does IndoxRouter do?\", document_embeddings, documents)\n",
        "print(\"RAG Answer:\", answer)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "\n",
        "## 9. Troubleshooting and Debugging\n",
        "\n",
        "### Enable Debug Mode\n",
        "\n",
        "Enable debug logging to see detailed information about requests and responses:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Enable debug logging\n",
        "client.enable_debug()\n",
        "\n",
        "# Try a request\n",
        "try:\n",
        "    response = client.chat(\n",
        "        messages=[{\"role\": \"user\", \"content\": \"Hello\"}],\n",
        "        model=\"openai/gpt-4o-mini\"\n",
        "    )\n",
        "    print(\"Success!\")\n",
        "except Exception as e:\n",
        "    print(f\"Error: {e}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "\n",
        "### Testing Server Connection\n",
        "\n",
        "Use the `test_connection` method to verify that your server is accessible and properly configured:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test the connection to the server\n",
        "connection_info = client.test_connection()\n",
        "print(f\"Connection status: {connection_info['status']}\")\n",
        "\n",
        "if connection_info['status'] == 'connected':\n",
        "    print(f\"Server URL: {connection_info['url']}\")\n",
        "    print(f\"Status code: {connection_info['status_code']}\")\n",
        "    if connection_info['server_info']:\n",
        "        print(f\"Server info: {connection_info['server_info']}\")\n",
        "else:\n",
        "    print(f\"Error: {connection_info['error']}\")\n",
        "    print(f\"Error type: {connection_info['error_type']}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "\n",
        "### Common Issues and Solutions\n",
        "\n",
        "#### \"Resource not found\" Error\n",
        "\n",
        "If you see a \"Resource not found\" error, it usually means one of the following:\n",
        "\n",
        "1. The server is not running. Make sure your IndoxRouter server is up and running:\n",
        "\n",
        "   ```bash\n",
        "   cd indoxRouter_server\n",
        "   python -m main\n",
        "   ```\n",
        "\n",
        "2. The base URL is incorrect. Check the URL in your environment:\n",
        "\n",
        "   "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(f\"Using base URL: {client.base_url}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1d3d019a",
      "metadata": {},
      "source": [
        "\n",
        "\n",
        "3. The API endpoint path is incorrect. The client automatically adds the API version prefix, but you can check the full URL in the debug logs.\n",
        "\n",
        "#### Server Errors (500 Internal Server Error)\n",
        "\n",
        "If you encounter a 500 Internal Server Error:\n",
        "\n",
        "1. Check the server logs for detailed error information:\n",
        "\n",
        "   ```bash\n",
        "   # Look at the server logs\n",
        "   cd indoxRouter_server\n",
        "   tail -f logs/server.log\n",
        "   ```\n",
        "\n",
        "2. Verify that the provider service is available and properly configured on the server.\n",
        "\n",
        "3. Check if your request parameters are valid for the specific model you're using.\n",
        "\n",
        "4. Try with a different model or provider to see if the issue is specific to one provider.\n",
        "\n",
        "5. If you see a \"too many values to unpack\" error, it might be related to how the server parses the model string. The client now automatically formats the model string to be compatible with the server, but you can try different formats:\n",
        "\n",
        "   "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c8737736",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Try using a different model format\n",
        "   try:\n",
        "       # First attempt with standard format\n",
        "       response = client.chat(\n",
        "           messages=[{\"role\": \"user\", \"content\": \"Hello\"}],\n",
        "           model=\"openai/gpt-4o-mini\"\n",
        "       )\n",
        "   except ProviderError as e:\n",
        "       if \"too many values to unpack\" in str(e):\n",
        "           # Try with a different provider/model\n",
        "           response = client.chat(\n",
        "               messages=[{\"role\": \"user\", \"content\": \"Hello\"}],\n",
        "               model=\"anthropic/claude-3-haiku\"\n",
        "           )"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "946ff17d",
      "metadata": {},
      "source": [
        "\n",
        "\n",
        "#### Authentication Errors\n",
        "\n",
        "If you see an \"Authentication failed\" error:\n",
        "\n",
        "1. Make sure your API key is correct:\n",
        "\n",
        "   "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# First few characters of the API key (for security)\n",
        "   print(f\"API key starts with: {client.api_key[:5]}...\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "\n",
        "2. Check if your API key is valid on the server.\n",
        "\n",
        "#### Connection Errors\n",
        "\n",
        "If you can't connect to the server:\n",
        "\n",
        "1. Use the `test_connection` method to diagnose the issue:\n",
        "\n",
        "   "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "connection_info = client.test_connection()\n",
        "   print(f\"Connection status: {connection_info}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "\n",
        "2. Check for firewall or network issues.\n",
        "\n",
        "#### Insufficient Credits\n",
        "\n",
        "If you see an \"Insufficient credits\" error:\n",
        "\n",
        "1. Check your current credit balance:\n",
        "\n",
        "   "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "try:\n",
        "      usage = client.get_usage()\n",
        "      print(f\"Remaining credits: ${usage['remaining_credits']}\")\n",
        "   except Exception as e:\n",
        "      print(f\"Error getting usage: {e}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "\n",
        "2. Contact your administrator to add more credits to your account.\n",
        "\n",
        "## 10. Cleanup\n",
        "\n",
        "Don't forget to close the client when you're done:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "client.close()\n",
        "print(\"Client closed successfully!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "\n",
        "## Conclusion\n",
        "\n",
        "This cookbook has demonstrated how to use the IndoxRouter client to interact with various AI providers through a unified API. You can now use these examples as a starting point for your own applications.\n",
        "\n",
        "For more information, refer to the [IndoxRouter documentation](https://docs.indoxrouter.com).\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
