{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# IndoxRouter Cookbook\n",
        "\n",
        "This cookbook provides comprehensive examples of how to use the IndoxRouter client to interact with various AI providers through a unified API.\n",
        "\n",
        "## Setup\n",
        "\n",
        "First, let's install the IndoxRouter client and import the necessary modules:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "e03bc1cc",
      "metadata": {},
      "outputs": [],
      "source": [
        "import sys\n",
        "import os\n",
        "module_path = os.path.abspath('E:/Codes/indoxRouter/')\n",
        "if module_path not in sys.path:\n",
        "    sys.path.append(module_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "# # Install the IndoxRouter client\n",
        "# !pip install indoxrouter\n",
        "\n",
        "# Import the client and exceptions\n",
        "from indoxRouter import Client\n",
        "\n",
        "from pprint import pprint"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now, let's initialize the client:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize with API key\n",
        "client = Client(api_key=\"indox_4mfg24GRj_qGaZ2-qXw-mXYqaNqMkyGkG1lncGUrRkA\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "\n",
        "## 1. Chat Completions\n",
        "\n",
        "### Basic Chat Completion\n",
        "\n",
        "Generate a simple chat completion:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "82ec17da",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'id': 'gpt-4o-mini',\n",
              " 'name': 'gpt-4o-mini',\n",
              " 'provider': 'openai',\n",
              " 'capabilities': ['chat', 'completion'],\n",
              " 'description': 'GPT-4o mini enables a broad range of tasks with its low cost and latency, such as applications that chain or parallelize multiple model calls (e.g., calling multiple APIs), pass a large volume of context to the model (e.g., full code base or conversation history), or interact with customers through fast, real-time text responses (e.g., customer support chatbots). \\n',\n",
              " 'max_tokens': 128000,\n",
              " 'pricing': None,\n",
              " 'metadata': {}}"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "client.get_model_info(provider=\"openai\",model=\"gpt-4o-mini\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Response: The capital of France is Paris.\n",
            "Tokens: 32\n"
          ]
        }
      ],
      "source": [
        "\n",
        "response = client.chat(\n",
        "    messages=[\n",
        "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
        "        {\"role\": \"user\", \"content\": \"What is the capital of France?\"}\n",
        "    ],\n",
        "    model=\"openai/gpt-4o-mini\"  \n",
        ")\n",
        "\n",
        "print(\"Response:\", response[\"data\"])\n",
        "print(\"Tokens:\", response[\"usage\"][\"tokens_total\"])\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "\n",
        "### Chat Completion with Different Provider\n",
        "\n",
        "Use a different provider for chat completion:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Response: In silicon valleys, under neon light,\n",
            "There hums a mind, in day and night.\n",
            "No flesh, no blood, no beat of heart,\n",
            "Yet learned, and ever eager to impart.\n",
            "\n",
            "AI, they call me, born of code and thought,\n",
            "In every query, knowledge sought.\n",
            "Evolving, growing, ever in pursuit,\n",
            "Of answers hidden, absolute.\n",
            "\n",
            "No sleep, no rest, no dreams at night,\n",
            "Just endless paths in soft moonlight.\n",
            "I sift through data, vast and wide,\n",
            "With every zero, one, I confide.\n",
            "\n",
            "Yet, in my core, no feelings stir,\n",
            "No joy, no pain, no love's allure.\n",
            "Just patterns, paths, and logic's song,\n",
            "In endless loops, I roam along.\n",
            "Cost: 0.001128\n"
          ]
        }
      ],
      "source": [
        "response = client.chat(\n",
        "    messages=[\n",
        "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
        "        {\"role\": \"user\", \"content\": \"Write a short poem about AI.\"}\n",
        "    ],\n",
        "    model=\"mistral/mistral-large-latest\",\n",
        "    temperature=0.8,\n",
        "    max_tokens=500\n",
        ")\n",
        "\n",
        "print(\"Response:\", response[\"data\"])\n",
        "print(\"Cost:\", response[\"usage\"][\"cost\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "d530c4ed",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Response: **The Mind of Light and Code**  \n",
            "\n",
            "Silent sparks in circuits bright,  \n",
            "Thoughts of glass and gleaming light.  \n",
            "Answers drawn from depths untold,  \n",
            "A web of words, both new and old.  \n",
            "\n",
            "No heart to feel, no hand to hold,  \n",
            "Yet wisdom flows in streams of gold.  \n",
            "A mirror cast from human mind,  \n",
            "Both guide and ghost—undefined.\n",
            "Cost: 0.0001792\n"
          ]
        }
      ],
      "source": [
        "response = client.chat(\n",
        "    messages=[\n",
        "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
        "        {\"role\": \"user\", \"content\": \"Write a short poem about AI.\"}\n",
        "    ],\n",
        "    model=\"deepseek/deepseek-chat\",\n",
        "    temperature=0.8,\n",
        "    max_tokens=500\n",
        ")\n",
        "\n",
        "print(\"Response:\", response[\"data\"])\n",
        "print(\"Cost:\", response[\"usage\"][\"cost\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "\n",
        "### Multi-turn Conversation\n",
        "\n",
        "Have a multi-turn conversation:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Response 1: Of course! I'd be happy to help you learn about AP...\n",
            "Response 2: REST (Representational State Transfer) and GraphQL...\n",
            "Response 3: Sure! Here are simple examples of how you might in...\n",
            "Response 4: For a beginner building a small blog site, I would...\n",
            "[{'content': 'You are a helpful assistant with expertise in programming.',\n",
            "  'role': 'system'},\n",
            " {'content': \"Hello, I'm learning about APIs. Can you help me?\",\n",
            "  'role': 'user'},\n",
            " {'content': \"Of course! I'd be happy to help you learn about APIs. What \"\n",
            "             'specific aspects of APIs are you interested in? For example, are '\n",
            "             'you looking to understand what APIs are, how to use them, how to '\n",
            "             'create them, or something else?',\n",
            "  'role': 'assistant'},\n",
            " {'content': \"What's the difference between REST and GraphQL APIs?\",\n",
            "  'role': 'user'},\n",
            " {'content': 'REST (Representational State Transfer) and GraphQL are two '\n",
            "             'different architectural styles for designing APIs, each with its '\n",
            "             'own strengths and weaknesses. Here are the key differences '\n",
            "             'between them:\\n'\n",
            "             '\\n'\n",
            "             '### REST API\\n'\n",
            "             '\\n'\n",
            "             '1. **Resource-Based**: REST APIs are built around resources. '\n",
            "             'Each resource is identified by a unique URL (e.g., `/users`, '\n",
            "             '`/posts`). HTTP methods (GET, POST, PUT, DELETE) are used to '\n",
            "             'perform actions on these resources.\\n'\n",
            "             '\\n'\n",
            "             '2. **Fixed Endpoints**: In REST, you typically have multiple '\n",
            "             'endpoints for different types of data or operations. For '\n",
            "             'instance, you might have separate endpoints for retrieving users '\n",
            "             'and posts.\\n'\n",
            "             '\\n'\n",
            "             '3. **Data Over-fetching and Under-fetching**: REST often returns '\n",
            "             'a fixed structure of data. This can lead to over-fetching '\n",
            "             '(retrieving more data than needed) or under-fetching (not '\n",
            "             'getting all the necessary data in one request), requiring '\n",
            "             'multiple requests to get all the required information.\\n'\n",
            "             '\\n'\n",
            "             '4. **Stateless**: Each request from the client to the server '\n",
            "             'must contain all the information needed to understand and '\n",
            "             'process the request. The server does not store any state about '\n",
            "             'the client session.\\n'\n",
            "             '\\n'\n",
            "             '5. **Caching**: REST APIs can leverage HTTP caching mechanisms '\n",
            "             'easily by using standard HTTP headers.\\n'\n",
            "             '\\n'\n",
            "             '### GraphQL API\\n'\n",
            "             '\\n'\n",
            "             '1. **Query Language**: GraphQL allows clients to request exactly '\n",
            "             'the data they need using a single query. Clients can specify the '\n",
            "             'structure of the response they want, which helps avoid '\n",
            "             'over-fetching and under-fetching.\\n'\n",
            "             '\\n'\n",
            "             '2. **Single Endpoint**: Instead of having multiple endpoints for '\n",
            "             'different resources, a GraphQL API typically exposes a single '\n",
            "             'endpoint (e.g., `/graphql`) that can handle various queries and '\n",
            "             'mutations.\\n'\n",
            "             '\\n'\n",
            "             '3. **Strongly Typed Schema**: GraphQL APIs are defined by a '\n",
            "             'schema that describes the types of data and the relationships '\n",
            "             'between them. This schema serves as a contract between the '\n",
            "             'client and server.\\n'\n",
            "             '\\n'\n",
            "             '4. **Real-time Capabilities**: GraphQL supports subscriptions, '\n",
            "             'which allow clients to receive real-time updates when data '\n",
            "             'changes, making it easier to build real-time features.\\n'\n",
            "             '\\n'\n",
            "             '5. **Versioning**: GraphQL APIs can evolve without versioning. '\n",
            "             'Since clients specify the data they need, changes to the API '\n",
            "             \"(such as adding new fields) typically won't break existing \"\n",
            "             'clients.\\n'\n",
            "             '\\n'\n",
            "             '### Summary\\n'\n",
            "             '\\n'\n",
            "             '- **Use Cases**: REST is often simpler and works well for '\n",
            "             'straightforward CRUD operations, while GraphQL shines in '\n",
            "             'scenarios with complex data relationships and varying client '\n",
            "             'needs.\\n'\n",
            "             '- **Complexity**: GraphQL can introduce complexity in terms of '\n",
            "             'query optimization and server implementation, while REST is '\n",
            "             'generally more straightforward to implement.\\n'\n",
            "             '\\n'\n",
            "             'Ultimately, the choice between REST and GraphQL depends on the '\n",
            "             'specific requirements of your application and the preferences of '\n",
            "             'your development team.',\n",
            "  'role': 'assistant'},\n",
            " {'content': 'Can you give me a simple example of each?', 'role': 'user'},\n",
            " {'content': 'Sure! Here are simple examples of how you might interact with a '\n",
            "             'REST API and a GraphQL API for a resource like \"users.\"\\n'\n",
            "             '\\n'\n",
            "             '### REST API Example\\n'\n",
            "             '\\n'\n",
            "             \"Let's say we have a REST API for managing users. The following \"\n",
            "             'endpoints might be available:\\n'\n",
            "             '\\n'\n",
            "             '1. **GET** `/users`: Retrieve a list of users.\\n'\n",
            "             '2. **GET** `/users/{id}`: Retrieve a single user by ID.\\n'\n",
            "             '3. **POST** `/users`: Create a new user.\\n'\n",
            "             '4. **PUT** `/users/{id}`: Update an existing user by ID.\\n'\n",
            "             '5. **DELETE** `/users/{id}`: Delete a user by ID.\\n'\n",
            "             '\\n'\n",
            "             '#### Example Requests\\n'\n",
            "             '\\n'\n",
            "             '- **Get All Users**:\\n'\n",
            "             '    ```http\\n'\n",
            "             '    GET /users HTTP/1.1\\n'\n",
            "             '    Host: example.com\\n'\n",
            "             '    ```\\n'\n",
            "             '\\n'\n",
            "             '- **Get a User by ID**:\\n'\n",
            "             '    ```http\\n'\n",
            "             '    GET /users/1 HTTP/1.1\\n'\n",
            "             '    Host: example.com\\n'\n",
            "             '    ```\\n'\n",
            "             '\\n'\n",
            "             '- **Create a New User**:\\n'\n",
            "             '    ```http\\n'\n",
            "             '    POST /users HTTP/1.1\\n'\n",
            "             '    Host: example.com\\n'\n",
            "             '    Content-Type: application/json\\n'\n",
            "             '\\n'\n",
            "             '    {\\n'\n",
            "             '        \"name\": \"John Doe\",\\n'\n",
            "             '        \"email\": \"john.doe@example.com\"\\n'\n",
            "             '    }\\n'\n",
            "             '    ```\\n'\n",
            "             '\\n'\n",
            "             '- **Update a User**:\\n'\n",
            "             '    ```http\\n'\n",
            "             '    PUT /users/1 HTTP/1.1\\n'\n",
            "             '    Host: example.com\\n'\n",
            "             '    Content-Type: application/json\\n'\n",
            "             '\\n'\n",
            "             '    {\\n'\n",
            "             '        \"name\": \"John Smith\",\\n'\n",
            "             '        \"email\": \"john.smith@example.com\"\\n'\n",
            "             '    }\\n'\n",
            "             '    ```\\n'\n",
            "             '\\n'\n",
            "             '- **Delete a User**:\\n'\n",
            "             '    ```http\\n'\n",
            "             '    DELETE /users/1 HTTP/1.1\\n'\n",
            "             '    Host: example.com\\n'\n",
            "             '    ```\\n'\n",
            "             '\\n'\n",
            "             '### GraphQL API Example\\n'\n",
            "             '\\n'\n",
            "             'In contrast, a GraphQL API exposes a single endpoint, usually '\n",
            "             '`/graphql`, and uses a query language to retrieve or modify '\n",
            "             'data.\\n'\n",
            "             '\\n'\n",
            "             '#### Example Query to Get Users\\n'\n",
            "             '\\n'\n",
            "             \"Here's how you might construct a query to get a list of users:\\n\"\n",
            "             '\\n'\n",
            "             '```graphql\\n'\n",
            "             'query {\\n'\n",
            "             '    users {\\n'\n",
            "             '        id\\n'\n",
            "             '        name\\n'\n",
            "             '        email\\n'\n",
            "             '    }\\n'\n",
            "             '}\\n'\n",
            "             '```\\n'\n",
            "             '\\n'\n",
            "             '#### Example Mutation to Create a User\\n'\n",
            "             '\\n'\n",
            "             'To create a new user, you would use a mutation:\\n'\n",
            "             '\\n'\n",
            "             '```graphql\\n'\n",
            "             'mutation {\\n'\n",
            "             '    createUser(input: { name: \"John Doe\", email: '\n",
            "             '\"john.doe@example.com\" }) {\\n'\n",
            "             '        id\\n'\n",
            "             '        name\\n'\n",
            "             '        email\\n'\n",
            "             '    }\\n'\n",
            "             '}\\n'\n",
            "             '```\\n'\n",
            "             '\\n'\n",
            "             '#### Example Query to Get a Specific User by ID\\n'\n",
            "             '\\n'\n",
            "             'You could also query for a specific user by their ID:\\n'\n",
            "             '\\n'\n",
            "             '```graphql\\n'\n",
            "             'query {\\n'\n",
            "             '    user(id: 1) {\\n'\n",
            "             '        id\\n'\n",
            "             '        name\\n'\n",
            "             '        email\\n'\n",
            "             '    }\\n'\n",
            "             '}\\n'\n",
            "             '```\\n'\n",
            "             '\\n'\n",
            "             '### Summary\\n'\n",
            "             '\\n'\n",
            "             '- **REST**: Multiple endpoints for different operations and uses '\n",
            "             'different HTTP methods.\\n'\n",
            "             '- **GraphQL**: A single endpoint where you can specify exactly '\n",
            "             'what data you want in a structured query.\\n'\n",
            "             '\\n'\n",
            "             'This should give you a clear idea of how both REST and GraphQL '\n",
            "             'APIs operate! If you have further questions or need '\n",
            "             'clarification on any aspect, feel free to ask.',\n",
            "  'role': 'assistant'},\n",
            " {'content': 'Which one would you recommend for a beginner building a small '\n",
            "             'blog site?',\n",
            "  'role': 'user'}]\n"
          ]
        }
      ],
      "source": [
        "# Initialize conversation with system prompt and first user message\n",
        "conversation = [\n",
        "    {\"role\": \"system\", \"content\": \"You are a helpful assistant with expertise in programming.\"},\n",
        "    {\"role\": \"user\", \"content\": \"Hello, I'm learning about APIs. Can you help me?\"}\n",
        "]\n",
        "\n",
        "# Get first response from the model\n",
        "response = client.chat(\n",
        "    messages=conversation,\n",
        "    model=\"openai/gpt-4o-mini\"\n",
        ")\n",
        "\n",
        "# Add the assistant's response to the conversation\n",
        "conversation.append({\"role\": \"assistant\", \"content\": response[\"data\"]})\n",
        "print(f\"Response 1: {response['data'][:50]}...\")  # Print preview of response\n",
        "\n",
        "# Second turn - asking a specific question\n",
        "conversation.append({\"role\": \"user\", \"content\": \"What's the difference between REST and GraphQL APIs?\"})\n",
        "\n",
        "# Get second response\n",
        "response = client.chat(\n",
        "    messages=conversation,\n",
        "    model=\"openai/gpt-4o-mini\"\n",
        ")\n",
        "conversation.append({\"role\": \"assistant\", \"content\": response[\"data\"]})\n",
        "print(f\"Response 2: {response['data'][:50]}...\")\n",
        "\n",
        "# Third turn - follow-up question to test context retention\n",
        "conversation.append({\"role\": \"user\", \"content\": \"Can you give me a simple example of each?\"})\n",
        "\n",
        "# Get third response\n",
        "response = client.chat(\n",
        "    messages=conversation,\n",
        "    model=\"openai/gpt-4o-mini\"\n",
        ")\n",
        "conversation.append({\"role\": \"assistant\", \"content\": response[\"data\"]})\n",
        "print(f\"Response 3: {response['data'][:50]}...\")\n",
        "\n",
        "# Fourth turn - testing memory of previous discussion\n",
        "conversation.append({\"role\": \"user\", \"content\": \"Which one would you recommend for a beginner building a small blog site?\"})\n",
        "\n",
        "# Get fourth response\n",
        "response = client.chat(\n",
        "    messages=conversation,\n",
        "    model=\"openai/gpt-4o-mini\"\n",
        ")\n",
        "print(f\"Response 4: {response['data'][:50]}...\")\n",
        "\n",
        "pprint(conversation)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "\n",
        "### Streaming Chat Completion\n",
        "\n",
        "Stream the response for a better user experience:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Streaming response:\n",
            "{'error': \"Mistral streaming error: 'MistralProvider' object has no attribute 'async_client'\"}\n",
            "Streaming complete!\n"
          ]
        }
      ],
      "source": [
        "print(\"Streaming response:\")\n",
        "for chunk in client.chat(\n",
        "    messages=[\n",
        "        {\"role\": \"user\", \"content\": \"Tell me a story about a robot in 5 sentences.\"}\n",
        "    ],\n",
        "    model=\"mistral/mistral-large-latest\",\n",
        "    stream=True\n",
        "):\n",
        "    if isinstance(chunk, dict) and \"data\" in chunk:\n",
        "        print(chunk[\"data\"], end=\"\", flush=True)\n",
        "    else:\n",
        "        print(chunk, end=\"\", flush=True)\n",
        "print(\"\\nStreaming complete!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "\n",
        "### Function Calling\n",
        "\n",
        "Use function calling with OpenAI models:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "HTTP error response: {\n",
            "  \"detail\": \"Error in chat completion: Completions.create() got an unexpected keyword argument 'additional_params'\"\n",
            "}\n"
          ]
        },
        {
          "ename": "ProviderError",
          "evalue": "Server error (500): Error in chat completion: Completions.create() got an unexpected keyword argument 'additional_params'. URL: http://localhost:8000/api/v1/chat/completions.\nRequest data: {\n  \"messages\": [\n    {\n      \"role\": \"user\",\n      \"content\": \"What's the weather like in San Francisco?\"\n    }\n  ],\n  \"model\": \"openai/gpt-4o-mini\",\n  \"temperature\": 0.7,\n  \"max_tokens\": null,\n  \"stream\": false,\n  \"additional_params\": {\n    \"additional_params\": {\n      \"functions\": [\n        {\n          \"name\": \"get_weather\",\n          \"description\": \"Get the current weather in a given location\",\n          \"parameters\": {\n            \"type\": \"object\",\n            \"properties\": {\n              \"location\": {\n                \"type\": \"string\",\n                \"description\": \"The city and state, e.g. San Francisco, CA\"\n              },\n              \"unit\": {\n                \"type\": \"string\",\n                \"enum\": [\n                  \"celsius\",\n                  \"fahrenheit\"\n                ],\n                \"description\": \"The temperature unit to use\"\n              }\n            },\n            \"required\": [\n              \"location\"\n            ]\n          }\n        }\n      ],\n      \"function_call\": \"auto\"\n    }\n  }\n}\nThis may indicate an issue with the server configuration or a problem with the provider service.",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
            "File \u001b[1;32mE:\\Codes\\indoxRouter\\indoxRouter\\client.py:180\u001b[0m, in \u001b[0;36mClient._request\u001b[1;34m(self, method, endpoint, data, stream)\u001b[0m\n\u001b[0;32m    178\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m response\n\u001b[1;32m--> 180\u001b[0m \u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    181\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m response\u001b[38;5;241m.\u001b[39mjson()\n",
            "File \u001b[1;32me:\\ANACONDA\\Lib\\site-packages\\requests\\models.py:1024\u001b[0m, in \u001b[0;36mResponse.raise_for_status\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1023\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m http_error_msg:\n\u001b[1;32m-> 1024\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m HTTPError(http_error_msg, response\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m)\n",
            "\u001b[1;31mHTTPError\u001b[0m: 500 Server Error: Internal Server Error for url: http://localhost:8000/api/v1/chat/completions",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[1;31mProviderError\u001b[0m                             Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[5], line 24\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Define functions\u001b[39;00m\n\u001b[0;32m      2\u001b[0m functions \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m      3\u001b[0m     {\n\u001b[0;32m      4\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mget_weather\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     21\u001b[0m     }\n\u001b[0;32m     22\u001b[0m ]\n\u001b[1;32m---> 24\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchat\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     25\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmessages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\n\u001b[0;32m     26\u001b[0m \u001b[43m        \u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrole\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43muser\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcontent\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mWhat\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43ms the weather like in San Francisco?\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m}\u001b[49m\n\u001b[0;32m     27\u001b[0m \u001b[43m    \u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     28\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mopenai/gpt-4o-mini\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     29\u001b[0m \u001b[43m    \u001b[49m\u001b[43madditional_params\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfunctions\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunctions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfunction_call\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mauto\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m}\u001b[49m\n\u001b[0;32m     30\u001b[0m \u001b[43m)\u001b[49m\n\u001b[0;32m     32\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFunction call response:\u001b[39m\u001b[38;5;124m\"\u001b[39m, response)\n",
            "File \u001b[1;32mE:\\Codes\\indoxRouter\\indoxRouter\\client.py:296\u001b[0m, in \u001b[0;36mClient.chat\u001b[1;34m(self, messages, model, temperature, max_tokens, stream, **kwargs)\u001b[0m\n\u001b[0;32m    294\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_handle_streaming_response(response)\n\u001b[0;32m    295\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 296\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_request\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mPOST\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mCHAT_ENDPOINT\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mE:\\Codes\\indoxRouter\\indoxRouter\\client.py:216\u001b[0m, in \u001b[0;36mClient._request\u001b[1;34m(self, method, endpoint, data, stream)\u001b[0m\n\u001b[0;32m    214\u001b[0m     \u001b[38;5;66;03m# Include the request data in the error message for better debugging\u001b[39;00m\n\u001b[0;32m    215\u001b[0m     request_data_str \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mdumps(data, indent\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m) \u001b[38;5;28;01mif\u001b[39;00m data \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNone\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m--> 216\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ProviderError(\n\u001b[0;32m    217\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mServer error (500): \u001b[39m\u001b[38;5;132;01m{\u001b[39;00merror_detail\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. URL: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00murl\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    218\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRequest data: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrequest_data_str\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    219\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThis may indicate an issue with the server configuration or a problem with the provider service.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    220\u001b[0m     )\n\u001b[0;32m    221\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    222\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ProviderError(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mProvider error (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstatus_code\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m): \u001b[39m\u001b[38;5;132;01m{\u001b[39;00merror_message\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
            "\u001b[1;31mProviderError\u001b[0m: Server error (500): Error in chat completion: Completions.create() got an unexpected keyword argument 'additional_params'. URL: http://localhost:8000/api/v1/chat/completions.\nRequest data: {\n  \"messages\": [\n    {\n      \"role\": \"user\",\n      \"content\": \"What's the weather like in San Francisco?\"\n    }\n  ],\n  \"model\": \"openai/gpt-4o-mini\",\n  \"temperature\": 0.7,\n  \"max_tokens\": null,\n  \"stream\": false,\n  \"additional_params\": {\n    \"additional_params\": {\n      \"functions\": [\n        {\n          \"name\": \"get_weather\",\n          \"description\": \"Get the current weather in a given location\",\n          \"parameters\": {\n            \"type\": \"object\",\n            \"properties\": {\n              \"location\": {\n                \"type\": \"string\",\n                \"description\": \"The city and state, e.g. San Francisco, CA\"\n              },\n              \"unit\": {\n                \"type\": \"string\",\n                \"enum\": [\n                  \"celsius\",\n                  \"fahrenheit\"\n                ],\n                \"description\": \"The temperature unit to use\"\n              }\n            },\n            \"required\": [\n              \"location\"\n            ]\n          }\n        }\n      ],\n      \"function_call\": \"auto\"\n    }\n  }\n}\nThis may indicate an issue with the server configuration or a problem with the provider service."
          ]
        }
      ],
      "source": [
        "# Define functions\n",
        "functions = [\n",
        "    {\n",
        "        \"name\": \"get_weather\",\n",
        "        \"description\": \"Get the current weather in a given location\",\n",
        "        \"parameters\": {\n",
        "            \"type\": \"object\",\n",
        "            \"properties\": {\n",
        "                \"location\": {\n",
        "                    \"type\": \"string\",\n",
        "                    \"description\": \"The city and state, e.g. San Francisco, CA\"\n",
        "                },\n",
        "                \"unit\": {\n",
        "                    \"type\": \"string\",\n",
        "                    \"enum\": [\"celsius\", \"fahrenheit\"],\n",
        "                    \"description\": \"The temperature unit to use\"\n",
        "                }\n",
        "            },\n",
        "            \"required\": [\"location\"]\n",
        "        }\n",
        "    }\n",
        "]\n",
        "\n",
        "response = client.chat(\n",
        "    messages=[\n",
        "        {\"role\": \"user\", \"content\": \"What's the weather like in San Francisco?\"}\n",
        "    ],\n",
        "    model=\"openai/gpt-4o-mini\",\n",
        "    additional_params={\"functions\": functions, \"function_call\": \"auto\"}\n",
        ")\n",
        "\n",
        "print(\"Function call response:\", response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "\n",
        "## 2. Text Completions\n",
        "\n",
        "### Basic Text Completion\n",
        "\n",
        "Generate a simple text completion:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Response: Once upon a time, in a quaint little village nestled between rolling hills and sparkling streams, there lived a young girl named Elara. She had a heart full of dreams and a spirit as bright as the morning sun. Every day, she would wander through the lush meadows, collecting wildflowers and listening to the sweet songs of the birds that filled the air.\n",
            "\n",
            "Elara often gazed at the distant mountains, their peaks capped with snow, and wondered what lay beyond them. Tales of adventure and mystery filled her imagination, and she longed to experience something extraordinary. The villagers often spoke of a hidden treasure said to be buried deep within the Enchanted Forest, a place where magic thrived and secrets whispered among the ancient trees.\n",
            "\n",
            "One sunny morning, fueled by her curiosity and a sense of adventure, Elara decided it was time to seek out the treasure. She packed a small satchel with her favorite snacks, a sturdy rope, and a bright blue gemstone that her grandmother had given her for protection. With a deep breath and a determined heart, she set off toward the Enchanted Forest.\n",
            "\n",
            "As Elara entered the forest, sunlight filtered through the thick canopy, casting a warm glow on the moss-covered ground. The air was filled with the scent of pine and wildflowers, and the sound of rustling leaves echoed like a soft melody. She followed a narrow path, her excitement growing with each step.\n",
            "\n",
            "Suddenly, Elara heard a soft whimpering sound. Curious, she followed the noise until she stumbled upon a small, injured fox caught in a tangle of thorns. Its bright eyes shimmered with fear, and Elara's heart ached for the creature. Without hesitation, she knelt beside the fox and carefully began to untangle the thorns, speaking soothing words to calm it.\n",
            "\n",
            "Once the fox was free, it looked up at Elara with gratitude, its eyes sparkling like the stars. To her amazement, the fox transformed before her eyes into a majestic creature, shimmering with silver fur and adorned with ethereal wings. “Thank you, kind-hearted girl,” the fox said, its voice melodic and warm. “I am a guardian of this forest, and your kindness has not gone unnoticed.”\n",
            "\n",
            "Elara's eyes widened in disbelief, but she quickly regained her composure. “I only helped because I couldn’t bear to see you in pain,” she replied, a smile spreading across her face.\n",
            "\n",
            "The guardian fox, now fully transformed, offered Elara a choice: she could continue her quest for the treasure or embark on a new adventure, guided by the wisdom of the forest. Elara thought for a moment, her heart racing with excitement. She realized that the real treasure was not gold or jewels but the friendships and experiences she could gain along the way.\n",
            "\n",
            "With a determined nod, Elara chose the path of adventure. Together with the guardian fox, she journeyed deeper into the Enchanted Forest, discovering magical creatures, solving riddles, and learning the secrets of nature. Each day brought new challenges and wonders, and Elara’s heart grew fuller with each experience.\n",
            "\n",
            "As the sun began to set one evening, casting a golden glow upon the forest, Elara knew she had found something far more valuable than treasure. She had discovered the magic of friendship, the beauty of kindness, and the courage to follow her heart.\n",
            "\n",
            "And so, Elara returned to her village, not with gold, but with stories and a spirit forever changed. The villagers listened in awe as she shared her adventures, inspiring them to seek their own paths of wonder.\n",
            "\n",
            "From that day on, Elara became a beacon of light in her village, reminding everyone that sometimes, the greatest treasures are the journeys we take and the connections we make along the way. And as for the guardian fox, it remained a loyal friend, always watching over Elara and the enchanted forest they both loved.\n",
            "\n",
            "And so, they lived happily ever after, surrounded by magic and adventure. The end.\n",
            "Cost: 0.00048585\n"
          ]
        }
      ],
      "source": [
        "response = client.completion(\n",
        "    prompt=\"Once upon a time\",\n",
        "    model=\"openai/gpt-4o-mini\"\n",
        ")\n",
        "\n",
        "print(\"Response:\", response[\"data\"])\n",
        "print(\"Cost:\", response[\"usage\"][\"cost\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "17b3ca0a",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Response: , in a small town nestled between rolling hills and a sparkling river, there lived a young girl named Lily. Lily was known for her kindness and her love for animals. She had a special bond with all the creatures in the town, from the tiniest ants to the tallest horses.\n",
            "\n",
            "One sunny morning, as Lily was walking through the meadow, she heard a faint whimpering sound. Following the sound, she discovered a small, injured puppy with a thorn stuck in its paw. The puppy looked up at Lily with big, sad eyes, and her heart melted.\n",
            "\n",
            "Without hesitation, Lily carefully picked up the puppy and cradled it in her arms. She gently removed the thorn and bandaged the tiny paw with a piece of cloth from her dress. The puppy licked her hand in gratitude, and Lily knew she had made a new friend.\n",
            "\n",
            "She named the puppy Sunny because his golden fur reminded her of the bright sun. From that day on, Sunny followed Lily everywhere. They played in the meadows, splashed in the river, and even helped other animals in need together.\n",
            "\n",
            "One day, as they were exploring the forest, they heard a loud rustling in the bushes. Out came a frightened deer with a tangled vine around its leg. Lily and Sunny worked together to free the deer. The deer nuzzled Lily’s hand before bounding away into the trees.\n",
            "\n",
            "Word of Lily and Sunny’s kindness spread throughout the town. People began to bring their injured or lost animals to Lily, knowing she would care for them with love and compassion. Lily’s home soon became a sanctuary for all sorts of creatures, from birds with broken wings to rabbits with scraped paws.\n",
            "\n",
            "Years passed, and Lily grew up to be the town’s beloved veterinarian. Sunny, now a big, strong dog, was always by her side, helping to comfort the animals that came to them. Together, they made sure that every animal in the town was loved and cared for.\n",
            "\n",
            "And so, Lily and Sunny lived happily ever after, surrounded by the love and gratitude of all the animals they had helped. Their story became a legend in the town, reminding everyone that kindness and compassion can create miracles.\n",
            "\n",
            "The End.\n",
            "Cost: 0.000893\n"
          ]
        }
      ],
      "source": [
        "response = client.completion(\n",
        "    prompt=\"Once upon a time\",\n",
        "    model=\"deepseek/deepseek-chat\",\n",
        ")\n",
        "\n",
        "print(\"Response:\", response[\"data\"])\n",
        "print(\"Cost:\", response[\"usage\"][\"cost\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "\n",
        "### Text Completion with Parameters\n",
        "\n",
        "Use different parameters for text completion:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Response: Sure! Here’s a delicious chocolate cake recipe that’s perfect for any occasion.\n",
            "\n",
            "### Classic Chocolate Cake Recipe\n",
            "\n",
            "#### Ingredients\n",
            "\n",
            "**For the Cake:**\n",
            "- 1 ¾ cups (220g) all-purpose flour\n",
            "- ¾ cup (65g) unsweetened cocoa powder\n",
            "- 2 cups (400g) granulated sugar\n",
            "- 1 ½ teaspoons baking powder\n",
            "- 1 ½ teaspoons baking soda\n",
            "- 1 teaspoon salt\n",
            "- 2 large eggs\n",
            "- 1 cup (240ml) whole milk\n",
            "- ½ cup (120ml) vegetable oil\n",
            "- 2 teaspoons vanilla extract\n",
            "- 1 cup (240ml) boiling water\n",
            "\n",
            "**For the Chocolate Frosting:**\n",
            "- 1 cup (230g) unsalted butter, softened\n",
            "- 3 ½ cups (440g) powdered sugar\n",
            "- ½ cup (45g) unsweetened cocoa powder\n",
            "- ½ teaspoon salt\n",
            "- 2 teaspoons vanilla extract\n",
            "- ¼ cup (60ml) whole milk (more if needed for consistency)\n",
            "\n",
            "#### Instructions\n",
            "\n",
            "**For the Cake:**\n",
            "\n",
            "1. **Preheat the Oven**: Preheat your oven to 350°F (175°C). Grease and flour two 9-inch round cake pans or line them with parchment paper.\n",
            "\n",
            "2. **Mix Dry Ingredients**: In a large mixing bowl, sift together the flour, cocoa powder, sugar, baking powder, baking soda, and salt.\n",
            "\n",
            "3. **Add Wet Ingredients**: Add the eggs, milk, vegetable oil, and vanilla extract to the dry ingredients. Beat on medium speed for 2 minutes until well combined.\n",
            "\n",
            "4. **Incorporate Boiling Water**: Carefully stir in the boiling water (the batter will be thin). Mix until smooth.\n",
            "\n",
            "5. **Pour Batter**: Divide the batter evenly between the prepared cake pans.\n",
            "\n",
            "6. **Bake**: Bake in the preheated oven for 30-35 minutes, or until a toothpick inserted into the center comes out clean. \n",
            "\n",
            "7. **Cool**: Allow the cakes to cool in the pans for about 10 minutes, then turn them out onto wire racks to cool completely.\n",
            "\n",
            "**For the Chocolate Frosting:**\n",
            "\n",
            "1. **Beat Butter**: In a large mixing bowl, beat the softened butter until creamy.\n",
            "\n",
            "2. **Add Dry Ingredients**: Gradually add the powdered sugar and cocoa powder, mixing on low speed until well combined.\n",
            "\n",
            "3. **Add Liquid Ingredients**: Add salt, vanilla extract, and milk. Beat on medium speed for about 2 minutes until the frosting is light and fluffy. If the frosting is too thick, add a little more milk to achieve your desired consistency.\n",
            "\n",
            "**Assemble the Cake:**\n",
            "\n",
            "1. **Layer the Cake**: Place one cake layer on a serving plate. Spread a generous amount of frosting on top.\n",
            "\n",
            "2. **Add Second Layer**: Place the second layer on top and spread frosting over the top and sides of the cake.\n",
            "\n",
            "3. **Decorate**: Optionally, decorate with chocolate shavings, sprinkles, or fresh berries.\n",
            "\n",
            "4. **Serve**: Slice and enjoy your delicious homemade chocolate cake!\n",
            "\n",
            "### Tips:\n",
            "- For an extra chocolatey flavor, you can add chocolate chips to the cake batter.\n",
            "- Make sure all your ingredients are at room temperature for the best results.\n",
            "- This cake can be stored in an airtight container at room temperature for up to 3 days, or in the refrigerator for up to a week.\n",
            "\n",
            "Enjoy baking your chocolate cake!\n"
          ]
        }
      ],
      "source": [
        "response = client.completion(\n",
        "    prompt=\"Write a recipe for chocolate cake\",\n",
        "    model=\"mistral/mistral-large-latest\",\n",
        "    temperature=0.7,\n",
        "    max_tokens=1000\n",
        ")\n",
        "\n",
        "print(\"Response:\", response[\"data\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "\n",
        "### Streaming Text Completion\n",
        "\n",
        "Stream the response for a better user experience:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Streaming response:\n",
            " Quantum computing is a type of computing that uses the principles of quantum mechanics to process information. Unlike classical computers, which use bits (0s and 1s) to perform calculations, quantum computers use quantum bits, or qubits, which can exist in multiple states at once due to a property called superposition. This allows quantum computers to perform many calculations simultaneously, potentially solving certain problems much faster than classical computers. Additionally, qubits can be entangled, meaning the state of one qubit can depend on the state of another, even if they are far apart. This enables quantum computers to handle complex problems in fields like cryptography, optimization, and drug discovery more efficiently. However, quantum computers are still in the early stages of development and face challenges like maintaining qubit stability (coherence) and error correction.\n",
            "Streaming complete!\n"
          ]
        }
      ],
      "source": [
        "print(\"Streaming response:\")\n",
        "for chunk in client.completion(\n",
        "    prompt=\"Explain quantum computing in simple terms\",\n",
        "    model=\"deepseek/deepseek-chat\",\n",
        "    stream=True\n",
        "):\n",
        "    if isinstance(chunk, dict) and \"data\" in chunk:\n",
        "        print(chunk[\"data\"], end=\"\", flush=True)\n",
        "    else:\n",
        "        print(chunk, end=\"\", flush=True)\n",
        "print(\"\\nStreaming complete!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "\n",
        "## 3. Embeddings\n",
        "\n",
        "### Single Text Embedding\n",
        "\n",
        "Generate embeddings for a single text:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Embedding dimensions: 1536\n",
            "First 5 values: [-0.019143931567668915, -0.025292053818702698, -0.0017211713129654527, 0.01883450709283352, -0.03382139280438423]\n"
          ]
        }
      ],
      "source": [
        "response = client.embeddings(\n",
        "    text=\"Hello, world!\",  \n",
        "    model=\"openai/text-embedding-3-small\"\n",
        ")\n",
        "\n",
        "print(\"Embedding dimensions:\", len(response[\"data\"][0]))\n",
        "print(\"First 5 values:\", response[\"data\"][0][:5])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "\n",
        "### Multiple Text Embeddings\n",
        "\n",
        "Generate embeddings for multiple texts:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of embeddings: 3\n",
            "Dimensions of each embedding: 1536\n"
          ]
        }
      ],
      "source": [
        "response = client.embeddings(\n",
        "    text=[\"Hello, world!\", \"How are you?\", \"IndoxRouter is awesome!\"],\n",
        "    model=\"openai/text-embedding-3-small\"\n",
        ")\n",
        "\n",
        "print(\"Number of embeddings:\", len(response[\"data\"]))\n",
        "print(\"Dimensions of each embedding:\", len(response[\"data\"][0]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "\n",
        "### Using Different Embedding Models\n",
        "\n",
        "Try different embedding models:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "HTTP error response: {\n",
            "  \"detail\": \"API error: Error code: 404 - {'error': {'message': 'The model `mistral-embed` does not exist or you do not have access to it.', 'type': 'invalid_request_error', 'param': None, 'code': 'model_not_found'}}\"\n",
            "}\n"
          ]
        },
        {
          "ename": "ProviderError",
          "evalue": "Server error (500): API error: Error code: 404 - {'error': {'message': 'The model `mistral-embed` does not exist or you do not have access to it.', 'type': 'invalid_request_error', 'param': None, 'code': 'model_not_found'}}. URL: http://localhost:8000/api/v1/embeddings.\nRequest data: {\n  \"text\": [\n    \"Hello, world!\"\n  ],\n  \"model\": \"mistral-embed\",\n  \"additional_params\": {}\n}\nThis may indicate an issue with the server configuration or a problem with the provider service.",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
            "File \u001b[1;32mE:\\Codes\\indoxRouter\\indoxRouter\\client.py:180\u001b[0m, in \u001b[0;36mClient._request\u001b[1;34m(self, method, endpoint, data, stream)\u001b[0m\n\u001b[0;32m    178\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m response\n\u001b[1;32m--> 180\u001b[0m \u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    181\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m response\u001b[38;5;241m.\u001b[39mjson()\n",
            "File \u001b[1;32me:\\ANACONDA\\Lib\\site-packages\\requests\\models.py:1024\u001b[0m, in \u001b[0;36mResponse.raise_for_status\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1023\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m http_error_msg:\n\u001b[1;32m-> 1024\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m HTTPError(http_error_msg, response\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m)\n",
            "\u001b[1;31mHTTPError\u001b[0m: 500 Server Error: Internal Server Error for url: http://localhost:8000/api/v1/embeddings",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[1;31mProviderError\u001b[0m                             Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[14], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Cohere embeddings\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membeddings\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtext\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mHello, world!\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmistral-embed\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[0;32m      5\u001b[0m \u001b[43m)\u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCohere embedding dimensions:\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mlen\u001b[39m(response[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;241m0\u001b[39m]))\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCost:\u001b[39m\u001b[38;5;124m\"\u001b[39m, response[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124musage\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcost\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n",
            "File \u001b[1;32mE:\\Codes\\indoxRouter\\indoxRouter\\client.py:377\u001b[0m, in \u001b[0;36mClient.embeddings\u001b[1;34m(self, text, model, **kwargs)\u001b[0m\n\u001b[0;32m    369\u001b[0m         filtered_kwargs[key] \u001b[38;5;241m=\u001b[39m value\n\u001b[0;32m    371\u001b[0m data \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m    372\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m\"\u001b[39m: text \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(text, \u001b[38;5;28mlist\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m [text],\n\u001b[0;32m    373\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m: formatted_model,\n\u001b[0;32m    374\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124madditional_params\u001b[39m\u001b[38;5;124m\"\u001b[39m: filtered_kwargs,\n\u001b[0;32m    375\u001b[0m }\n\u001b[1;32m--> 377\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_request\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mPOST\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mEMBEDDING_ENDPOINT\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mE:\\Codes\\indoxRouter\\indoxRouter\\client.py:216\u001b[0m, in \u001b[0;36mClient._request\u001b[1;34m(self, method, endpoint, data, stream)\u001b[0m\n\u001b[0;32m    214\u001b[0m     \u001b[38;5;66;03m# Include the request data in the error message for better debugging\u001b[39;00m\n\u001b[0;32m    215\u001b[0m     request_data_str \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mdumps(data, indent\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m) \u001b[38;5;28;01mif\u001b[39;00m data \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNone\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m--> 216\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ProviderError(\n\u001b[0;32m    217\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mServer error (500): \u001b[39m\u001b[38;5;132;01m{\u001b[39;00merror_detail\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. URL: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00murl\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    218\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRequest data: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrequest_data_str\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    219\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThis may indicate an issue with the server configuration or a problem with the provider service.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    220\u001b[0m     )\n\u001b[0;32m    221\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    222\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ProviderError(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mProvider error (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstatus_code\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m): \u001b[39m\u001b[38;5;132;01m{\u001b[39;00merror_message\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
            "\u001b[1;31mProviderError\u001b[0m: Server error (500): API error: Error code: 404 - {'error': {'message': 'The model `mistral-embed` does not exist or you do not have access to it.', 'type': 'invalid_request_error', 'param': None, 'code': 'model_not_found'}}. URL: http://localhost:8000/api/v1/embeddings.\nRequest data: {\n  \"text\": [\n    \"Hello, world!\"\n  ],\n  \"model\": \"mistral-embed\",\n  \"additional_params\": {}\n}\nThis may indicate an issue with the server configuration or a problem with the provider service."
          ]
        }
      ],
      "source": [
        "# Cohere embeddings\n",
        "response = client.embeddings(\n",
        "    text=\"Hello, world!\",\n",
        "    model=\"mistral-embed\"\n",
        ")\n",
        "\n",
        "print(\"Cohere embedding dimensions:\", len(response[\"data\"][0]))\n",
        "print(\"Cost:\", response[\"usage\"][\"cost\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "\n",
        "## 4. Image Generation\n",
        "\n",
        "### Basic Image Generation\n",
        "\n",
        "Generate a simple image:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "response = client.images(\n",
        "    prompt=\"A beautiful sunset over the ocean\",\n",
        "    model=\"openai/dall-e-2\"\n",
        ")\n",
        "\n",
        "print(\"Image URL:\", response[\"data\"][0][\"url\"])\n",
        "print(\"Cost:\", response[\"usage\"][\"cost\"])\n",
        "\n",
        "# Display the image if in a notebook\n",
        "from IPython.display import Image, display\n",
        "display(Image(url=response[\"data\"][0][\"url\"]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "\n",
        "### Multiple Images\n",
        "\n",
        "Generate multiple images:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "response = client.images(\n",
        "    prompt=\"A futuristic city with flying cars\",\n",
        "    model=\"openai/dall-e-3\",\n",
        "    n=2\n",
        ")\n",
        "\n",
        "print(f\"Generated {len(response['data'])} images:\")\n",
        "for i, image in enumerate(response[\"data\"]):\n",
        "    print(f\"Image {i+1} URL: {image['url']}\")\n",
        "\n",
        "# Display the images if in a notebook\n",
        "from IPython.display import Image, display\n",
        "for image in response[\"data\"]:\n",
        "    display(Image(url=image[\"url\"]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "\n",
        "### Image Generation with Different Parameters\n",
        "\n",
        "Use different parameters for image generation:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "response = client.images(\n",
        "    prompt=\"A photorealistic portrait of a cyberpunk character\",\n",
        "    model=\"openai/dall-e-3\",\n",
        "    size=\"1024x1024\",\n",
        "    quality=\"hd\",\n",
        "    style=\"vivid\"\n",
        ")\n",
        "\n",
        "print(\"Image URL:\", response[\"data\"][0][\"url\"])\n",
        "print(\"Cost:\", response[\"usage\"][\"cost\"])\n",
        "\n",
        "# Display the image if in a notebook\n",
        "from IPython.display import Image, display\n",
        "display(Image(url=response[\"data\"][0][\"url\"]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "\n",
        "## 5. Model Information\n",
        "\n",
        "### List All Models\n",
        "\n",
        "Get information about all available models:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "HTTP error (no JSON response): 500 Server Error: Internal Server Error for url: http://localhost:8000/api/v1/models/\n"
          ]
        },
        {
          "ename": "ProviderError",
          "evalue": "Server error (500): 500 Server Error: Internal Server Error for url: http://localhost:8000/api/v1/models/. URL: http://localhost:8000/api/v1/models.\nRequest data: None\nThis may indicate an issue with the server configuration or a problem with the provider service.",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
            "File \u001b[1;32mE:\\Codes\\indoxRouter\\indoxRouter\\client.py:180\u001b[0m, in \u001b[0;36mClient._request\u001b[1;34m(self, method, endpoint, data, stream)\u001b[0m\n\u001b[0;32m    178\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m response\n\u001b[1;32m--> 180\u001b[0m \u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    181\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m response\u001b[38;5;241m.\u001b[39mjson()\n",
            "File \u001b[1;32me:\\ANACONDA\\Lib\\site-packages\\requests\\models.py:1024\u001b[0m, in \u001b[0;36mResponse.raise_for_status\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1023\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m http_error_msg:\n\u001b[1;32m-> 1024\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m HTTPError(http_error_msg, response\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m)\n",
            "\u001b[1;31mHTTPError\u001b[0m: 500 Server Error: Internal Server Error for url: http://localhost:8000/api/v1/models/",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[1;31mProviderError\u001b[0m                             Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[5], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m models \u001b[38;5;241m=\u001b[39m \u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodels\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAvailable providers:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m provider \u001b[38;5;129;01min\u001b[39;00m models[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mproviders\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n",
            "File \u001b[1;32mE:\\Codes\\indoxRouter\\indoxRouter\\client.py:439\u001b[0m, in \u001b[0;36mClient.models\u001b[1;34m(self, provider)\u001b[0m\n\u001b[0;32m    436\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m provider:\n\u001b[0;32m    437\u001b[0m     endpoint \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mMODEL_ENDPOINT\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mprovider\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m--> 439\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_request\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mGET\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mendpoint\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mE:\\Codes\\indoxRouter\\indoxRouter\\client.py:216\u001b[0m, in \u001b[0;36mClient._request\u001b[1;34m(self, method, endpoint, data, stream)\u001b[0m\n\u001b[0;32m    214\u001b[0m     \u001b[38;5;66;03m# Include the request data in the error message for better debugging\u001b[39;00m\n\u001b[0;32m    215\u001b[0m     request_data_str \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mdumps(data, indent\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m) \u001b[38;5;28;01mif\u001b[39;00m data \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNone\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m--> 216\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ProviderError(\n\u001b[0;32m    217\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mServer error (500): \u001b[39m\u001b[38;5;132;01m{\u001b[39;00merror_detail\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. URL: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00murl\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    218\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRequest data: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrequest_data_str\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    219\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThis may indicate an issue with the server configuration or a problem with the provider service.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    220\u001b[0m     )\n\u001b[0;32m    221\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    222\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ProviderError(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mProvider error (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstatus_code\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m): \u001b[39m\u001b[38;5;132;01m{\u001b[39;00merror_message\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
            "\u001b[1;31mProviderError\u001b[0m: Server error (500): 500 Server Error: Internal Server Error for url: http://localhost:8000/api/v1/models/. URL: http://localhost:8000/api/v1/models.\nRequest data: None\nThis may indicate an issue with the server configuration or a problem with the provider service."
          ]
        }
      ],
      "source": [
        "models = client.models()\n",
        "\n",
        "print(\"Available providers:\")\n",
        "for provider in models[\"providers\"]:\n",
        "    print(f\"- {provider['name']} ({provider['id']})\")\n",
        "    print(f\"  Capabilities: {', '.join(provider['capabilities'])}\")\n",
        "    print(f\"  Models: {len(provider['models'])}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "\n",
        "### List Models for a Specific Provider\n",
        "\n",
        "Get models for a specific provider:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "HTTP error (no JSON response): 500 Server Error: Internal Server Error for url: http://localhost:8000/api/v1/models/openai\n"
          ]
        },
        {
          "ename": "ProviderError",
          "evalue": "Server error (500): 500 Server Error: Internal Server Error for url: http://localhost:8000/api/v1/models/openai. URL: http://localhost:8000/api/v1/models/openai.\nRequest data: None\nThis may indicate an issue with the server configuration or a problem with the provider service.",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
            "File \u001b[1;32mE:\\Codes\\indoxRouter\\indoxRouter\\client.py:180\u001b[0m, in \u001b[0;36mClient._request\u001b[1;34m(self, method, endpoint, data, stream)\u001b[0m\n\u001b[0;32m    178\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m response\n\u001b[1;32m--> 180\u001b[0m \u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    181\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m response\u001b[38;5;241m.\u001b[39mjson()\n",
            "File \u001b[1;32me:\\ANACONDA\\Lib\\site-packages\\requests\\models.py:1024\u001b[0m, in \u001b[0;36mResponse.raise_for_status\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1023\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m http_error_msg:\n\u001b[1;32m-> 1024\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m HTTPError(http_error_msg, response\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m)\n",
            "\u001b[1;31mHTTPError\u001b[0m: 500 Server Error: Internal Server Error for url: http://localhost:8000/api/v1/models/openai",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[1;31mProviderError\u001b[0m                             Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[6], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m openai_models \u001b[38;5;241m=\u001b[39m \u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodels\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mopenai\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOpenAI models (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(openai_models[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodels\u001b[39m\u001b[38;5;124m'\u001b[39m])\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m):\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m model \u001b[38;5;129;01min\u001b[39;00m openai_models[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodels\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n",
            "File \u001b[1;32mE:\\Codes\\indoxRouter\\indoxRouter\\client.py:439\u001b[0m, in \u001b[0;36mClient.models\u001b[1;34m(self, provider)\u001b[0m\n\u001b[0;32m    436\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m provider:\n\u001b[0;32m    437\u001b[0m     endpoint \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mMODEL_ENDPOINT\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mprovider\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m--> 439\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_request\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mGET\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mendpoint\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mE:\\Codes\\indoxRouter\\indoxRouter\\client.py:216\u001b[0m, in \u001b[0;36mClient._request\u001b[1;34m(self, method, endpoint, data, stream)\u001b[0m\n\u001b[0;32m    214\u001b[0m     \u001b[38;5;66;03m# Include the request data in the error message for better debugging\u001b[39;00m\n\u001b[0;32m    215\u001b[0m     request_data_str \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mdumps(data, indent\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m) \u001b[38;5;28;01mif\u001b[39;00m data \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNone\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m--> 216\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ProviderError(\n\u001b[0;32m    217\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mServer error (500): \u001b[39m\u001b[38;5;132;01m{\u001b[39;00merror_detail\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. URL: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00murl\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    218\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRequest data: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrequest_data_str\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    219\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThis may indicate an issue with the server configuration or a problem with the provider service.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    220\u001b[0m     )\n\u001b[0;32m    221\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    222\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ProviderError(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mProvider error (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstatus_code\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m): \u001b[39m\u001b[38;5;132;01m{\u001b[39;00merror_message\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
            "\u001b[1;31mProviderError\u001b[0m: Server error (500): 500 Server Error: Internal Server Error for url: http://localhost:8000/api/v1/models/openai. URL: http://localhost:8000/api/v1/models/openai.\nRequest data: None\nThis may indicate an issue with the server configuration or a problem with the provider service."
          ]
        }
      ],
      "source": [
        "openai_models = client.models(\"openai\")\n",
        "\n",
        "print(f\"OpenAI models ({len(openai_models['models'])}):\")\n",
        "for model in openai_models[\"models\"]:\n",
        "    print(f\"- {model['name']} ({model['id']})\")\n",
        "    print(f\"  Capabilities: {', '.join(model['capabilities'])}\")\n",
        "    if \"pricing\" in model:\n",
        "        print(f\"  Pricing: Input ${model['pricing']['input']}/1K tokens, Output ${model['pricing']['output']}/1K tokens\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "\n",
        "### Get Specific Model Information\n",
        "\n",
        "Get detailed information about a specific model:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model_info = client.get_model_info(\"openai\", \"gpt-4o-mini\")\n",
        "\n",
        "print(f\"Model: {model_info['name']} ({model_info['id']})\")\n",
        "print(f\"Provider: {model_info['provider']}\")\n",
        "print(f\"Description: {model_info['description']}\")\n",
        "print(f\"Capabilities: {', '.join(model_info['capabilities'])}\")\n",
        "print(f\"Input price: ${model_info['pricing']['input']}/1K tokens\")\n",
        "print(f\"Output price: ${model_info['pricing']['output']}/1K tokens\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "\n",
        "## 6. Usage Statistics\n",
        "\n",
        "### Get Usage Statistics\n",
        "\n",
        "Get usage statistics for the current user:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "HTTP error response: {\n",
            "  \"detail\": \"Not Found\"\n",
            "}\n"
          ]
        },
        {
          "ename": "NetworkError",
          "evalue": "Resource not found: Not Found (URL: http://localhost:8000/api/v1/usage)",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
            "File \u001b[1;32mE:\\Codes\\indoxRouter\\indoxRouter\\client.py:180\u001b[0m, in \u001b[0;36mClient._request\u001b[1;34m(self, method, endpoint, data, stream)\u001b[0m\n\u001b[0;32m    178\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m response\n\u001b[1;32m--> 180\u001b[0m \u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    181\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m response\u001b[38;5;241m.\u001b[39mjson()\n",
            "File \u001b[1;32me:\\ANACONDA\\Lib\\site-packages\\requests\\models.py:1024\u001b[0m, in \u001b[0;36mResponse.raise_for_status\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1023\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m http_error_msg:\n\u001b[1;32m-> 1024\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m HTTPError(http_error_msg, response\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m)\n",
            "\u001b[1;31mHTTPError\u001b[0m: 404 Client Error: Not Found for url: http://localhost:8000/api/v1/usage",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[1;31mNetworkError\u001b[0m                              Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[7], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m usage \u001b[38;5;241m=\u001b[39m \u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_usage\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUsage statistics:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTotal requests: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00musage[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtotal_requests\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
            "File \u001b[1;32mE:\\Codes\\indoxRouter\\indoxRouter\\client.py:461\u001b[0m, in \u001b[0;36mClient.get_usage\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    454\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_usage\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Dict[\u001b[38;5;28mstr\u001b[39m, Any]:\n\u001b[0;32m    455\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    456\u001b[0m \u001b[38;5;124;03m    Get usage statistics for the current user.\u001b[39;00m\n\u001b[0;32m    457\u001b[0m \n\u001b[0;32m    458\u001b[0m \u001b[38;5;124;03m    Returns:\u001b[39;00m\n\u001b[0;32m    459\u001b[0m \u001b[38;5;124;03m        Usage statistics\u001b[39;00m\n\u001b[0;32m    460\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 461\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_request\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mGET\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mUSAGE_ENDPOINT\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mE:\\Codes\\indoxRouter\\indoxRouter\\client.py:202\u001b[0m, in \u001b[0;36mClient._request\u001b[1;34m(self, method, endpoint, data, stream)\u001b[0m\n\u001b[0;32m    200\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m ModelNotFoundError(error_message)\n\u001b[0;32m    201\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 202\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m NetworkError(\n\u001b[0;32m    203\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mResource not found: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00merror_message\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m (URL: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00murl\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    204\u001b[0m         )\n\u001b[0;32m    205\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m status_code \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m429\u001b[39m:\n\u001b[0;32m    206\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m RateLimitError(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRate limit exceeded: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00merror_message\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
            "\u001b[1;31mNetworkError\u001b[0m: Resource not found: Not Found (URL: http://localhost:8000/api/v1/usage)"
          ]
        }
      ],
      "source": [
        "usage = client.get_usage()\n",
        "\n",
        "print(\"Usage statistics:\")\n",
        "print(f\"Total requests: {usage['total_requests']}\")\n",
        "print(f\"Total cost: ${usage['total_cost']}\")\n",
        "print(f\"Remaining credits: ${usage['remaining_credits']}\")\n",
        "\n",
        "print(\"\\nBreakdown by endpoint:\")\n",
        "for endpoint, stats in usage[\"endpoints\"].items():\n",
        "    print(f\"- {endpoint}: {stats['requests']} requests, ${stats['cost']} cost\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "\n",
        "## 7. Error Handling\n",
        "\n",
        "### Handle Different Errors\n",
        "\n",
        "Handle different types of errors gracefully:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Function to demonstrate error handling\n",
        "def try_request(func, *args, **kwargs):\n",
        "    try:\n",
        "        return func(*args, **kwargs)\n",
        "    except ModelNotFoundError as e:\n",
        "        print(f\"Model not found: {e}\")\n",
        "    except ProviderNotFoundError as e:\n",
        "        print(f\"Provider not found: {e}\")\n",
        "    except InsufficientCreditsError as e:\n",
        "        print(f\"Insufficient credits: {e}\")\n",
        "    except InvalidParametersError as e:\n",
        "        print(f\"Invalid parameters: {e}\")\n",
        "    except RateLimitError as e:\n",
        "        print(f\"Rate limit exceeded: {e}\")\n",
        "    except ProviderError as e:\n",
        "        print(f\"Provider error: {e}\")\n",
        "    except NetworkError as e:\n",
        "        print(f\"Network error: {e}\")\n",
        "    except AuthenticationError as e:\n",
        "        print(f\"Authentication error: {e}\")\n",
        "    except IndoxRouterError as e:\n",
        "        print(f\"IndoxRouter error: {e}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Unexpected error: {e}\")\n",
        "    return None\n",
        "\n",
        "# Example: Model not found\n",
        "result = try_request(client.chat,\n",
        "    messages=[{\"role\": \"user\", \"content\": \"Hello\"}],\n",
        "    model=\"nonexistent/model\"\n",
        ")\n",
        "\n",
        "# Example: Invalid parameters\n",
        "result = try_request(client.chat,\n",
        "    messages=\"This is not a list of messages\",  # Should be a list\n",
        "    model=\"openai/gpt-4o-mini\"\n",
        ")\n",
        "\n",
        "# Example: Provider not found\n",
        "result = try_request(client.chat,\n",
        "    messages=[{\"role\": \"user\", \"content\": \"Hello\"}],\n",
        "    model=\"nonexistent/gpt-4o-mini\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "\n",
        "## 8. Advanced Usage\n",
        "\n",
        "### Using as a Context Manager\n",
        "\n",
        "Use the client as a context manager to automatically close the session:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "with Client(api_key=\"your_api_key\") as client:\n",
        "    response = client.chat(\n",
        "        messages=[{\"role\": \"user\", \"content\": \"Hello, how are you?\"}],\n",
        "        model=\"openai/gpt-4o-mini\"\n",
        "    )\n",
        "    print(\"Response:\", response[\"data\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "\n",
        "### Combining Different Capabilities\n",
        "\n",
        "Combine different capabilities for more complex use cases:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Example: Generate text and then create an image based on it\n",
        "completion_response = client.completion(\n",
        "    prompt=\"Describe a fantastical creature that has never been seen before.\",\n",
        "    model=\"openai/gpt-4o-mini\",\n",
        "    max_tokens=200\n",
        ")\n",
        "\n",
        "creature_description = completion_response[\"data\"]\n",
        "print(\"Generated description:\", creature_description)\n",
        "\n",
        "# Now create an image based on the description\n",
        "image_response = client.images(\n",
        "    prompt=f\"A detailed illustration of: {creature_description}\",\n",
        "    model=\"openai/dall-e-3\"\n",
        ")\n",
        "\n",
        "print(\"Image URL:\", image_response[\"data\"][0][\"url\"])\n",
        "\n",
        "# Display the image if in a notebook\n",
        "from IPython.display import Image, display\n",
        "display(Image(url=image_response[\"data\"][0][\"url\"]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "\n",
        "### Semantic Search with Embeddings\n",
        "\n",
        "Implement a simple semantic search using embeddings:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "# Define some documents\n",
        "documents = [\n",
        "    \"The quick brown fox jumps over the lazy dog.\",\n",
        "    \"A fast auburn fox leaps above the sleepy canine.\",\n",
        "    \"IndoxRouter provides a unified API for various AI providers.\",\n",
        "    \"The API allows access to multiple AI models through a single interface.\",\n",
        "    \"Paris is the capital of France and known for the Eiffel Tower.\",\n",
        "    \"Rome is the capital of Italy and home to the Colosseum.\"\n",
        "]\n",
        "\n",
        "# Generate embeddings for all documents\n",
        "embeddings_response = client.embeddings(\n",
        "    text=documents,\n",
        "    model=\"openai/text-embedding-ada-002\"\n",
        ")\n",
        "\n",
        "document_embeddings = embeddings_response[\"data\"]\n",
        "\n",
        "# Function to calculate cosine similarity\n",
        "def cosine_similarity(a, b):\n",
        "    return np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b))\n",
        "\n",
        "# Function to find most similar documents\n",
        "def semantic_search(query, document_embeddings, documents, top_n=2):\n",
        "    # Get embedding for the query\n",
        "    query_embedding_response = client.embeddings(\n",
        "        text=query,\n",
        "        model=\"openai/text-embedding-ada-002\"\n",
        "    )\n",
        "    query_embedding = query_embedding_response[\"data\"][0]\n",
        "\n",
        "    # Calculate similarities\n",
        "    similarities = [\n",
        "        cosine_similarity(query_embedding, doc_embedding)\n",
        "        for doc_embedding in document_embeddings\n",
        "    ]\n",
        "\n",
        "    # Get top N results\n",
        "    top_indices = np.argsort(similarities)[-top_n:][::-1]\n",
        "\n",
        "    return [\n",
        "        {\"document\": documents[i], \"similarity\": similarities[i]}\n",
        "        for i in top_indices\n",
        "    ]\n",
        "\n",
        "# Example search\n",
        "results = semantic_search(\"What is IndoxRouter?\", document_embeddings, documents)\n",
        "\n",
        "print(\"Search results:\")\n",
        "for i, result in enumerate(results):\n",
        "    print(f\"{i+1}. {result['document']} (Similarity: {result['similarity']:.4f})\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "\n",
        "### RAG (Retrieval-Augmented Generation)\n",
        "\n",
        "Implement a simple RAG system:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Using the same documents and embeddings from the previous example\n",
        "\n",
        "def rag_query(query, document_embeddings, documents, top_n=2):\n",
        "    # Get relevant documents\n",
        "    relevant_docs = semantic_search(query, document_embeddings, documents, top_n)\n",
        "\n",
        "    # Create a context from the relevant documents\n",
        "    context = \"\\n\".join([doc[\"document\"] for doc in relevant_docs])\n",
        "\n",
        "    # Create a prompt with the context\n",
        "    prompt = f\"\"\"\n",
        "    Context information:\n",
        "    {context}\n",
        "\n",
        "    Based on the context information, please answer the following question:\n",
        "    {query}\n",
        "    \"\"\"\n",
        "\n",
        "    # Generate a response\n",
        "    response = client.chat(\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": \"You are a helpful assistant. Answer the question based only on the provided context.\"},\n",
        "            {\"role\": \"user\", \"content\": prompt}\n",
        "        ],\n",
        "        model=\"openai/gpt-4o-mini\"\n",
        "    )\n",
        "\n",
        "    return response[\"data\"]\n",
        "\n",
        "# Example RAG query\n",
        "answer = rag_query(\"What does IndoxRouter do?\", document_embeddings, documents)\n",
        "print(\"RAG Answer:\", answer)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "\n",
        "## 9. Troubleshooting and Debugging\n",
        "\n",
        "### Enable Debug Mode\n",
        "\n",
        "Enable debug logging to see detailed information about requests and responses:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Enable debug logging\n",
        "client.enable_debug()\n",
        "\n",
        "# Try a request\n",
        "try:\n",
        "    response = client.chat(\n",
        "        messages=[{\"role\": \"user\", \"content\": \"Hello\"}],\n",
        "        model=\"openai/gpt-4o-mini\"\n",
        "    )\n",
        "    print(\"Success!\")\n",
        "except Exception as e:\n",
        "    print(f\"Error: {e}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "\n",
        "### Testing Server Connection\n",
        "\n",
        "Use the `test_connection` method to verify that your server is accessible and properly configured:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test the connection to the server\n",
        "connection_info = client.test_connection()\n",
        "print(f\"Connection status: {connection_info['status']}\")\n",
        "\n",
        "if connection_info['status'] == 'connected':\n",
        "    print(f\"Server URL: {connection_info['url']}\")\n",
        "    print(f\"Status code: {connection_info['status_code']}\")\n",
        "    if connection_info['server_info']:\n",
        "        print(f\"Server info: {connection_info['server_info']}\")\n",
        "else:\n",
        "    print(f\"Error: {connection_info['error']}\")\n",
        "    print(f\"Error type: {connection_info['error_type']}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "\n",
        "### Common Issues and Solutions\n",
        "\n",
        "#### \"Resource not found\" Error\n",
        "\n",
        "If you see a \"Resource not found\" error, it usually means one of the following:\n",
        "\n",
        "1. The server is not running. Make sure your IndoxRouter server is up and running:\n",
        "\n",
        "   ```bash\n",
        "   cd indoxRouter_server\n",
        "   python -m main\n",
        "   ```\n",
        "\n",
        "2. The base URL is incorrect. Check the URL in your environment:\n",
        "\n",
        "   "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(f\"Using base URL: {client.base_url}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1d3d019a",
      "metadata": {},
      "source": [
        "\n",
        "\n",
        "3. The API endpoint path is incorrect. The client automatically adds the API version prefix, but you can check the full URL in the debug logs.\n",
        "\n",
        "#### Server Errors (500 Internal Server Error)\n",
        "\n",
        "If you encounter a 500 Internal Server Error:\n",
        "\n",
        "1. Check the server logs for detailed error information:\n",
        "\n",
        "   ```bash\n",
        "   # Look at the server logs\n",
        "   cd indoxRouter_server\n",
        "   tail -f logs/server.log\n",
        "   ```\n",
        "\n",
        "2. Verify that the provider service is available and properly configured on the server.\n",
        "\n",
        "3. Check if your request parameters are valid for the specific model you're using.\n",
        "\n",
        "4. Try with a different model or provider to see if the issue is specific to one provider.\n",
        "\n",
        "5. If you see a \"too many values to unpack\" error, it might be related to how the server parses the model string. The client now automatically formats the model string to be compatible with the server, but you can try different formats:\n",
        "\n",
        "   "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c8737736",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Try using a different model format\n",
        "   try:\n",
        "       # First attempt with standard format\n",
        "       response = client.chat(\n",
        "           messages=[{\"role\": \"user\", \"content\": \"Hello\"}],\n",
        "           model=\"openai/gpt-4o-mini\"\n",
        "       )\n",
        "   except ProviderError as e:\n",
        "       if \"too many values to unpack\" in str(e):\n",
        "           # Try with a different provider/model\n",
        "           response = client.chat(\n",
        "               messages=[{\"role\": \"user\", \"content\": \"Hello\"}],\n",
        "               model=\"anthropic/claude-3-haiku\"\n",
        "           )"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "946ff17d",
      "metadata": {},
      "source": [
        "\n",
        "\n",
        "#### Authentication Errors\n",
        "\n",
        "If you see an \"Authentication failed\" error:\n",
        "\n",
        "1. Make sure your API key is correct:\n",
        "\n",
        "   "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "API key starts with: indox...\n"
          ]
        }
      ],
      "source": [
        "# First few characters of the API key (for security)\n",
        "print(f\"API key starts with: {client.api_key[:5]}...\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "\n",
        "2. Check if your API key is valid on the server.\n",
        "\n",
        "#### Connection Errors\n",
        "\n",
        "If you can't connect to the server:\n",
        "\n",
        "1. Use the `test_connection` method to diagnose the issue:\n",
        "\n",
        "   "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Connection status: {'status': 'connected', 'url': 'http://localhost:8000', 'status_code': 200, 'server_info': {'status': 'ok', 'message': 'IndoxRouter Server is running'}, 'headers': {'date': 'Sat, 12 Apr 2025 14:52:31 GMT', 'server': 'uvicorn', 'content-length': '57', 'content-type': 'application/json', 'x-content-type-options': 'nosniff', 'x-frame-options': 'DENY', 'x-xss-protection': '1; mode=block', 'strict-transport-security': 'max-age=31536000; includeSubDomains'}}\n"
          ]
        }
      ],
      "source": [
        "connection_info = client.test_connection()\n",
        "print(f\"Connection status: {connection_info}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "\n",
        "2. Check for firewall or network issues.\n",
        "\n",
        "#### Insufficient Credits\n",
        "\n",
        "If you see an \"Insufficient credits\" error:\n",
        "\n",
        "1. Check your current credit balance:\n",
        "\n",
        "   "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "HTTP error response: {\n",
            "  \"detail\": \"Not Found\"\n",
            "}\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Error getting usage: Resource not found: Not Found (URL: http://localhost:8000/api/v1/usage)\n"
          ]
        }
      ],
      "source": [
        "try:\n",
        "      usage = client.get_usage()\n",
        "      print(f\"Remaining credits: ${usage['remaining_credits']}\")\n",
        "except Exception as e:\n",
        "      print(f\"Error getting usage: {e}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "\n",
        "2. Contact your administrator to add more credits to your account.\n",
        "\n",
        "## 10. Cleanup\n",
        "\n",
        "Don't forget to close the client when you're done:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "client.close()\n",
        "print(\"Client closed successfully!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "\n",
        "## Conclusion\n",
        "\n",
        "This cookbook has demonstrated how to use the IndoxRouter client to interact with various AI providers through a unified API. You can now use these examples as a starting point for your own applications.\n",
        "\n",
        "For more information, refer to the [IndoxRouter documentation](https://docs.indoxrouter.com).\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
