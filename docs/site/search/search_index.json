{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"IndoxRouter","text":"<p>A unified Python client for accessing multiple AI providers through a single, consistent API. Switch between OpenAI, Anthropic, Google, Mistral, DeepSeek, XAI, and Qwen models seamlessly without changing your code.</p>"},{"location":"#installation","title":"Installation","text":"<pre><code>pip install indoxrouter\n</code></pre>"},{"location":"#quick-start","title":"Quick Start","text":""},{"location":"#initialize-the-client","title":"Initialize the Client","text":"<pre><code>from indoxrouter import Client\n\n# Initialize with API key\nclient = Client(api_key=\"your_api_key\")\n\n# Or use environment variable INDOX_ROUTER_API_KEY\nclient = Client()\n</code></pre>"},{"location":"#chat-completion-example","title":"Chat Completion Example","text":"<pre><code>response = client.chat(\n    messages=[\n        {\"role\": \"user\", \"content\": \"Tell me a story about a robot in 5 sentences.\"}\n    ],\n    model=\"deepseek/deepseek-chat\"\n)\n\nprint(response['data'])\nprint(f\"Cost: ${response['usage']['cost']}\")\nprint(f\"Tokens used: {response['usage']['tokens_total']}\")\n</code></pre>"},{"location":"#byok-bring-your-own-key-support","title":"BYOK (Bring Your Own Key) Support","text":"<p>IndoxRouter supports BYOK, allowing you to use your own API keys for AI providers instead of using the platform's shared keys. This provides several benefits:</p> <ul> <li>No credit deduction from your IndoxRouter account</li> <li>No rate limiting from the platform</li> <li>Direct provider access with your own API keys</li> <li>Cost control - you pay providers directly at their rates</li> </ul>"},{"location":"#using-byok","title":"Using BYOK","text":"<pre><code># Use your own OpenAI API key\nresponse = client.chat(\n    messages=[{\"role\": \"user\", \"content\": \"Hello!\"}],\n    model=\"openai/gpt-4\",\n    byok_api_key=\"sk-your-openai-key-here\"\n)\n\n# Use your own Google API key for image generation\nresponse = client.images(\n    prompt=\"A beautiful sunset\",\n    model=\"google/imagen-3.0-generate-002\",\n    byok_api_key=\"your-google-api-key\"\n)\n\n# Use your own API key for embeddings\nresponse = client.embeddings(\n    text=\"Sample text for embedding\",\n    model=\"openai/text-embedding-ada-002\",\n    byok_api_key=\"sk-your-openai-key-here\"\n)\n</code></pre>"},{"location":"#byok-benefits","title":"BYOK Benefits","text":"<ul> <li>Cost Savings: No platform markup on API calls</li> <li>Higher Limits: Use provider's native rate limits</li> <li>Direct Billing: Pay providers directly at their rates</li> <li>Full Control: Access to all provider-specific features</li> <li>No Platform Dependencies: Works even if IndoxRouter is down</li> </ul>"},{"location":"#supported-endpoints","title":"Supported Endpoints","text":"<p>All AI endpoints support BYOK:</p> <ul> <li>\u2705 Chat completions (<code>client.chat()</code>)</li> <li>\u2705 Text completions (<code>client.completion()</code>)</li> <li>\u2705 Embeddings (<code>client.embeddings()</code>)</li> <li>\u2705 Image generation (<code>client.images()</code>)</li> <li>\u2705 Video generation (<code>client.videos()</code>)</li> <li>\u2705 Text-to-speech (<code>client.text_to_speech()</code>)</li> </ul>"},{"location":"#response-format","title":"Response Format","text":"<p>Every response includes detailed usage information:</p> <pre><code>{\n    'request_id': 'c08cc108-6b0d-48bd-a660-546143f1b9fa',\n    'created_at': '2025-05-19T06:07:38.077269',\n    'duration_ms': 9664.651870727539,\n    'provider': 'deepseek',\n    'model': 'deepseek-chat',\n    'success': True,\n    'message': '',\n    'usage': {\n        'tokens_prompt': 15,\n        'tokens_completion': 107,\n        'tokens_total': 122,\n        'cost': 0.000229,\n        'latency': 9.487398862838745,\n        'timestamp': '2025-05-19T06:07:38.065330'\n    },\n    'data': 'Your AI response text here...',\n    'finish_reason': None\n}\n</code></pre>"},{"location":"#usage-tracking","title":"Usage Tracking","text":"<p>Monitor your usage and costs:</p> <pre><code># Get detailed usage statistics\nusage = client.get_usage()\nprint(f\"Total requests: {usage['total_requests']}\")\nprint(f\"Total cost: ${usage['total_cost']}\")\nprint(f\"Remaining credits: ${usage['remaining_credits']}\")\n</code></pre>"},{"location":"#model-information","title":"Model Information","text":"<p>Get detailed information about available models:</p> <pre><code># Get specific model info\nmodel_info = client.get_model_info(provider=\"openai\", model=\"gpt-4o-mini\")\nprint(f\"Context window: {model_info['specs']['context_window']}\")\nprint(f\"Capabilities: {model_info['capabilities']}\")\n\n# List all available models\nmodels = client.models()\nfor provider in models:\n    print(f\"Provider: {provider['name']}\")\n    for model in provider.get('text_completions', []):\n        print(f\"  - {model['modelName']}\")\n</code></pre>"},{"location":"#using-with-openai-sdk","title":"Using with OpenAI SDK","text":"<p>You can also use the OpenAI SDK with IndoxRouter's base URL:</p> <pre><code>from openai import OpenAI\n\nclient = OpenAI(\n    api_key=\"your_indoxrouter_api_key\",\n    base_url=\"https://api.indoxrouter.com\"\n)\n\nresponse = client.chat.completions.create(\n    model=\"anthropic/claude-3-haiku-20240307\",\n    messages=[{\"role\": \"user\", \"content\": \"Hello!\"}]\n)\n</code></pre>"},{"location":"#examples-by-use-case","title":"Examples by Use Case","text":""},{"location":"#cost-optimized-chat","title":"Cost-Optimized Chat","text":"<pre><code># Use fast, cost-effective models for high-volume applications\nresponse = client.chat(\n    messages=[{\"role\": \"user\", \"content\": \"Summarize this text...\"}],\n    model=\"openai/gpt-3.5-turbo\",  # Most cost-effective\n    max_tokens=100\n)\n</code></pre>"},{"location":"#high-quality-analysis","title":"High-Quality Analysis","text":"<pre><code># Use premium models for complex reasoning\nresponse = client.chat(\n    messages=[{\"role\": \"user\", \"content\": \"Analyze this complex problem...\"}],\n    model=\"anthropic/claude-3-opus-20240229\",  # Highest quality\n    temperature=0.1  # More focused responses\n)\n</code></pre>"},{"location":"#code-generation","title":"Code Generation","text":"<pre><code># Use specialized coding models\nresponse = client.chat(\n    messages=[{\"role\": \"user\", \"content\": \"Write a Python function to...\"}],\n    model=\"deepseek/deepseek-coder\",  # Optimized for coding\n    temperature=0.0  # Deterministic code\n)\n</code></pre>"},{"location":"#vision-image-analysis","title":"Vision &amp; Image Analysis","text":"<pre><code>import base64\n\n# Analyze images with vision-capable models\nwith open(\"photo.jpg\", \"rb\") as f:\n    image_base64 = base64.b64encode(f.read()).decode('utf-8')\n\nresponse = client.chat(\n    messages=[\n        {\n            \"role\": \"user\",\n            \"content\": [\n                {\"type\": \"text\", \"text\": \"What's in this image?\"},\n                {\n                    \"type\": \"image\",\n                    \"image\": {\n                        \"data\": image_base64,\n                        \"media_type\": \"image/jpeg\"\n                    }\n                }\n            ]\n        }\n    ],\n    model=\"openai/gpt-4o\"\n)\n\nprint(response['data'])\n</code></pre>"},{"location":"#image-generation","title":"Image Generation","text":"<pre><code># Generate images with different providers\nresponse = client.images(\n    prompt=\"A futuristic cityscape at sunset\",\n    model=\"openai/dall-e-3\",\n    size=\"1024x1024\",\n    style=\"vivid\"\n)\n\nimage_url = response['data'][0]['url']\n</code></pre>"},{"location":"#video-generation","title":"Video Generation","text":"<pre><code># Generate videos (asynchronous process)\nresponse = client.videos(\n    prompt=\"A majestic eagle soaring over mountain peaks, cinematic camera movement\",\n    model=\"openai/sora-2\",\n    size=\"1280x720\",\n    duration=4\n)\n\njob_id = response['data']['job_id']\n\n# Check job status\nstatus_response = client.jobs(job_id=job_id)\nif status_response['status'] == 'completed':\n    video_url = status_response['result']['data'][0]['url']\n</code></pre>"},{"location":"#rate-limits","title":"Rate Limits","text":"<p>IndoxRouter has three tiers with different rate limits:</p> Tier Requests/Minute Tokens/Hour Best For Free 10 10,000 Testing &amp; prototyping Standard 60 100,000 Production applications Enterprise 500 1,000,000 High-volume applications <p>Rate limit information is included in error responses when limits are exceeded.</p>"},{"location":"#next-steps","title":"Next Steps","text":"<ul> <li>Getting Started: Detailed setup guide</li> <li>Usage Examples: Comprehensive usage examples</li> <li>API Reference: Full API documentation</li> </ul> <p>Last updated: July 27, 2025</p>"},{"location":"getting-started/","title":"Getting Started","text":"<p>This guide will help you get started with the IndoxRouter library, showing you how to install the package, set up your API key, and make your first API call.</p>"},{"location":"getting-started/#installation","title":"Installation","text":"<p>To install the IndoxRouter, use pip:</p> <pre><code>pip install indoxrouter\n</code></pre>"},{"location":"getting-started/#setting-up-your-api-key","title":"Setting Up Your API Key","text":"<p>To use IndoxRouter, you need an API key from your IndoxRouter Server instance. There are several ways to configure your API key:</p>"},{"location":"getting-started/#method-1-directly-in-the-client-constructor","title":"Method 1: Directly in the Client constructor","text":"<pre><code>from indoxrouter import Client\n\nclient = Client(api_key=\"your_api_key\")\n</code></pre>"},{"location":"getting-started/#method-2-using-environment-variables","title":"Method 2: Using environment variables","text":"<p>You can set the <code>INDOX_ROUTER_API_KEY</code> environment variable and the client will use it automatically:</p> <pre><code># In your terminal or .env file\n# export INDOX_ROUTER_API_KEY=your_api_key\n\n# In your Python code\nfrom indoxrouter import Client\n\nclient = Client()  # Will use the environment variable\n</code></pre>"},{"location":"getting-started/#method-3-configuration-file","title":"Method 3: Configuration file","text":"<p>Coming soon: Support for loading configuration from a file.</p>"},{"location":"getting-started/#verifying-your-api-key","title":"Verifying Your API Key","text":"<p>You can verify that your API key is working correctly by using the test_connection method:</p> <pre><code>from indoxrouter import Client\n\nclient = Client(api_key=\"your_api_key\")\ntry:\n    result = client.test_connection()\n    print(\"Connection successful!\")\n    print(f\"Connected to server version: {result.get('version', 'unknown')}\")\nexcept Exception as e:\n    print(f\"Connection failed: {str(e)}\")\n</code></pre>"},{"location":"getting-started/#making-your-first-api-call","title":"Making Your First API Call","text":"<p>Here's a simple example of making a chat completion request:</p> <pre><code>from indoxrouter import Client\n\n# Initialize the client\nclient = Client(api_key=\"your_api_key\")\n\n# Make a chat completion request\nresponse = client.chat(\n    messages=[\n        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n        {\"role\": \"user\", \"content\": \"Hello, how are you today?\"}\n    ],\n    model=\"openai/gpt-4o-mini\"\n)\n\n# Print the response\nprint(response[\"data\"])\n</code></pre>"},{"location":"getting-started/#error-handling","title":"Error Handling","text":"<p>IndoxRouter provides clear error handling. Here's an example of how to handle errors:</p> <pre><code>from indoxrouter import Client, ModelNotFoundError, ProviderError, AuthenticationError\n\ntry:\n    client = Client(api_key=\"your_api_key\")\n    response = client.chat(\n        messages=[{\"role\": \"user\", \"content\": \"Hello\"}],\n        model=\"nonexistent-provider/nonexistent-model\"\n    )\nexcept AuthenticationError as e:\n    print(f\"Authentication failed: {e}\")\nexcept ModelNotFoundError as e:\n    print(f\"Model not found: {e}\")\nexcept ProviderError as e:\n    print(f\"Provider error: {e}\")\nexcept Exception as e:\n    print(f\"Unexpected error: {e}\")\n</code></pre>"},{"location":"getting-started/#best-practices","title":"Best Practices","text":"<ul> <li>Set appropriate timeouts for your use case</li> <li>Handle errors appropriately for your application</li> <li>Consider using environment variables for API keys rather than hardcoding them</li> </ul>"},{"location":"getting-started/#next-steps","title":"Next Steps","text":"<p>Now that you're set up, check out the Usage Guide for more detailed information on working with the different API capabilities:</p> <ul> <li>Basic Usage</li> <li>Chat Completions</li> <li>Text Completions</li> <li>Embeddings</li> <li>Image Generation</li> <li>Video Generation</li> </ul>"},{"location":"api/client/","title":"Client API Reference","text":"<p>The <code>Client</code> class is the main entry point for interacting with the IndoxRouter API. This page documents all the methods and functionality provided by the client.</p>"},{"location":"api/client/#initialization","title":"Initialization","text":"<pre><code>from indoxrouter import Client\n\nclient = Client(\n    api_key=\"your_api_key\",\n    timeout=30\n)\n</code></pre>"},{"location":"api/client/#parameters","title":"Parameters","text":"<ul> <li><code>api_key</code> (<code>str</code>, optional): Your API key for authentication. If not provided, the client will look for the <code>INDOX_ROUTER_API_KEY</code> environment variable.</li> <li><code>timeout</code> (<code>int</code>, optional): Request timeout in seconds. Defaults to 30.</li> </ul>"},{"location":"api/client/#methods","title":"Methods","text":""},{"location":"api/client/#authentication","title":"Authentication","text":"<pre><code>def _authenticate(self):\n    \"\"\"\n    Authenticate with the server and get JWT tokens.\n    This uses the /auth/token endpoint to get JWT tokens using the API key.\n    \"\"\"\n</code></pre> <p>This method is called automatically during initialization. It exchanges the API key for JWT tokens that are used for subsequent requests.</p>"},{"location":"api/client/#chat-completions","title":"Chat Completions","text":"<pre><code>def chat(\n    self,\n    messages: List[Dict[str, str]],\n    model: str = DEFAULT_MODEL,\n    temperature: float = 0.7,\n    max_tokens: Optional[int] = None,\n    stream: bool = False,\n    **kwargs,\n) -&gt; Dict[str, Any]:\n    \"\"\"\n    Generate a chat completion.\n\n    Args:\n        messages: A list of message objects with role and content keys.\n        model: The model to use in format \"provider/model_name\".\n        temperature: Controls randomness. Higher values (e.g., 0.8) make output more random,\n                     lower values (e.g., 0.2) make it more deterministic.\n        max_tokens: Maximum number of tokens to generate.\n        stream: Whether to stream the response.\n        **kwargs: Additional keyword arguments to pass to the API.\n\n    Returns:\n        A dictionary containing the API response.\n    \"\"\"\n</code></pre>"},{"location":"api/client/#text-completions","title":"Text Completions","text":"<pre><code>def completion(\n    self,\n    prompt: str,\n    model: str = DEFAULT_MODEL,\n    temperature: float = 0.7,\n    max_tokens: Optional[int] = None,\n    stream: bool = False,\n    **kwargs,\n) -&gt; Dict[str, Any]:\n    \"\"\"\n    Generate a text completion.\n\n    Args:\n        prompt: The prompt to complete.\n        model: The model to use in format \"provider/model_name\".\n        temperature: Controls randomness. Higher values make output more random,\n                     lower values make it more deterministic.\n        max_tokens: Maximum number of tokens to generate.\n        stream: Whether to stream the response.\n        **kwargs: Additional keyword arguments to pass to the API.\n\n    Returns:\n        A dictionary containing the API response.\n    \"\"\"\n</code></pre>"},{"location":"api/client/#embeddings","title":"Embeddings","text":"<pre><code>def embeddings(\n    self,\n    text: Union[str, List[str]],\n    model: str = DEFAULT_EMBEDDING_MODEL,\n    **kwargs,\n) -&gt; Dict[str, Any]:\n    \"\"\"\n    Generate embeddings for the given text.\n\n    Args:\n        text: The text to embed. Can be a string or a list of strings.\n        model: The model to use in format \"provider/model_name\".\n        **kwargs: Additional keyword arguments to pass to the API.\n\n    Returns:\n        A dictionary containing the API response.\n    \"\"\"\n</code></pre>"},{"location":"api/client/#image-generation","title":"Image Generation","text":"<pre><code>def images(\n    self,\n    prompt: str,\n    model: str = DEFAULT_IMAGE_MODEL,\n    size: str = \"1024x1024\",\n    n: int = 1,\n    quality: str = \"standard\",\n    style: str = \"vivid\",\n    **kwargs,\n) -&gt; Dict[str, Any]:\n    \"\"\"\n    Generate images from a text prompt.\n\n    Args:\n        prompt: The prompt to use for generating the image.\n        model: The model to use in format \"provider/model_name\".\n        size: The size of the image in format \"widthxheight\".\n        n: The number of images to generate.\n        quality: The quality of the image (\"standard\" or \"hd\").\n        style: The style of the image (\"vivid\" or \"natural\").\n        **kwargs: Additional keyword arguments to pass to the API.\n\n    Returns:\n        A dictionary containing the API response.\n    \"\"\"\n</code></pre>"},{"location":"api/client/#model-information","title":"Model Information","text":"<pre><code>def models(self, provider: Optional[str] = None) -&gt; Dict[str, Any]:\n    \"\"\"\n    Get information about available models.\n\n    Args:\n        provider: Optional provider ID to filter by.\n\n    Returns:\n        A dictionary containing information about available models.\n    \"\"\"\n\ndef get_model_info(self, provider: str, model: str) -&gt; Dict[str, Any]:\n    \"\"\"\n    Get information about a specific model.\n\n    Args:\n        provider: The provider ID.\n        model: The model ID.\n\n    Returns:\n        A dictionary containing information about the model.\n    \"\"\"\n</code></pre>"},{"location":"api/client/#usage-information","title":"Usage Information","text":"<pre><code>def get_usage(self) -&gt; Dict[str, Any]:\n    \"\"\"\n    Get usage information for the current user.\n\n    Returns:\n        A dictionary containing usage information.\n    \"\"\"\n</code></pre>"},{"location":"api/client/#testing-and-diagnostics","title":"Testing and Diagnostics","text":"<pre><code>def test_connection(self) -&gt; Dict[str, Any]:\n    \"\"\"\n    Test the connection to the server.\n\n    Returns:\n        A dictionary containing information about the connection.\n    \"\"\"\n\ndef diagnose_request(self, endpoint: str, data: Dict[str, Any]) -&gt; Dict[str, Any]:\n    \"\"\"\n    Diagnose a request without sending it.\n\n    Args:\n        endpoint: The API endpoint (e.g., \"chat/completions\").\n        data: The request data.\n\n    Returns:\n        A dictionary containing diagnostic information.\n    \"\"\"\n\ndef enable_debug(self, level=logging.DEBUG):\n    \"\"\"\n    Enable debug logging.\n\n    Args:\n        level: The logging level to use.\n    \"\"\"\n</code></pre>"},{"location":"api/client/#resource-management","title":"Resource Management","text":"<pre><code>def close(self):\n    \"\"\"\n    Close the client session and free up resources.\n    \"\"\"\n\ndef __enter__(self):\n    \"\"\"\n    Enter the context manager.\n    \"\"\"\n    return self\n\ndef __exit__(self, exc_type, exc_val, exc_tb):\n    \"\"\"\n    Exit the context manager and clean up resources.\n    \"\"\"\n    self.close()\n</code></pre>"},{"location":"api/client/#configuration","title":"Configuration","text":"<pre><code>def set_base_url(self, base_url: str) -&gt; None:\n    \"\"\"\n    Set the base URL for the API.\n\n    Args:\n        base_url: The base URL to use.\n    \"\"\"\n</code></pre>"},{"location":"api/exceptions/","title":"Exceptions","text":"<p>IndoxRouter provides specific exception classes to help you handle different types of errors gracefully.</p>"},{"location":"api/exceptions/#exception-hierarchy","title":"Exception Hierarchy","text":"<pre><code>IndoxRouterError (base exception)\n\u251c\u2500\u2500 AuthenticationError\n\u251c\u2500\u2500 RateLimitError\n\u251c\u2500\u2500 APIError\n\u251c\u2500\u2500 NetworkError\n\u2514\u2500\u2500 ValidationError\n</code></pre>"},{"location":"api/exceptions/#base-exception","title":"Base Exception","text":""},{"location":"api/exceptions/#indoxroutererror","title":"IndoxRouterError","text":"<p>The base exception class for all IndoxRouter-related errors.</p> <pre><code>from indoxrouter import IndoxRouterError\n\ntry:\n    response = client.chat(messages=[...], model=\"invalid/model\")\nexcept IndoxRouterError as e:\n    print(f\"An error occurred: {e}\")\n    print(f\"Error type: {type(e).__name__}\")\n</code></pre>"},{"location":"api/exceptions/#specific-exceptions","title":"Specific Exceptions","text":""},{"location":"api/exceptions/#authenticationerror","title":"AuthenticationError","text":"<p>Raised when API key is invalid or missing.</p> <pre><code>from indoxrouter import Client, AuthenticationError\n\ntry:\n    client = Client(api_key=\"invalid_key\")\n    response = client.chat(messages=[...])\nexcept AuthenticationError as e:\n    print(\"Authentication failed. Please check your API key.\")\n    print(f\"Error details: {e}\")\n</code></pre>"},{"location":"api/exceptions/#ratelimiterror","title":"RateLimitError","text":"<p>Raised when you exceed the rate limits.</p> <pre><code>from indoxrouter import RateLimitError\nimport time\n\ntry:\n    response = client.chat(messages=[...])\nexcept RateLimitError as e:\n    print(\"Rate limit exceeded. Waiting before retry...\")\n    time.sleep(60)  # Wait 1 minute\n    # Retry the request\n</code></pre>"},{"location":"api/exceptions/#apierror","title":"APIError","text":"<p>Raised for general API errors from the provider.</p> <pre><code>from indoxrouter import APIError\n\ntry:\n    response = client.chat(messages=[...])\nexcept APIError as e:\n    print(f\"API error occurred: {e}\")\n    print(f\"Status code: {e.status_code}\")\n    print(f\"Error message: {e.message}\")\n</code></pre>"},{"location":"api/exceptions/#networkerror","title":"NetworkError","text":"<p>Raised for network-related issues.</p> <pre><code>from indoxrouter import NetworkError\n\ntry:\n    response = client.chat(messages=[...])\nexcept NetworkError as e:\n    print(\"Network error occurred. Please check your connection.\")\n    print(f\"Error details: {e}\")\n</code></pre>"},{"location":"api/exceptions/#validationerror","title":"ValidationError","text":"<p>Raised when request parameters are invalid.</p> <pre><code>from indoxrouter import ValidationError\n\ntry:\n    response = client.chat(\n        messages=[],  # Empty messages list\n        model=\"openai/gpt-4o-mini\"\n    )\nexcept ValidationError as e:\n    print(\"Invalid request parameters:\")\n    print(f\"Error details: {e}\")\n</code></pre>"},{"location":"api/exceptions/#error-handling-best-practices","title":"Error Handling Best Practices","text":""},{"location":"api/exceptions/#comprehensive-error-handling","title":"Comprehensive Error Handling","text":"<pre><code>from indoxrouter import (\n    Client,\n    AuthenticationError,\n    RateLimitError,\n    APIError,\n    NetworkError,\n    ValidationError,\n    IndoxRouterError\n)\nimport time\n\ndef robust_chat(client, messages, model, max_retries=3):\n    for attempt in range(max_retries):\n        try:\n            response = client.chat(messages=messages, model=model)\n            return response\n\n        except AuthenticationError:\n            print(\"Authentication failed. Please check your API key.\")\n            return None\n\n        except ValidationError as e:\n            print(f\"Invalid request parameters: {e}\")\n            return None\n\n        except RateLimitError:\n            if attempt &lt; max_retries - 1:\n                wait_time = 2 ** attempt\n                print(f\"Rate limit hit. Waiting {wait_time} seconds...\")\n                time.sleep(wait_time)\n                continue\n            else:\n                print(\"Rate limit exceeded after all retries.\")\n                return None\n\n        except NetworkError:\n            if attempt &lt; max_retries - 1:\n                wait_time = 2 ** attempt\n                print(f\"Network error. Retrying in {wait_time} seconds...\")\n                time.sleep(wait_time)\n                continue\n            else:\n                print(\"Network error persists after all retries.\")\n                return None\n\n        except APIError as e:\n            print(f\"API error: {e}\")\n            if e.status_code &gt;= 500 and attempt &lt; max_retries - 1:\n                # Retry on server errors\n                wait_time = 2 ** attempt\n                print(f\"Server error. Retrying in {wait_time} seconds...\")\n                time.sleep(wait_time)\n                continue\n            else:\n                return None\n\n        except IndoxRouterError as e:\n            print(f\"Unexpected IndoxRouter error: {e}\")\n            return None\n\n    return None\n\n# Usage\nclient = Client(api_key=\"your_api_key\")\nresponse = robust_chat(\n    client,\n    [{\"role\": \"user\", \"content\": \"Hello!\"}],\n    \"openai/gpt-4o-mini\"\n)\n\nif response:\n    print(response['choices'][0]['message']['content'])\nelse:\n    print(\"Failed to get response after all attempts.\")\n</code></pre>"},{"location":"api/exceptions/#logging-errors","title":"Logging Errors","text":"<pre><code>import logging\nfrom indoxrouter import Client, IndoxRouterError\n\n# Set up logging\nlogging.basicConfig(level=logging.INFO)\nlogger = logging.getLogger(__name__)\n\ndef chat_with_logging(client, messages, model):\n    try:\n        response = client.chat(messages=messages, model=model)\n        logger.info(f\"Successfully generated response using {model}\")\n        return response\n\n    except IndoxRouterError as e:\n        logger.error(f\"IndoxRouter error: {type(e).__name__}: {e}\")\n        raise\n\n    except Exception as e:\n        logger.error(f\"Unexpected error: {type(e).__name__}: {e}\")\n        raise\n\n# Usage\nclient = Client(api_key=\"your_api_key\")\ntry:\n    response = chat_with_logging(\n        client,\n        [{\"role\": \"user\", \"content\": \"Hello!\"}],\n        \"openai/gpt-4o-mini\"\n    )\nexcept IndoxRouterError:\n    print(\"Failed to generate response due to IndoxRouter error.\")\n</code></pre>"},{"location":"api/exceptions/#error-response-format","title":"Error Response Format","text":"<p>When an exception occurs, you can access additional information:</p> <pre><code>try:\n    response = client.chat(messages=[...])\nexcept APIError as e:\n    print(f\"Status Code: {e.status_code}\")\n    print(f\"Error Type: {e.error_type}\")\n    print(f\"Message: {e.message}\")\n    print(f\"Request ID: {e.request_id}\")  # For debugging with support\n</code></pre>"},{"location":"api/responses/","title":"Response Schemas","text":"<p>This page documents the response formats for all IndoxRouter API endpoints.</p>"},{"location":"api/responses/#chat-completion-response","title":"Chat Completion Response","text":"<pre><code>{\n    \"request_id\": \"550e8400-e29b-41d4-a716-446655440000\",\n    \"created_at\": \"2024-10-07T10:30:00.000Z\",\n    \"duration_ms\": 1250.5,\n    \"provider\": \"openai\",\n    \"model\": \"gpt-4o-mini\",\n    \"success\": true,\n    \"message\": \"\",\n    \"data\": \"Hello! How can I help you today?\",\n    \"finish_reason\": \"stop\",\n    \"usage\": {\n        \"tokens_prompt\": 12,\n        \"tokens_completion\": 8,\n        \"tokens_total\": 20,\n        \"cost\": 0.0006,\n        \"latency\": 1.25,\n        \"cache_read_tokens\": 0,\n        \"cache_write_tokens\": 0,\n        \"reasoning_tokens\": 0,\n        \"web_search_count\": 0,\n        \"request_count\": 1,\n        \"cost_breakdown\": {\n            \"prompt_cost\": 0.00036,\n            \"completion_cost\": 0.00024,\n            \"cache_read_cost\": 0.0,\n            \"cache_write_cost\": 0.0,\n            \"reasoning_cost\": 0.0,\n            \"web_search_cost\": 0.0,\n            \"total_cost\": 0.0006\n        }\n    },\n    \"raw_response\": {...},\n    \"byok_api_key\": false\n}\n</code></pre>"},{"location":"api/responses/#text-completion-response","title":"Text Completion Response","text":"<pre><code>{\n    \"request_id\": \"550e8400-e29b-41d4-a716-446655440001\",\n    \"created_at\": \"2024-10-07T10:30:00.000Z\",\n    \"duration_ms\": 890.2,\n    \"provider\": \"openai\",\n    \"model\": \"gpt-3.5-turbo-instruct\",\n    \"success\": true,\n    \"message\": \"\",\n    \"data\": \"This is a sample completion text.\",\n    \"finish_reason\": \"stop\",\n    \"images\": null,\n    \"usage\": {\n        \"tokens_prompt\": 5,\n        \"tokens_completion\": 7,\n        \"tokens_total\": 12,\n        \"cost\": 0.00018,\n        \"latency\": 0.89,\n        \"cache_read_tokens\": 0,\n        \"cache_write_tokens\": 0,\n        \"reasoning_tokens\": 0,\n        \"web_search_count\": 0,\n        \"request_count\": 1,\n        \"cost_breakdown\": {\n            \"prompt_cost\": 0.000075,\n            \"completion_cost\": 0.000105,\n            \"cache_read_cost\": 0.0,\n            \"cache_write_cost\": 0.0,\n            \"reasoning_cost\": 0.0,\n            \"web_search_cost\": 0.0,\n            \"total_cost\": 0.00018\n        }\n    },\n    \"raw_response\": {...},\n    \"byok_api_key\": false\n}\n</code></pre>"},{"location":"api/responses/#text-completion-response-with-images","title":"Text Completion Response with Images","text":"<p>For models that support image generation:</p> <pre><code>{\n    \"request_id\": \"48c93623-286e-4e03-807b-938e53cb5076\",\n    \"created_at\": \"2025-10-26T16:48:59.574195\",\n    \"duration_ms\": 10853.046178817749,\n    \"provider\": \"google\",\n    \"model\": \"gemini-2.5-flash-image\",\n    \"success\": true,\n    \"message\": \"\",\n    \"data\": \"A cat is a small, domesticated carnivorous mammal... Here's a drawing of a cat for you:\",\n    \"finish_reason\": \"stop\",\n    \"images\": [\n        {\n            \"url\": \"https://indoxrouter.s3.amazonaws.com/dev_user_4/image/d0847065-2f2b-4529-8484-0e98e19b7318_20251026_164858.png?...\",\n            \"index\": 0\n        }\n    ],\n    \"usage\": {\n        \"tokens_prompt\": 8,\n        \"tokens_completion\": 1377,\n        \"tokens_total\": 1385,\n        \"cost\": 0.0034449,\n        \"latency\": 9.228402614593506,\n        \"cache_read_tokens\": 0,\n        \"cache_write_tokens\": 0,\n        \"reasoning_tokens\": 0,\n        \"web_search_count\": 0,\n        \"request_count\": 1,\n        \"cost_breakdown\": {\n            \"prompt_cost\": 0.0000024,\n            \"completion_cost\": 0.0034425,\n            \"cache_read_cost\": 0.0,\n            \"cache_write_cost\": 0.0,\n            \"reasoning_cost\": 0.0,\n            \"web_search_cost\": 0.0,\n            \"total_cost\": 0.0034449\n        }\n    },\n    \"raw_response\": {...},\n    \"byok_api_key\": false\n}\n</code></pre>"},{"location":"api/responses/#embedding-response","title":"Embedding Response","text":"<pre><code>{\n    \"request_id\": \"550e8400-e29b-41d4-a716-446655440002\",\n    \"created_at\": \"2024-10-07T10:30:00.000Z\",\n    \"duration_ms\": 450.8,\n    \"provider\": \"openai\",\n    \"model\": \"text-embedding-ada-002\",\n    \"success\": true,\n    \"message\": \"\",\n    \"data\": [\n        [0.0023064255, -0.009327292, ...],\n        [0.001234567, 0.008765432, ...]\n    ],\n    \"dimensions\": 1536,\n    \"usage\": {\n        \"tokens_prompt\": 8,\n        \"tokens_completion\": 0,\n        \"tokens_total\": 8,\n        \"cost\": 0.0000032,\n        \"latency\": 0.45,\n        \"cache_read_tokens\": 0,\n        \"cache_write_tokens\": 0,\n        \"reasoning_tokens\": 0,\n        \"web_search_count\": 0,\n        \"request_count\": 1,\n        \"cost_breakdown\": {\n            \"prompt_cost\": 0.0000032,\n            \"completion_cost\": 0.0,\n            \"cache_read_cost\": 0.0,\n            \"cache_write_cost\": 0.0,\n            \"reasoning_cost\": 0.0,\n            \"web_search_cost\": 0.0,\n            \"total_cost\": 0.0000032\n        }\n    },\n    \"raw_response\": {...},\n    \"byok_api_key\": false\n}\n</code></pre>"},{"location":"api/responses/#image-generation-response","title":"Image Generation Response","text":""},{"location":"api/responses/#url-based-models-dall-e-2-dall-e-3","title":"URL-based Models (DALL-E 2, DALL-E 3)","text":"<pre><code>{\n    \"request_id\": \"550e8400-e29b-41d4-a716-446655440003\",\n    \"created_at\": \"2024-10-07T10:30:00.000Z\",\n    \"duration_ms\": 3250.1,\n    \"provider\": \"openai\",\n    \"model\": \"dall-e-3\",\n    \"success\": true,\n    \"message\": \"\",\n    \"data\": [\n        {\n            \"url\": \"https://oaidalleapiprodscus.blob.core.windows.net/private/...\",\n            \"revised_prompt\": \"A beautiful sunset over mountains...\"\n        }\n    ],\n    \"usage\": {\n        \"tokens_prompt\": 0,\n        \"tokens_completion\": 0,\n        \"tokens_total\": 0,\n        \"cost\": 0.04,\n        \"latency\": 3.25,\n        \"cache_read_tokens\": 0,\n        \"cache_write_tokens\": 0,\n        \"reasoning_tokens\": 0,\n        \"web_search_count\": 0,\n        \"request_count\": 1,\n        \"cost_breakdown\": {\n            \"prompt_cost\": 0.0,\n            \"completion_cost\": 0.0,\n            \"cache_read_cost\": 0.0,\n            \"cache_write_cost\": 0.0,\n            \"reasoning_cost\": 0.0,\n            \"web_search_cost\": 0.0,\n            \"total_cost\": 0.04\n        }\n    },\n    \"raw_response\": {...},\n    \"byok_api_key\": false\n}\n</code></pre>"},{"location":"api/responses/#base64-based-models-gpt-image-1","title":"Base64-based Models (GPT-Image-1)","text":"<pre><code>{\n    \"created\": 1677652288,\n    \"data\": [\n        {\n            \"b64_json\": \"iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAADUlEQVR42mP8/5+hHgAHggJ/PchI7wAAAABJRU5ErkJggg==\"\n        }\n    ],\n    \"provider\": \"openai\",\n    \"model\": \"gpt-image-1\",\n    \"usage\": {\n        \"request_count\": 1,\n        \"cost_breakdown\": {\n            \"total_cost\": 0.02\n        }\n    }\n}\n</code></pre>"},{"location":"api/responses/#model-information-response","title":"Model Information Response","text":"<pre><code>{\n    \"models\": [\n        {\n            \"id\": \"gpt-4\",\n            \"object\": \"model\",\n            \"created\": 1677610602,\n            \"owned_by\": \"openai\",\n            \"provider\": \"openai\",\n            \"capabilities\": [\"chat\", \"completion\"],\n            \"context_length\": 8192,\n            \"pricing\": {\n                \"prompt\": 0.03,\n                \"completion\": 0.06,\n                \"unit\": \"1K tokens\"\n            }\n        }\n    ]\n}\n</code></pre>"},{"location":"api/responses/#usage-statistics-response","title":"Usage Statistics Response","text":"<pre><code>{\n    \"total_requests\": 1250,\n    \"total_cost\": 12.50,\n    \"remaining_credits\": 87.50,\n    \"total_tokens\": {\n        \"input\": 22500,\n        \"output\": 22500,\n        \"total\": 45000\n    },\n    \"endpoints\": {\n        \"chat\": {\n            \"requests\": 1000,\n            \"cost\": 10.00,\n            \"tokens\": {\n                \"input\": 18000,\n                \"output\": 18000,\n                \"total\": 36000\n            }\n        },\n        \"embeddings\": {\n            \"requests\": 200,\n            \"cost\": 2.00,\n            \"tokens\": {\n                \"input\": 4500,\n                \"output\": 0,\n                \"total\": 4500\n            }\n        },\n        \"images\": {\n            \"requests\": 50,\n            \"cost\": 0.50,\n            \"tokens\": {\n                \"input\": 0,\n                \"output\": 0,\n                \"total\": 0\n            }\n        }\n    },\n    \"providers\": {\n        \"openai\": {\n            \"requests\": 800,\n            \"cost\": 8.40,\n            \"tokens\": {\n                \"input\": 14000,\n                \"output\": 14000,\n                \"total\": 28000\n            }\n        },\n        \"anthropic\": {\n            \"requests\": 300,\n            \"cost\": 2.88,\n            \"tokens\": {\n                \"input\": 6000,\n                \"output\": 6000,\n                \"total\": 12000\n            }\n        }\n    },\n    \"models\": {\n        \"gpt-4\": {\n            \"requests\": 400,\n            \"cost\": 4.50,\n            \"tokens\": {\n                \"input\": 7500,\n                \"output\": 7500,\n                \"total\": 15000\n            }\n        },\n        \"claude-3-sonnet\": {\n            \"requests\": 300,\n            \"cost\": 2.88,\n            \"tokens\": {\n                \"input\": 6000,\n                \"output\": 6000,\n                \"total\": 12000\n            }\n        }\n    },\n    \"daily_usage\": [\n        {\n            \"date\": \"2024-10-01\",\n            \"requests\": 45,\n            \"cost\": 1.25,\n            \"tokens\": {\n                \"input\": 1125,\n                \"output\": 1125,\n                \"total\": 2250\n            }\n        },\n        {\n            \"date\": \"2024-10-02\",\n            \"requests\": 38,\n            \"cost\": 0.95,\n            \"tokens\": {\n                \"input\": 950,\n                \"output\": 950,\n                \"total\": 1900\n            }\n        }\n    ]\n}\n</code></pre>"},{"location":"api/responses/#error-response","title":"Error Response","text":"<pre><code>{\n    \"error\": {\n        \"message\": \"Invalid API key provided\",\n        \"type\": \"invalid_request_error\",\n        \"param\": null,\n        \"code\": \"invalid_api_key\"\n    }\n}\n</code></pre>"},{"location":"api/responses/#response-fields","title":"Response Fields","text":""},{"location":"api/responses/#usage-object","title":"Usage Object","text":"<p>All API responses include a <code>usage</code> object with the following fields:</p> <ul> <li><code>tokens_prompt</code> (integer): Number of tokens in the prompt/input</li> <li><code>tokens_completion</code> (integer): Number of tokens in the completion/output</li> <li><code>tokens_total</code> (integer): Total tokens used (prompt + completion)</li> <li><code>cost</code> (float): Total cost for the request</li> <li><code>latency</code> (float): Request latency in seconds</li> <li><code>cache_read_tokens</code> (integer): Tokens read from cache</li> <li><code>cache_write_tokens</code> (integer): Tokens written to cache</li> <li><code>reasoning_tokens</code> (integer): Tokens used for reasoning (reasoning models)</li> <li><code>web_search_count</code> (integer): Number of web searches performed</li> <li><code>request_count</code> (integer): Number of API requests made</li> <li><code>cost_breakdown</code> (object): Detailed cost information</li> </ul>"},{"location":"api/responses/#cost-breakdown-object","title":"Cost Breakdown Object","text":"<p>The <code>cost_breakdown</code> object provides detailed pricing:</p> <ul> <li><code>prompt_cost</code> (float): Cost for prompt tokens</li> <li><code>completion_cost</code> (float): Cost for completion tokens</li> <li><code>cache_read_cost</code> (float): Cost for cache read operations</li> <li><code>cache_write_cost</code> (float): Cost for cache write operations</li> <li><code>reasoning_cost</code> (float): Cost for reasoning tokens</li> <li><code>web_search_cost</code> (float): Cost for web search operations</li> <li><code>total_cost</code> (float): Total cost for the request</li> </ul>"},{"location":"api/responses/#finish-reasons","title":"Finish Reasons","text":"<p>Possible values for <code>finish_reason</code>:</p> <ul> <li><code>stop</code>: Natural stopping point or provided stop sequence</li> <li><code>length</code>: Maximum token limit reached</li> <li><code>content_filter</code>: Content filtered due to policy violations</li> <li><code>tool_calls</code>: Model called a function/tool</li> <li><code>function_call</code>: Model called a function (deprecated)</li> </ul>"},{"location":"api/responses/#http-status-codes","title":"HTTP Status Codes","text":"<ul> <li>200: Success</li> <li>400: Bad Request - Invalid parameters</li> <li>401: Unauthorized - Invalid API key</li> <li>403: Forbidden - Insufficient permissions</li> <li>429: Too Many Requests - Rate limit exceeded</li> <li>500: Internal Server Error</li> <li>502: Bad Gateway - Provider error</li> <li>503: Service Unavailable - Temporary outage</li> </ul>"},{"location":"examples/advanced/","title":"Advanced Examples","text":"<p>This section covers advanced usage patterns and real-world applications of IndoxRouter.</p>"},{"location":"examples/advanced/#streaming-responses","title":"Streaming Responses","text":"<p>Handle real-time streaming responses for better user experience:</p> <pre><code>from indoxrouter import Client\n\nclient = Client(api_key=\"your_api_key\")\n\n# Stream chat responses\nresponse = client.chat(\n    messages=[\n        {\"role\": \"user\", \"content\": \"Write a short story about AI\"}\n    ],\n    model=\"openai/gpt-4o-mini\",\n    stream=True\n)\n\nfor chunk in response:\n    if chunk.get(\"data\"):\n        print(chunk[\"data\"], end=\"\", flush=True)\n</code></pre>"},{"location":"examples/advanced/#batch-processing","title":"Batch Processing","text":"<p>Process multiple requests efficiently using synchronous calls:</p> <pre><code>from concurrent.futures import ThreadPoolExecutor\nfrom indoxrouter import Client\n\ndef process_batch():\n    client = Client(api_key=\"your_api_key\")\n\n    prompts = [\n        \"Explain quantum computing\",\n        \"What is machine learning?\",\n        \"How does blockchain work?\"\n    ]\n\n    # Process requests in parallel using threads\n    def make_request(prompt):\n        return client.chat(\n            messages=[{\"role\": \"user\", \"content\": prompt}],\n            model=\"openai/gpt-4o-mini\"\n        )\n\n    with ThreadPoolExecutor(max_workers=3) as executor:\n        responses = list(executor.map(make_request, prompts))\n\n    for i, response in enumerate(responses):\n        print(f\"Question {i+1}: {prompts[i]}\")\n        print(f\"Answer: {response['data']}\")\n        print(\"---\")\n\n# Run the batch processing\nprocess_batch()\n</code></pre> <p>Or using asyncio with synchronous client:</p> <pre><code>import asyncio\nfrom indoxrouter import Client\n\nasync def process_batch():\n    client = Client(api_key=\"your_api_key\")\n\n    prompts = [\n        \"Explain quantum computing\",\n        \"What is machine learning?\",\n        \"How does blockchain work?\"\n    ]\n\n    # Run synchronous requests in thread pool\n    loop = asyncio.get_event_loop()\n\n    def make_request(prompt):\n        return client.chat(\n            messages=[{\"role\": \"user\", \"content\": prompt}],\n            model=\"openai/gpt-4o-mini\"\n        )\n\n    tasks = [\n        loop.run_in_executor(None, make_request, prompt)\n        for prompt in prompts\n    ]\n\n    responses = await asyncio.gather(*tasks)\n\n    for i, response in enumerate(responses):\n        print(f\"Question {i+1}: {prompts[i]}\")\n        print(f\"Answer: {response['data']}\")\n        print(\"---\")\n\n# Run the batch processing\nasyncio.run(process_batch())\n</code></pre>"},{"location":"examples/advanced/#error-handling-and-retries","title":"Error Handling and Retries","text":"<p>Implement robust error handling:</p> <pre><code>import time\nfrom indoxrouter import Client, IndoxRouterError\n\ndef chat_with_retry(client, messages, model, max_retries=3):\n    for attempt in range(max_retries):\n        try:\n            response = client.chat(\n                messages=messages,\n                model=model\n            )\n            return response\n        except IndoxRouterError as e:\n            if attempt == max_retries - 1:\n                raise e\n\n            print(f\"Attempt {attempt + 1} failed: {e}\")\n            time.sleep(2 ** attempt)  # Exponential backoff\n\n    return None\n\n# Usage\nclient = Client(api_key=\"your_api_key\")\n\ntry:\n    response = chat_with_retry(\n        client,\n        [{\"role\": \"user\", \"content\": \"Hello!\"}],\n        \"openai/gpt-4o-mini\"\n    )\n    print(response['data'])\nexcept IndoxRouterError as e:\n    print(f\"Failed after all retries: {e}\")\n</code></pre>"},{"location":"examples/advanced/#custom-model-routing","title":"Custom Model Routing","text":"<p>Route requests to different models based on content:</p> <pre><code>from indoxrouter import Client\n\nclass SmartRouter:\n    def __init__(self, api_key):\n        self.client = Client(api_key=api_key)\n\n    def route_request(self, message):\n        # Analyze the request to choose the best model\n        content = message.lower()\n\n        if any(word in content for word in ['code', 'programming', 'function']):\n            return \"openai/gpt-4o\"  # Better for coding\n        elif any(word in content for word in ['creative', 'story', 'poem']):\n            return \"anthropic/claude-3-opus\"  # Better for creativity\n        elif len(content) &lt; 50:\n            return \"openai/gpt-3.5-turbo\"  # Fast for simple queries\n        else:\n            return \"openai/gpt-4o-mini\"  # Default choice\n\n    def chat(self, message):\n        model = self.route_request(message)\n\n        response = self.client.chat(\n            messages=[{\"role\": \"user\", \"content\": message}],\n            model=model\n        )\n\n        return {\n            \"model_used\": model,\n            \"response\": response['data']\n        }\n\n# Usage\nrouter = SmartRouter(api_key=\"your_api_key\")\n\nresult = router.chat(\"Write a Python function to sort a list\")\nprint(f\"Model used: {result['model_used']}\")\nprint(f\"Response: {result['response']}\")\n</code></pre>"},{"location":"examples/advanced/#tool-calling-function-calling","title":"Tool Calling (Function Calling)","text":"<p>Use tool calling for structured outputs with compatible models:</p> <pre><code>from indoxrouter import Client\nimport json\n\nclient = Client(api_key=\"your_api_key\")\n\n# Define available tools (OpenAI tools format)\ntools = [\n    {\n        \"type\": \"function\",\n        \"function\": {\n            \"name\": \"get_weather\",\n            \"description\": \"Get weather information for a location\",\n            \"parameters\": {\n                \"type\": \"object\",\n                \"properties\": {\n                    \"location\": {\n                        \"type\": \"string\",\n                        \"description\": \"The city and state, e.g. San Francisco, CA\"\n                    },\n                    \"unit\": {\n                        \"type\": \"string\",\n                        \"enum\": [\"celsius\", \"fahrenheit\"],\n                        \"description\": \"Temperature unit\"\n                    }\n                },\n                \"required\": [\"location\"]\n            }\n        }\n    }\n]\n\nresponse = client.chat(\n    messages=[\n        {\"role\": \"user\", \"content\": \"What's the weather like in New York?\"}\n    ],\n    model=\"openai/gpt-4o-mini\",\n    tools=tools,\n    tool_choice=\"auto\"\n)\n\n# Handle tool call (Note: This feature may not be fully implemented in current backend)\nif response.get('tool_calls'):\n    tool_calls = response['tool_calls']\n    for tool_call in tool_calls:\n        tool_name = tool_call['function']['name']\n        tool_args = json.loads(tool_call['function']['arguments'])\n\n        print(f\"Tool called: {tool_name}\")\n        print(f\"Arguments: {tool_args}\")\n\n        # In a real application, you would call the actual tool here\n        weather_result = f\"The weather in {tool_args['location']} is sunny, 72\u00b0F\"\n\n        # Send the tool result back to the model\n        follow_up = client.chat(\n            messages=[\n                {\"role\": \"user\", \"content\": \"What's the weather like in New York?\"},\n                {\"role\": \"assistant\", \"content\": response['data'], \"tool_calls\": tool_calls},\n                {\"role\": \"tool\", \"tool_call_id\": tool_call['id'], \"content\": weather_result}\n            ],\n            model=\"openai/gpt-4o-mini\"\n        )\n\n        print(follow_up['data'])\nelse:\n    # Regular response without tool calls\n    print(f\"Response: {response['data']}\")\n</code></pre> <p>Note: Tool calling support may vary by provider and model. Not all models in IndoxRouter currently support tool calling.</p>"},{"location":"examples/basic/","title":"Basic Examples","text":"<p>This page provides some basic examples of using the IndoxRouter Client for common tasks.</p>"},{"location":"examples/basic/#chat-completion-example","title":"Chat Completion Example","text":"<pre><code>from indoxrouter import Client\n\n# Initialize the client\nwith Client(api_key=\"your_api_key\") as client:\n    # Generate a chat completion\n    response = client.chat(\n        messages=[\n            {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n            {\"role\": \"user\", \"content\": \"What is machine learning and why is it important?\"}\n        ],\n        model=\"openai/gpt-4o-mini\",\n        temperature=0.7\n    )\n\n    # Print the response\n    print(response[\"data\"])\n</code></pre>"},{"location":"examples/basic/#text-completion-example","title":"Text Completion Example","text":"<pre><code>from indoxrouter import Client\n\n# Initialize the client\nwith Client(api_key=\"your_api_key\") as client:\n    # Generate a text completion\n    response = client.completion(\n        prompt=\"Write a short poem about artificial intelligence:\",\n        model=\"openai/gpt-4o-mini\",\n        max_tokens=100\n    )\n\n    # Print the response\n    print(response[\"data\"])\n</code></pre>"},{"location":"examples/basic/#embedding-example","title":"Embedding Example","text":"<pre><code>from indoxrouter import Client\n\n# Initialize the client\nwith Client(api_key=\"your_api_key\") as client:\n    # Generate embeddings for multiple texts\n    response = client.embeddings(\n        text=[\n            \"Machine learning is a branch of artificial intelligence.\",\n            \"Natural language processing helps computers understand human language.\"\n        ],\n        model=\"openai/text-embedding-3-small\"\n    )\n\n    # Print the first few dimensions of each embedding\n    for i, embedding_data in enumerate(response[\"data\"]):\n        embedding = embedding_data[\"embedding\"]\n        print(f\"Embedding {i+1} (first 5 dimensions): {embedding[:5]}\")\n        print(f\"Embedding {i+1} dimensions: {len(embedding)}\")\n</code></pre>"},{"location":"examples/basic/#vision-image-analysis-example","title":"Vision &amp; Image Analysis Example","text":"<pre><code>from indoxrouter import Client\nimport base64\n\n# Initialize the client\nwith Client(api_key=\"your_api_key\") as client:\n    # Read and encode an image\n    with open(\"photo.jpg\", \"rb\") as f:\n        image_base64 = base64.b64encode(f.read()).decode('utf-8')\n\n    # Analyze the image with a vision-capable model\n    response = client.chat(\n        messages=[\n            {\n                \"role\": \"user\",\n                \"content\": [\n                    {\n                        \"type\": \"text\",\n                        \"text\": \"What's in this image? Please describe it in detail.\"\n                    },\n                    {\n                        \"type\": \"image\",\n                        \"image\": {\n                            \"data\": image_base64,\n                            \"media_type\": \"image/jpeg\"\n                        }\n                    }\n                ]\n            }\n        ],\n        model=\"openai/gpt-4o\"\n    )\n\n    # Print the image description\n    print(response[\"data\"])\n</code></pre>"},{"location":"examples/basic/#image-generation-example","title":"Image Generation Example","text":"<pre><code>from indoxrouter import Client\n\n# Initialize the client\nwith Client(api_key=\"your_api_key\") as client:\n    # Generate an image\n    response = client.images(\n        prompt=\"A futuristic city with flying cars and towering skyscrapers at sunset\",\n        model=\"openai/dall-e-3\",\n        size=\"1024x1024\"\n    )\n\n    # Print the image URL\n    print(f\"Generated image URL: {response['data'][0]['url']}\")\n</code></pre>"},{"location":"examples/basic/#model-information-example","title":"Model Information Example","text":"<pre><code>from indoxrouter import Client\nimport json\n\n# Initialize the client\nwith Client(api_key=\"your_api_key\") as client:\n    # Get information about all available models\n    providers = client.models()\n\n    # Print information about each provider\n    for provider in providers:\n        print(f\"Provider: {provider['id']} ({provider['name']})\")\n        print(f\"Description: {provider.get('description', 'No description')}\")\n        print(f\"Models available: {len(provider['models'])}\")\n        print(\"Model IDs:\")\n        for model in provider['models']:\n            print(f\"  - {model['id']}\")\n        print()\n\n    # Get details about a specific model\n    model_info = client.get_model_info(\"openai\", \"gpt-4o-mini\")\n    print(f\"Model: {model_info['id']}\")\n    print(f\"Description: {model_info.get('description', 'No description')}\")\n    print(f\"Capabilities: {', '.join(model_info.get('capabilities', []))}\")\n    print(f\"Max tokens: {model_info.get('max_tokens', 'Unknown')}\")\n</code></pre>"},{"location":"examples/openai-sdk/","title":"Using OpenAI SDK with IndoxRouter","text":"<p>You can use the familiar OpenAI SDK with IndoxRouter to access all supported providers through the OpenAI-compatible API. This is perfect if you're already using OpenAI SDK in your codebase.</p>"},{"location":"examples/openai-sdk/#setup","title":"Setup","text":"<p>Install the OpenAI SDK and configure it to use IndoxRouter:</p> <pre><code>pip install openai\n</code></pre> <pre><code>from openai import OpenAI\n\n# Configure OpenAI client to use IndoxRouter\nclient = OpenAI(\n    api_key=\"your_indoxrouter_api_key\",  # Your IndoxRouter API key\n    base_url=\"https://api.indoxrouter.com\"  # IndoxRouter base URL\n)\n</code></pre>"},{"location":"examples/openai-sdk/#chat-completions","title":"Chat Completions","text":"<p>Use any provider's models through the OpenAI SDK interface:</p>"},{"location":"examples/openai-sdk/#openai-models","title":"OpenAI Models","text":"<pre><code># GPT-4o\nresponse = client.chat.completions.create(\n    model=\"openai/gpt-4o\",\n    messages=[\n        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n        {\"role\": \"user\", \"content\": \"Explain quantum computing\"}\n    ],\n    temperature=0.7,\n    max_tokens=500\n)\n\nprint(response.choices[0].message.content)\n</code></pre>"},{"location":"examples/openai-sdk/#anthropic-models","title":"Anthropic Models","text":"<pre><code># Claude 3 Opus\nresponse = client.chat.completions.create(\n    model=\"anthropic/claude-3-opus-20240229\",\n    messages=[\n        {\"role\": \"user\", \"content\": \"Write a creative story about a time traveler\"}\n    ],\n    temperature=0.8,\n    max_tokens=800\n)\n\nprint(response.choices[0].message.content)\n</code></pre>"},{"location":"examples/openai-sdk/#google-models","title":"Google Models","text":"<pre><code># Gemini Pro\nresponse = client.chat.completions.create(\n    model=\"google/gemini-1.5-pro\",\n    messages=[\n        {\"role\": \"user\", \"content\": \"Analyze the impact of AI on healthcare\"}\n    ],\n    temperature=0.3,\n    max_tokens=1000\n)\n\nprint(response.choices[0].message.content)\n</code></pre>"},{"location":"examples/openai-sdk/#deepseek-models","title":"DeepSeek Models","text":"<pre><code># DeepSeek for coding\nresponse = client.chat.completions.create(\n    model=\"deepseek/deepseek-coder\",\n    messages=[\n        {\"role\": \"user\", \"content\": \"Write a Python function to calculate Fibonacci numbers\"}\n    ],\n    temperature=0,\n    max_tokens=300\n)\n\nprint(response.choices[0].message.content)\n</code></pre>"},{"location":"examples/openai-sdk/#streaming-responses","title":"Streaming Responses","text":"<p>Stream responses from any provider:</p> <pre><code># Streaming with Claude\nstream = client.chat.completions.create(\n    model=\"anthropic/claude-3-sonnet-20240229\",\n    messages=[\n        {\"role\": \"user\", \"content\": \"Tell me a long story about space exploration\"}\n    ],\n    stream=True,\n    max_tokens=1500\n)\n\nprint(\"Story: \", end=\"\")\nfor chunk in stream:\n    if chunk.choices[0].delta.content:\n        print(chunk.choices[0].delta.content, end=\"\", flush=True)\nprint()\n</code></pre>"},{"location":"examples/openai-sdk/#text-completions","title":"Text Completions","text":"<p>Use text completion models:</p> <pre><code># GPT-3.5 Turbo Instruct\nresponse = client.completions.create(\n    model=\"openai/gpt-3.5-turbo-instruct\",\n    prompt=\"The future of artificial intelligence is\",\n    max_tokens=200,\n    temperature=0.7\n)\n\nprint(response.choices[0].text)\n</code></pre>"},{"location":"examples/openai-sdk/#embeddings","title":"Embeddings","text":"<p>Generate embeddings using different providers:</p> <pre><code># OpenAI embeddings\nresponse = client.embeddings.create(\n    model=\"openai/text-embedding-3-small\",\n    input=\"Hello, world!\"\n)\n\nembedding = response.data[0].embedding\nprint(f\"Embedding dimensions: {len(embedding)}\")\nprint(f\"First 5 values: {embedding[:5]}\")\n\n# Multiple texts\nresponse = client.embeddings.create(\n    model=\"openai/text-embedding-3-large\",\n    input=[\n        \"Document 1: Introduction to machine learning\",\n        \"Document 2: Deep learning fundamentals\",\n        \"Document 3: Natural language processing\"\n    ]\n)\n\nfor i, embedding_obj in enumerate(response.data):\n    print(f\"Document {i+1} embedding dimensions: {len(embedding_obj.embedding)}\")\n</code></pre>"},{"location":"examples/openai-sdk/#image-generation","title":"Image Generation","text":"<p>Generate images using DALL-E or other providers:</p> <pre><code># DALL-E 3\nresponse = client.images.generate(\n    model=\"openai/dall-e-3\",\n    prompt=\"A futuristic cityscape with flying cars at sunset\",\n    size=\"1024x1024\",\n    quality=\"hd\",\n    style=\"vivid\",\n    n=1\n)\n\nimage_url = response.data[0].url\nprint(f\"Generated image: {image_url}\")\n\n# Get revised prompt\nif hasattr(response.data[0], 'revised_prompt'):\n    print(f\"Revised prompt: {response.data[0].revised_prompt}\")\n</code></pre>"},{"location":"examples/openai-sdk/#error-handling","title":"Error Handling","text":"<p>Handle errors using OpenAI SDK patterns:</p> <pre><code>from openai import OpenAI, RateLimitError, AuthenticationError\n\nclient = OpenAI(\n    api_key=\"your_indoxrouter_api_key\",\n    base_url=\"https://api.indoxrouter.com\"\n)\n\ntry:\n    response = client.chat.completions.create(\n        model=\"openai/gpt-4o\",\n        messages=[{\"role\": \"user\", \"content\": \"Hello\"}]\n    )\n    print(response.choices[0].message.content)\n\nexcept RateLimitError as e:\n    print(f\"Rate limit exceeded: {e}\")\n\nexcept AuthenticationError as e:\n    print(f\"Authentication failed: {e}\")\n\nexcept Exception as e:\n    print(f\"Request failed: {e}\")\n</code></pre>"},{"location":"examples/openai-sdk/#advanced-usage","title":"Advanced Usage","text":""},{"location":"examples/openai-sdk/#function-calling","title":"Function Calling","text":"<p>Use function calling with supported models:</p> <pre><code>import json\n\n# Define a function\ntools = [\n    {\n        \"type\": \"function\",\n        \"function\": {\n            \"name\": \"get_weather\",\n            \"description\": \"Get current weather for a location\",\n            \"parameters\": {\n                \"type\": \"object\",\n                \"properties\": {\n                    \"location\": {\n                        \"type\": \"string\",\n                        \"description\": \"The city and state, e.g. San Francisco, CA\"\n                    }\n                },\n                \"required\": [\"location\"]\n            }\n        }\n    }\n]\n\nresponse = client.chat.completions.create(\n    model=\"openai/gpt-4o\",\n    messages=[\n        {\"role\": \"user\", \"content\": \"What's the weather like in San Francisco?\"}\n    ],\n    tools=tools,\n    tool_choice=\"auto\"\n)\n\n# Check if the model wants to call a function\nif response.choices[0].message.tool_calls:\n    tool_call = response.choices[0].message.tool_calls[0]\n    function_name = tool_call.function.name\n    function_args = json.loads(tool_call.function.arguments)\n\n    print(f\"Model wants to call: {function_name}\")\n    print(f\"With arguments: {function_args}\")\n\n    # Simulate function execution\n    weather_result = {\"temperature\": \"72\u00b0F\", \"condition\": \"sunny\"}\n\n    # Send function result back\n    messages = [\n        {\"role\": \"user\", \"content\": \"What's the weather like in San Francisco?\"},\n        response.choices[0].message,\n        {\n            \"role\": \"tool\",\n            \"tool_call_id\": tool_call.id,\n            \"content\": json.dumps(weather_result)\n        }\n    ]\n\n    final_response = client.chat.completions.create(\n        model=\"openai/gpt-4o\",\n        messages=messages\n    )\n\n    print(final_response.choices[0].message.content)\n</code></pre>"},{"location":"examples/openai-sdk/#json-mode","title":"JSON Mode","text":"<p>Request structured JSON responses:</p> <pre><code>response = client.chat.completions.create(\n    model=\"openai/gpt-4o\",\n    messages=[\n        {\"role\": \"system\", \"content\": \"You are a helpful assistant designed to output JSON.\"},\n        {\"role\": \"user\", \"content\": \"Generate a JSON object with information about Paris, France\"}\n    ],\n    response_format={\"type\": \"json_object\"}\n)\n\n# Parse the JSON response\nimport json\ncity_info = json.loads(response.choices[0].message.content)\nprint(json.dumps(city_info, indent=2))\n</code></pre>"},{"location":"examples/openai-sdk/#reproducible-outputs","title":"Reproducible Outputs","text":"<p>Use seed for reproducible outputs (when supported):</p> <pre><code># Same prompt with same seed should give same result\nresponse1 = client.chat.completions.create(\n    model=\"openai/gpt-4o\",\n    messages=[{\"role\": \"user\", \"content\": \"Generate a random number\"}],\n    seed=12345,\n    temperature=0\n)\n\nresponse2 = client.chat.completions.create(\n    model=\"openai/gpt-4o\",\n    messages=[{\"role\": \"user\", \"content\": \"Generate a random number\"}],\n    seed=12345,\n    temperature=0\n)\n\nprint(f\"Response 1: {response1.choices[0].message.content}\")\nprint(f\"Response 2: {response2.choices[0].message.content}\")\nprint(f\"Same result: {response1.choices[0].message.content == response2.choices[0].message.content}\")\n</code></pre>"},{"location":"examples/openai-sdk/#model-comparison","title":"Model Comparison","text":"<p>Easily compare responses from different providers:</p> <pre><code>def compare_models(prompt, models):\n    \"\"\"Compare responses from different models.\"\"\"\n\n    results = {}\n\n    for model in models:\n        try:\n            response = client.chat.completions.create(\n                model=model,\n                messages=[{\"role\": \"user\", \"content\": prompt}],\n                temperature=0.7,\n                max_tokens=300\n            )\n\n            results[model] = {\n                'response': response.choices[0].message.content,\n                'finish_reason': response.choices[0].finish_reason,\n                'usage': response.usage._asdict() if response.usage else None\n            }\n\n        except Exception as e:\n            results[model] = {'error': str(e)}\n\n    return results\n\n# Compare different models\nprompt = \"Explain the concept of artificial general intelligence in simple terms.\"\nmodels = [\n    \"openai/gpt-4o-mini\",\n    \"anthropic/claude-3-haiku-20240307\",\n    \"google/gemini-1.5-flash\",\n    \"deepseek/deepseek-chat\"\n]\n\ncomparison = compare_models(prompt, models)\n\nfor model, result in comparison.items():\n    print(f\"\\n{'='*50}\")\n    print(f\"Model: {model}\")\n    print(f\"{'='*50}\")\n\n    if 'error' in result:\n        print(f\"Error: {result['error']}\")\n    else:\n        print(f\"Response: {result['response'][:200]}...\")\n        if result['usage']:\n            print(f\"Tokens: {result['usage']['total_tokens']}\")\n</code></pre>"},{"location":"examples/openai-sdk/#provider-specific-features","title":"Provider-Specific Features","text":""},{"location":"examples/openai-sdk/#anthropic-claude-features","title":"Anthropic Claude Features","text":"<pre><code># Claude with system message in messages (Anthropic style)\nresponse = client.chat.completions.create(\n    model=\"anthropic/claude-3-opus-20240229\",\n    messages=[\n        {\"role\": \"system\", \"content\": \"You are Claude, an AI assistant created by Anthropic.\"},\n        {\"role\": \"user\", \"content\": \"What are your capabilities?\"}\n    ],\n    max_tokens=500\n)\n</code></pre>"},{"location":"examples/openai-sdk/#google-gemini-features","title":"Google Gemini Features","text":"<pre><code># Gemini with longer context\nresponse = client.chat.completions.create(\n    model=\"google/gemini-1.5-pro\",\n    messages=[\n        {\"role\": \"user\", \"content\": \"Analyze this very long document...\" + \"x\" * 10000}\n    ],\n    max_tokens=1000\n)\n</code></pre>"},{"location":"examples/openai-sdk/#batch-processing","title":"Batch Processing","text":"<p>Process multiple requests efficiently:</p> <pre><code>import asyncio\nfrom openai import AsyncOpenAI\n\n# Use async client for better performance\nasync_client = AsyncOpenAI(\n    api_key=\"your_indoxrouter_api_key\",\n    base_url=\"https://api.indoxrouter.com\"\n)\n\nasync def process_batch(prompts, model=\"openai/gpt-4o-mini\"):\n    \"\"\"Process multiple prompts concurrently.\"\"\"\n\n    async def single_request(prompt):\n        try:\n            response = await async_client.chat.completions.create(\n                model=model,\n                messages=[{\"role\": \"user\", \"content\": prompt}],\n                temperature=0.7,\n                max_tokens=200\n            )\n            return {\n                'prompt': prompt,\n                'response': response.choices[0].message.content,\n                'success': True\n            }\n        except Exception as e:\n            return {\n                'prompt': prompt,\n                'error': str(e),\n                'success': False\n            }\n\n    # Process all prompts concurrently\n    tasks = [single_request(prompt) for prompt in prompts]\n    results = await asyncio.gather(*tasks)\n\n    return results\n\n# Example usage\nprompts = [\n    \"What is machine learning?\",\n    \"Explain quantum computing\",\n    \"How does blockchain work?\",\n    \"What is artificial intelligence?\"\n]\n\n# Run batch processing\nresults = asyncio.run(process_batch(prompts))\n\n# Display results\nfor result in results:\n    if result['success']:\n        print(f\"Q: {result['prompt']}\")\n        print(f\"A: {result['response'][:100]}...\")\n        print()\n    else:\n        print(f\"Failed: {result['prompt']} - {result['error']}\")\n</code></pre>"},{"location":"examples/openai-sdk/#migration-from-openai","title":"Migration from OpenAI","text":"<p>If you're migrating from direct OpenAI usage to IndoxRouter:</p>"},{"location":"examples/openai-sdk/#before-direct-openai","title":"Before (Direct OpenAI)","text":"<pre><code>from openai import OpenAI\n\nclient = OpenAI(api_key=\"sk-openai-key...\")\n\nresponse = client.chat.completions.create(\n    model=\"gpt-4\",\n    messages=[{\"role\": \"user\", \"content\": \"Hello\"}]\n)\n</code></pre>"},{"location":"examples/openai-sdk/#after-indoxrouter","title":"After (IndoxRouter)","text":"<pre><code>from openai import OpenAI\n\n# Only change: API key and base URL\nclient = OpenAI(\n    api_key=\"your_indoxrouter_api_key\",  # IndoxRouter API key\n    base_url=\"https://api.indoxrouter.com\"  # IndoxRouter base URL\n)\n\nresponse = client.chat.completions.create(\n    model=\"openai/gpt-4o\",  # Specify provider/model\n    messages=[{\"role\": \"user\", \"content\": \"Hello\"}]\n)\n\n# Access to other providers with same code!\nresponse = client.chat.completions.create(\n    model=\"anthropic/claude-3-opus-20240229\",  # Switch to Anthropic\n    messages=[{\"role\": \"user\", \"content\": \"Hello\"}]\n)\n</code></pre>"},{"location":"examples/openai-sdk/#best-practices","title":"Best Practices","text":""},{"location":"examples/openai-sdk/#1-handle-provider-specific-differences","title":"1. Handle Provider-Specific Differences","text":"<pre><code>def robust_chat_completion(model, messages, **kwargs):\n    \"\"\"Make chat completion with provider-specific handling.\"\"\"\n\n    try:\n        response = client.chat.completions.create(\n            model=model,\n            messages=messages,\n            **kwargs\n        )\n        return response\n\n    except Exception as e:\n        error_msg = str(e).lower()\n\n        # Handle common provider-specific errors\n        if \"context length\" in error_msg:\n            # Reduce max_tokens or message length\n            kwargs['max_tokens'] = min(kwargs.get('max_tokens', 4000), 2000)\n            return client.chat.completions.create(model=model, messages=messages, **kwargs)\n\n        elif \"rate limit\" in error_msg:\n            # Implement retry with backoff\n            import time\n            time.sleep(60)\n            return client.chat.completions.create(model=model, messages=messages, **kwargs)\n\n        else:\n            raise\n</code></pre>"},{"location":"examples/openai-sdk/#2-cost-tracking","title":"2. Cost Tracking","text":"<pre><code>def track_usage(response):\n    \"\"\"Track token usage and costs.\"\"\"\n\n    if hasattr(response, 'usage') and response.usage:\n        usage = response.usage\n\n        # Estimate cost (you'd get actual cost from IndoxRouter response headers)\n        print(f\"Tokens used: {usage.total_tokens}\")\n        print(f\"  Prompt: {usage.prompt_tokens}\")\n        print(f\"  Completion: {usage.completion_tokens}\")\n\n        # Note: Actual costs would be in IndoxRouter's response format\n        # when using the native client, not available in OpenAI SDK format\n\n# Usage\nresponse = client.chat.completions.create(\n    model=\"openai/gpt-4o-mini\",\n    messages=[{\"role\": \"user\", \"content\": \"Hello\"}]\n)\n\ntrack_usage(response)\n</code></pre>"},{"location":"examples/openai-sdk/#3-model-fallback","title":"3. Model Fallback","text":"<pre><code>def chat_with_fallback(messages, preferred_models, **kwargs):\n    \"\"\"Try multiple models as fallbacks.\"\"\"\n\n    for model in preferred_models:\n        try:\n            response = client.chat.completions.create(\n                model=model,\n                messages=messages,\n                **kwargs\n            )\n            print(f\"\u2705 Success with {model}\")\n            return response\n\n        except Exception as e:\n            print(f\"\u274c Failed with {model}: {e}\")\n            continue\n\n    raise Exception(\"All fallback models failed\")\n\n# Example usage\nfallback_models = [\n    \"openai/gpt-4o\",                      # Try premium first\n    \"openai/gpt-4o-mini\",                 # Fallback to cheaper\n    \"anthropic/claude-3-sonnet-20240229\", # Different provider\n    \"deepseek/deepseek-chat\"              # Most economical\n]\n\nresponse = chat_with_fallback(\n    messages=[{\"role\": \"user\", \"content\": \"Complex analysis task\"}],\n    preferred_models=fallback_models,\n    temperature=0.3,\n    max_tokens=1000\n)\n</code></pre> <p>This approach lets you use the familiar OpenAI SDK while getting access to all IndoxRouter providers and their cost tracking features!</p>"},{"location":"usage/basic-usage/","title":"Basic Usage","text":"<p>This page covers the fundamental patterns and concepts for using IndoxRouter effectively.</p>"},{"location":"usage/basic-usage/#client-initialization","title":"Client Initialization","text":"<pre><code>from indoxrouter import Client\n\n# Initialize with API key\nclient = Client(api_key=\"your_api_key\")\n</code></pre>"},{"location":"usage/basic-usage/#model-specification","title":"Model Specification","text":"<p>IndoxRouter uses a consistent format for specifying models: <code>provider/model_name</code>. This allows you to easily switch between providers while keeping your code structure the same.</p> <p>Examples:</p> <ul> <li><code>openai/gpt-4o-mini</code></li> <li><code>anthropic/claude-3-sonnet-20240229</code></li> <li><code>mistral/mistral-large-latest</code></li> <li><code>google/gemini-1.5-pro</code></li> </ul>"},{"location":"usage/basic-usage/#common-parameters","title":"Common Parameters","text":"<p>All API methods accept a set of common parameters:</p> <ul> <li><code>model</code>: The model to use in format <code>provider/model_name</code></li> <li><code>temperature</code>: Controls randomness (0-1). Lower values = more deterministic. Default is 0.7</li> <li><code>max_tokens</code>: Maximum number of tokens to generate</li> </ul> <p>Additional parameters specific to each provider can be passed as keyword arguments.</p>"},{"location":"usage/basic-usage/#response-structure","title":"Response Structure","text":"<p>Responses from the API closely follow the OpenAI API response format, with some additions for consistency across providers:</p> <pre><code>{'request_id': 'b881942c-e21d-4f9d-ad82-47344945c642',\n 'created_at': '2025-06-15T09:53:26.130868',\n 'duration_ms': 1737.612247467041,\n 'provider': 'openai',\n 'model': 'gpt-4o-mini',\n 'success': True,\n 'message': '',\n 'usage': {'tokens_prompt': 24,\n  'tokens_completion': 7,\n  'tokens_total': 31,\n  'cost': 7.8e-06,\n  'latency': 1.629077672958374,\n  'timestamp': '2025-06-15T09:53:26.114626',\n  'cache_read_tokens': 0,\n  'cache_write_tokens': 0,\n  'reasoning_tokens': 0,\n  'web_search_count': 0,\n  'request_count': 1,\n  'cost_breakdown': {'input_tokens': 3.6e-06,\n   'output_tokens': 4.2e-06,\n   'cache_read': 0.0,\n   'cache_write': 0.0,\n   'reasoning': 0.0,\n   'web_search': 0.0,\n   'request': 0.0}},\n 'raw_response': None,\n 'data': 'The capital of France is Paris.',\n 'finish_reason': None}\n</code></pre>"},{"location":"usage/basic-usage/#error-handling","title":"Error Handling","text":"<p>The client provides a set of specific exception classes for different error types:</p> <ul> <li><code>AuthenticationError</code>: Issues with API key or authentication</li> <li><code>ProviderNotFoundError</code>: The requested provider doesn't exist</li> <li><code>ModelNotFoundError</code>: The requested model doesn't exist</li> <li><code>InvalidParametersError</code>: The provided parameters are invalid</li> <li><code>RateLimitError</code>: The rate limit has been exceeded</li> <li><code>ProviderError</code>: An error occurred with the provider's service</li> <li><code>InsufficientCreditsError</code>: Not enough credits to complete the request</li> <li><code>NetworkError</code>: Network connectivity issues</li> </ul> <p>Example error handling:</p> <pre><code>from indoxrouter import Client, ModelNotFoundError, ProviderError\n\ntry:\n    client = Client(api_key=\"your_api_key\")\n    response = client.chat(\n        messages=[{\"role\": \"user\", \"content\": \"Hello\"}],\n        model=\"nonexistent-provider/nonexistent-model\"\n    )\nexcept ModelNotFoundError as e:\n    print(f\"Model not found: {e}\")\nexcept ProviderError as e:\n    print(f\"Provider error: {e}\")\n</code></pre>"},{"location":"usage/basic-usage/#debugging","title":"Debugging","text":"<p>If you're experiencing issues, you can enable debug logging:</p> <pre><code>import logging\nfrom indoxrouter import Client\n\n# Enable debug logging\nlogging.basicConfig(level=logging.DEBUG)\n\nclient = Client(api_key=\"your_api_key\")\nclient.enable_debug()  # This enables additional debugging information\n\n# Your code here\n</code></pre> <p>You can also use the <code>diagnose_request</code> method to get detailed information about a request without actually sending it:</p> <pre><code>diagnostic_info = client.diagnose_request(\n    \"chat/completions\",\n    {\n        \"messages\": [{\"role\": \"user\", \"content\": \"Hello\"}],\n        \"model\": \"openai/gpt-4o-mini\"\n    }\n)\nprint(diagnostic_info)\n</code></pre>"},{"location":"usage/basic-usage/#next-steps","title":"Next Steps","text":"<p>Now that you understand the basics, check out the detailed guides for each capability:</p> <ul> <li>Chat Completions</li> <li>Text Completions</li> <li>Embeddings</li> <li>Image Generation</li> <li>Video Generation</li> </ul>"},{"location":"usage/byok/","title":"BYOK (Bring Your Own Key) Support","text":"<p>IndoxRouter supports BYOK (Bring Your Own Key), allowing you to use your own API keys for AI providers instead of using the platform's shared API keys. This feature provides several benefits and gives you full control over your AI provider usage.</p>"},{"location":"usage/byok/#what-is-byok","title":"What is BYOK?","text":"<p>BYOK (Bring Your Own Key) allows you to:</p> <ul> <li>Use your own API keys for AI providers (OpenAI, Anthropic, Google, etc.)</li> <li>Bypass platform rate limits and use provider's native limits</li> <li>Avoid credit deduction from your IndoxRouter account</li> <li>Pay providers directly at their rates without platform markup</li> <li>Access full provider features without platform restrictions</li> </ul>"},{"location":"usage/byok/#how-byok-works","title":"How BYOK Works","text":"<p>When you provide a <code>byok_api_key</code> parameter:</p> <ol> <li>IndoxRouter authenticates you with your platform API key</li> <li>Your provider API key is used for the actual AI request</li> <li>No credits are deducted from your IndoxRouter account</li> <li>No rate limiting is applied by the platform</li> <li>Direct provider connection is established</li> </ol>"},{"location":"usage/byok/#supported-endpoints","title":"Supported Endpoints","text":"<p>All AI endpoints in IndoxRouter support BYOK:</p>"},{"location":"usage/byok/#chat-completions","title":"Chat Completions","text":"<pre><code>response = client.chat(\n    messages=[{\"role\": \"user\", \"content\": \"Hello!\"}],\n    model=\"openai/gpt-4\",\n    byok_api_key=\"sk-your-openai-key-here\"\n)\n</code></pre>"},{"location":"usage/byok/#text-completions","title":"Text Completions","text":"<pre><code>response = client.completion(\n    prompt=\"Complete this sentence:\",\n    model=\"openai/gpt-3.5-turbo-instruct\",\n    byok_api_key=\"sk-your-openai-key-here\"\n)\n</code></pre>"},{"location":"usage/byok/#embeddings","title":"Embeddings","text":"<pre><code>response = client.embeddings(\n    text=\"Text to embed\",\n    model=\"openai/text-embedding-3-small\",\n    byok_api_key=\"sk-your-openai-key-here\"\n)\n</code></pre>"},{"location":"usage/byok/#image-generation","title":"Image Generation","text":"<pre><code>response = client.images(\n    prompt=\"A beautiful sunset\",\n    model=\"openai/dall-e-3\",\n    size=\"1024x1024\",\n    byok_api_key=\"sk-your-openai-key-here\"\n)\n</code></pre>"},{"location":"usage/byok/#text-to-speech","title":"Text-to-Speech","text":"<pre><code>response = client.text_to_speech(\n    input=\"Hello, world!\",\n    model=\"openai/tts-1\",\n    voice=\"alloy\",\n    byok_api_key=\"sk-your-openai-key-here\"\n)\n</code></pre>"},{"location":"usage/byok/#provider-specific-examples","title":"Provider-Specific Examples","text":""},{"location":"usage/byok/#openai","title":"OpenAI","text":"<pre><code># Chat with GPT-4\nresponse = client.chat(\n    messages=[{\"role\": \"user\", \"content\": \"Explain quantum computing\"}],\n    model=\"openai/gpt-4\",\n    byok_api_key=\"sk-your-openai-key-here\"\n)\n\n# Generate images with DALL-E 3\nresponse = client.images(\n    prompt=\"A futuristic cityscape\",\n    model=\"openai/dall-e-3\",\n    size=\"1024x1024\",\n    byok_api_key=\"sk-your-openai-key-here\"\n)\n\n# Create embeddings\nresponse = client.embeddings(\n    text=\"Machine learning concepts\",\n    model=\"openai/text-embedding-3-small\",\n    byok_api_key=\"sk-your-openai-key-here\"\n)\n</code></pre>"},{"location":"usage/byok/#anthropic","title":"Anthropic","text":"<pre><code># Chat with Claude\nresponse = client.chat(\n    messages=[{\"role\": \"user\", \"content\": \"Write a story\"}],\n    model=\"anthropic/claude-3-sonnet-20240229\",\n    byok_api_key=\"sk-ant-your-anthropic-key-here\"\n)\n</code></pre>"},{"location":"usage/byok/#google","title":"Google","text":"<pre><code># Chat with Gemini\nresponse = client.chat(\n    messages=[{\"role\": \"user\", \"content\": \"Explain AI\"}],\n    model=\"google/gemini-1.5-pro\",\n    byok_api_key=\"your-google-api-key-here\"\n)\n\n# Generate images with Imagen\nresponse = client.images(\n    prompt=\"A mountain landscape\",\n    model=\"google/imagen-3.0-generate-002\",\n    aspect_ratio=\"16:9\",  # Google uses aspect ratios\n    byok_api_key=\"your-google-api-key-here\"\n)\n</code></pre>"},{"location":"usage/byok/#mistral","title":"Mistral","text":"<pre><code># Chat with Mistral\nresponse = client.chat(\n    messages=[{\"role\": \"user\", \"content\": \"Help with coding\"}],\n    model=\"mistral/mistral-large-latest\",\n    byok_api_key=\"your-mistral-api-key-here\"\n)\n</code></pre>"},{"location":"usage/byok/#benefits-of-byok","title":"Benefits of BYOK","text":""},{"location":"usage/byok/#cost-savings","title":"Cost Savings","text":"<ul> <li>No platform markup on API calls</li> <li>Direct provider billing at their rates</li> <li>No credit consumption from your IndoxRouter account</li> <li>Predictable costs based on provider pricing</li> </ul>"},{"location":"usage/byok/#performance-limits","title":"Performance &amp; Limits","text":"<ul> <li>Higher rate limits using provider's native limits</li> <li>No platform bottlenecks or rate limiting</li> <li>Direct provider connection for faster response times</li> <li>Full provider capabilities without restrictions</li> </ul>"},{"location":"usage/byok/#control-flexibility","title":"Control &amp; Flexibility","text":"<ul> <li>Full control over your provider accounts</li> <li>Direct access to provider dashboards and analytics</li> <li>Provider-specific features and parameters</li> <li>No dependency on platform availability</li> </ul>"},{"location":"usage/byok/#use-cases","title":"Use Cases","text":"<ul> <li>High-volume applications that need higher rate limits</li> <li>Cost-sensitive projects that benefit from direct provider pricing</li> <li>Enterprise applications that require full provider control</li> <li>Development and testing with your own API keys</li> </ul>"},{"location":"usage/byok/#security-considerations","title":"Security Considerations","text":""},{"location":"usage/byok/#api-key-management","title":"API Key Management","text":"<ul> <li>Keep your API keys secure and never expose them in client-side code</li> <li>Use environment variables for storing API keys</li> <li>Rotate keys regularly for security best practices</li> <li>Monitor usage through provider dashboards</li> </ul>"},{"location":"usage/byok/#best-practices","title":"Best Practices","text":"<pre><code>import os\n\n# Store API keys in environment variables\nOPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\nANTHROPIC_API_KEY = os.getenv(\"ANTHROPIC_API_KEY\")\n\n# Use in requests\nresponse = client.chat(\n    messages=[{\"role\": \"user\", \"content\": \"Hello\"}],\n    model=\"openai/gpt-4\",\n    byok_api_key=OPENAI_API_KEY\n)\n</code></pre>"},{"location":"usage/byok/#troubleshooting","title":"Troubleshooting","text":""},{"location":"usage/byok/#common-issues","title":"Common Issues","text":""},{"location":"usage/byok/#invalid-api-key","title":"Invalid API Key","text":"<pre><code># Ensure your API key is valid and has proper permissions\nresponse = client.chat(\n    messages=[{\"role\": \"user\", \"content\": \"Test\"}],\n    model=\"openai/gpt-4\",\n    byok_api_key=\"sk-valid-openai-key-here\"\n)\n</code></pre>"},{"location":"usage/byok/#provider-mismatch","title":"Provider Mismatch","text":"<pre><code># Ensure the model matches the provider of your API key\n# OpenAI key with OpenAI model\nresponse = client.chat(\n    messages=[{\"role\": \"user\", \"content\": \"Test\"}],\n    model=\"openai/gpt-4\",  # \u2705 Correct\n    byok_api_key=\"sk-openai-key-here\"\n)\n\n# Don't use OpenAI key with Anthropic model\n# response = client.chat(\n#     messages=[{\"role\": \"user\", \"content\": \"Test\"}],\n#     model=\"anthropic/claude-3\",  # \u274c Wrong provider\n#     byok_api_key=\"sk-openai-key-here\"\n# )\n</code></pre>"},{"location":"usage/byok/#rate-limiting","title":"Rate Limiting","text":"<ul> <li>BYOK bypasses platform rate limits but not provider rate limits</li> <li>Check your provider's rate limits in their dashboard</li> <li>Monitor usage to avoid hitting provider limits</li> </ul>"},{"location":"usage/byok/#debug-information","title":"Debug Information","text":"<p>When using BYOK, responses include:</p> <ul> <li><code>\"byok_api_key\": true</code> in the response data</li> <li><code>X-RateLimit-Bypass: BYOK API key used</code> in response headers</li> <li>Cost field set to 0 in usage statistics</li> </ul>"},{"location":"usage/byok/#migration-guide","title":"Migration Guide","text":""},{"location":"usage/byok/#from-platform-keys-to-byok","title":"From Platform Keys to BYOK","text":"<ol> <li>Get API keys from your preferred providers</li> <li>Update your code to include <code>byok_api_key</code> parameter</li> <li>Test with small requests to ensure everything works</li> <li>Monitor costs through provider dashboards</li> <li>Scale up as needed</li> </ol>"},{"location":"usage/byok/#example-migration","title":"Example Migration","text":"<p>Before (Platform Keys):</p> <pre><code>response = client.chat(\n    messages=[{\"role\": \"user\", \"content\": \"Hello\"}],\n    model=\"openai/gpt-4\"\n)\n</code></pre> <p>After (BYOK):</p> <pre><code>response = client.chat(\n    messages=[{\"role\": \"user\", \"content\": \"Hello\"}],\n    model=\"openai/gpt-4\",\n    byok_api_key=\"sk-your-openai-key-here\"\n)\n</code></pre>"},{"location":"usage/byok/#next-steps","title":"Next Steps","text":"<ul> <li>Basic Usage: Learn the fundamentals of IndoxRouter</li> <li>Chat Completions: Detailed chat completion examples</li> <li>Image Generation: Create images with BYOK support</li> <li>Embeddings: Generate embeddings with your own keys</li> <li>Text-to-Speech: Convert text to speech with BYOK</li> </ul> <p>BYOK support is available for all IndoxRouter AI endpoints. Start using your own API keys today for better control, cost savings, and performance.</p>"},{"location":"usage/chat/","title":"Chat Completions","text":"<p>Chat completions are the primary way to interact with conversational AI models like GPT-4, Claude, and Gemini. This guide covers how to use the chat completions feature of the IndoxRouter Client.</p>"},{"location":"usage/chat/#basic-usage","title":"Basic Usage","text":"<p>The simplest way to use chat completions is with the <code>chat()</code> method:</p> <pre><code>from indoxrouter import Client\n\nclient = Client(api_key=\"your_api_key\")\n\nresponse = client.chat(\n    messages=[\n        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n        {\"role\": \"user\", \"content\": \"Tell me a joke about programming.\"}\n    ],\n    model=\"openai/gpt-4o-mini\"\n)\n\nprint(response[\"data\"])\n</code></pre>"},{"location":"usage/chat/#message-format","title":"Message Format","text":"<p>The <code>messages</code> parameter is a list of dictionaries, each with <code>role</code> and <code>content</code> keys:</p> <ul> <li><code>role</code>: Can be one of \"system\", \"user\", \"assistant\", or \"function\"</li> <li><code>content</code>: The content of the message (string for text-only, or list for multimodal)</li> </ul> <p>Example message formats:</p> <pre><code># System message (instructions to the AI)\n{\"role\": \"system\", \"content\": \"You are a helpful assistant.\"}\n\n# User message (the user's input)\n{\"role\": \"user\", \"content\": \"What's the weather like today?\"}\n\n# Assistant message (previous responses from the assistant)\n{\"role\": \"assistant\", \"content\": \"I don't have access to current weather information.\"}\n\n# Function message (for function calling, when available)\n{\"role\": \"function\", \"name\": \"get_weather\", \"content\": '{\"temperature\": 72, \"condition\": \"sunny\"}'}\n</code></pre>"},{"location":"usage/chat/#multimodal-messages-text-images","title":"Multimodal Messages (Text + Images)","text":"<p>For vision-capable models, you can send images along with text by using a list format for the content:</p> <pre><code>import base64\n\n# Read and encode image\nwith open(\"image.jpg\", \"rb\") as f:\n    image_base64 = base64.b64encode(f.read()).decode('utf-8')\n\n# Multimodal message with text and image\n{\n    \"role\": \"user\",\n    \"content\": [\n        {\n            \"type\": \"text\",\n            \"text\": \"What's in this image?\"\n        },\n        {\n            \"type\": \"image\",\n            \"image\": {\n                \"data\": image_base64,\n                \"media_type\": \"image/jpeg\"\n            }\n        }\n    ]\n}\n</code></pre> <p>Vision Models</p> <p>Not all models support image inputs. Vision-capable models include <code>gpt-4o</code>, <code>claude-sonnet-4.5</code>, <code>gemini-2.0-flash</code>, and many others. See the Vision &amp; Multimodal guide for complete documentation.</p>"},{"location":"usage/chat/#model-selection","title":"Model Selection","text":"<p>You can specify different models using the <code>provider/model_name</code> format:</p> <pre><code># OpenAI\nresponse = client.chat(\n    messages=[{\"role\": \"user\", \"content\": \"Hello\"}],\n    model=\"openai/gpt-4o-mini\"\n)\n\n# Anthropic\nresponse = client.chat(\n    messages=[{\"role\": \"user\", \"content\": \"Hello\"}],\n    model=\"anthropic/claude-3-sonnet-20240229\"\n)\n\n# Google\nresponse = client.chat(\n    messages=[{\"role\": \"user\", \"content\": \"Hello\"}],\n    model=\"google/gemini-1.5-pro\"\n)\n\n# Mistral\nresponse = client.chat(\n    messages=[{\"role\": \"user\", \"content\": \"Hello\"}],\n    model=\"mistral/mistral-large-latest\"\n)\n</code></pre>"},{"location":"usage/chat/#common-parameters","title":"Common Parameters","text":"<p>The chat method accepts several parameters to control the generation:</p> <pre><code>response = client.chat(\n    messages=[{\"role\": \"user\", \"content\": \"Write a poem about AI.\"}],\n    model=\"openai/gpt-4o-mini\",\n    temperature=0.7,  # Controls randomness (0-1)\n    max_tokens=100,   # Maximum number of tokens to generate\n    stream=False,     # Whether to stream the response\n)\n</code></pre>"},{"location":"usage/chat/#byok-bring-your-own-key-support","title":"BYOK (Bring Your Own Key) Support","text":"<p>IndoxRouter supports BYOK, allowing you to use your own API keys for AI providers. This bypasses platform rate limits and credit deductions:</p> <pre><code># Use your own OpenAI API key\nresponse = client.chat(\n    messages=[{\"role\": \"user\", \"content\": \"Hello!\"}],\n    model=\"openai/gpt-4\",\n    byok_api_key=\"sk-your-openai-key-here\"\n)\n\n# Use your own Anthropic API key\nresponse = client.chat(\n    messages=[{\"role\": \"user\", \"content\": \"Tell me a story\"}],\n    model=\"anthropic/claude-3-sonnet-20240229\",\n    byok_api_key=\"sk-ant-your-anthropic-key-here\"\n)\n\n# Use your own Google API key\nresponse = client.chat(\n    messages=[{\"role\": \"user\", \"content\": \"Explain quantum computing\"}],\n    model=\"google/gemini-1.5-pro\",\n    byok_api_key=\"your-google-api-key-here\"\n)\n</code></pre>"},{"location":"usage/chat/#byok-benefits-for-chat","title":"BYOK Benefits for Chat","text":"<ul> <li>No Credit Deduction: Your IndoxRouter credits remain unchanged</li> <li>No Rate Limiting: Bypass platform rate limits</li> <li>Direct Provider Access: Connect directly to your provider accounts</li> <li>Cost Control: Pay providers directly at their rates</li> <li>Full Features: Access to all provider-specific chat features</li> </ul>"},{"location":"usage/chat/#streaming-responses","title":"Streaming Responses","text":"<p>For long responses, you might want to stream the response to get it piece by piece:</p> <pre><code>print(\"Streaming response:\")\nfor chunk in client.chat(\n    messages=[\n        {\"role\": \"user\", \"content\": \"Tell me a story about a robot in 5 sentences.\"}\n    ],\n    model=\"mistral/ministral-8b-latest\",\n    stream=True\n):\n    if isinstance(chunk, dict) and \"data\" in chunk:\n        print(chunk[\"data\"], end=\"\", flush=True)\n    else:\n        print(chunk, end=\"\", flush=True)\nprint(\"\\nStreaming complete!\")\n</code></pre>"},{"location":"usage/chat/#managing-conversations","title":"Managing Conversations","text":"<p>For multi-turn conversations, you'll need to keep track of the message history:</p> <pre><code># Initialize the conversation with a system message\nmessages = [\n    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"}\n]\n\n# First user message\nmessages.append({\"role\": \"user\", \"content\": \"Hello, who are you?\"})\nresponse = client.chat(messages=messages, model=\"openai/gpt-4o-mini\")\nassistant_response = response[\"choices\"][0][\"message\"][\"content\"]\nmessages.append({\"role\": \"assistant\", \"content\": assistant_response})\nprint(f\"Assistant: {assistant_response}\")\n\n# Second user message\nmessages.append({\"role\": \"user\", \"content\": \"What can you help me with?\"})\nresponse = client.chat(messages=messages, model=\"openai/gpt-4o-mini\")\nassistant_response = response[\"choices\"][0][\"message\"][\"content\"]\nmessages.append({\"role\": \"assistant\", \"content\": assistant_response})\nprint(f\"Assistant: {assistant_response}\")\n</code></pre>"},{"location":"usage/chat/#response-format","title":"Response Format","text":"<p>The response from the chat method follows this structure:</p> <pre><code>{'request_id': 'b881942c-e21d-4f9d-ad82-47344945c642',\n 'created_at': '2025-06-15T09:53:26.130868',\n 'duration_ms': 1737.612247467041,\n 'provider': 'openai',\n 'model': 'gpt-4o-mini',\n 'success': True,\n 'message': '',\n 'usage': {'tokens_prompt': 24,\n  'tokens_completion': 7,\n  'tokens_total': 31,\n  'cost': 7.8e-06,\n  'latency': 1.629077672958374,\n  'timestamp': '2025-06-15T09:53:26.114626',\n  'cache_read_tokens': 0,\n  'cache_write_tokens': 0,\n  'reasoning_tokens': 0,\n  'web_search_count': 0,\n  'request_count': 1,\n  'cost_breakdown': {'input_tokens': 3.6e-06,\n   'output_tokens': 4.2e-06,\n   'cache_read': 0.0,\n   'cache_write': 0.0,\n   'reasoning': 0.0,\n   'web_search': 0.0,\n   'request': 0.0}},\n 'raw_response': None,\n 'data': 'The capital of France is Paris.',\n 'finish_reason': None}\n</code></pre>"},{"location":"usage/completions/","title":"Text Completions","text":"<p>IndoxRouter supports text completion endpoints for generating text based on prompts.</p>"},{"location":"usage/completions/#basic-text-completion","title":"Basic Text Completion","text":"<pre><code>from indoxrouter import Client\n\nclient = Client(api_key=\"your_api_key\")\n\n# Generate text completion\nresponse = client.completions(\n    prompt=\"The future of artificial intelligence is\",\n    model=\"openai/gpt-3.5-turbo-instruct\",\n    max_tokens=100,\n    temperature=0.7\n)\n\nprint(\"Response:\", response[\"data\"])\n</code></pre>"},{"location":"usage/completions/#parameters","title":"Parameters","text":"<ul> <li><code>prompt</code>: The text prompt to complete</li> <li><code>model</code>: The model to use for completion</li> <li><code>max_tokens</code>: Maximum number of tokens to generate</li> <li><code>temperature</code>: Controls randomness (0.0 to 2.0)</li> <li><code>top_p</code>: Controls diversity via nucleus sampling</li> <li><code>frequency_penalty</code>: Penalizes frequent tokens</li> <li><code>presence_penalty</code>: Penalizes new tokens</li> </ul>"},{"location":"usage/completions/#byok-bring-your-own-key-support","title":"BYOK (Bring Your Own Key) Support","text":"<p>IndoxRouter supports BYOK for text completions, allowing you to use your own API keys for AI providers:</p> <pre><code># Use your own OpenAI API key for completions\nresponse = client.completion(\n    prompt=\"The future of artificial intelligence is\",\n    model=\"openai/gpt-3.5-turbo-instruct\",\n    max_tokens=100,\n    temperature=0.7,\n    byok_api_key=\"sk-your-openai-key-here\"\n)\n\n# Use your own Anthropic API key for completions\nresponse = client.completion(\n    prompt=\"Explain quantum computing in simple terms:\",\n    model=\"anthropic/claude-3-sonnet-20240229\",\n    max_tokens=150,\n    byok_api_key=\"sk-ant-your-anthropic-key-here\"\n)\n\n# Use your own Google API key for completions\nresponse = client.completion(\n    prompt=\"Write a short story about a robot:\",\n    model=\"google/gemini-1.5-pro\",\n    max_tokens=200,\n    byok_api_key=\"your-google-api-key-here\"\n)\n</code></pre>"},{"location":"usage/completions/#byok-benefits-for-text-completions","title":"BYOK Benefits for Text Completions","text":"<ul> <li>No Credit Deduction: Your IndoxRouter credits remain unchanged</li> <li>No Rate Limiting: Bypass platform rate limits</li> <li>Direct Provider Access: Connect directly to your provider accounts</li> <li>Cost Control: Pay providers directly at their rates</li> <li>Full Features: Access to all provider-specific completion features</li> </ul>"},{"location":"usage/completions/#example-use-cases","title":"Example Use Cases","text":""},{"location":"usage/completions/#creative-writing","title":"Creative Writing","text":"<pre><code>response = client.completions(\n    prompt=\"Once upon a time in a magical forest,\",\n    model=\"openai/gpt-3.5-turbo-instruct\",\n    max_tokens=200,\n    temperature=0.9\n)\n</code></pre>"},{"location":"usage/completions/#code-generation","title":"Code Generation","text":"<pre><code>response = client.completions(\n    prompt=\"# Python function to calculate fibonacci numbers\\ndef fibonacci(n):\",\n    model=\"openai/gpt-3.5-turbo-instruct\",\n    max_tokens=150,\n    temperature=0.2\n)\n</code></pre>"},{"location":"usage/embeddings/","title":"Embeddings","text":"<p>Embeddings are vector representations of text that capture semantic meaning, making them useful for similarity search, clustering, classification, and retrieval applications. This guide covers how to use the embeddings feature of IndoxRouter.</p>"},{"location":"usage/embeddings/#basic-usage","title":"Basic Usage","text":"<p>The simplest way to generate embeddings is with the <code>embeddings()</code> method:</p> <pre><code>from indoxrouter import Client\n\nclient = Client(api_key=\"your_api_key\")\n\n# Generate embeddings for a single text\nresponse = client.embeddings(\n    text=\"Machine learning is a field of study that gives computers the ability to learn without being explicitly programmed.\",\n    model=\"openai/text-embedding-3-small\"\n)\n\n# Print the embedding dimensions\nprint(f\"Embedding dimensions: {len(response['data'][0]['embedding'])}\")\nprint(f\"First few dimensions: {response['data'][0]['embedding'][:5]}\")\n</code></pre>"},{"location":"usage/embeddings/#processing-multiple-texts","title":"Processing Multiple Texts","text":"<p>You can generate embeddings for multiple texts in a single request by passing a list of strings:</p> <pre><code>from indoxrouter import Client\n\nclient = Client(api_key=\"your_api_key\")\n\n# Generate embeddings for multiple texts\nresponse = client.embeddings(\n    text=[\n        \"Artificial intelligence is revolutionizing industries worldwide.\",\n        \"Natural language processing helps computers understand human language.\",\n        \"Machine learning algorithms improve with more training data.\"\n    ],\n    model=\"openai/text-embedding-3-small\"\n)\n\n# Print information about the embeddings\nfor i, item in enumerate(response[\"data\"]):\n    embedding = item[\"embedding\"]\n    print(f\"Text {i+1}: Dimensions: {len(embedding)}\")\n</code></pre>"},{"location":"usage/embeddings/#model-selection","title":"Model Selection","text":"<p>You can select different embedding models from various providers:</p> <pre><code># OpenAI\nopenai_response = client.embeddings(\n    text=\"Example text for embedding\",\n    model=\"openai/text-embedding-3-small\"\n)\n\n# OpenAI larger model\nopenai_large_response = client.embeddings(\n    text=\"Example text for embedding\",\n    model=\"openai/text-embedding-3-large\"\n)\n\n# Google\ngoogle_response = client.embeddings(\n    text=\"Example text for embedding\",\n    model=\"google/text-embedding-gecko\"\n)\n\n# Mistral\nmistral_response = client.embeddings(\n    text=\"Example text for embedding\",\n    model=\"mistral/mistral-embed\"\n)\n</code></pre>"},{"location":"usage/embeddings/#byok-bring-your-own-key-support","title":"BYOK (Bring Your Own Key) Support","text":"<p>IndoxRouter supports BYOK for embeddings, allowing you to use your own API keys for AI providers:</p> <pre><code># Use your own OpenAI API key for embeddings\nresponse = client.embeddings(\n    text=\"Machine learning is transforming industries\",\n    model=\"openai/text-embedding-3-small\",\n    byok_api_key=\"sk-your-openai-key-here\"\n)\n\n# Use your own Google API key for embeddings\nresponse = client.embeddings(\n    text=\"Natural language processing examples\",\n    model=\"google/text-embedding-gecko\",\n    byok_api_key=\"your-google-api-key-here\"\n)\n\n# Use your own Mistral API key for embeddings\nresponse = client.embeddings(\n    text=\"AI and machine learning concepts\",\n    model=\"mistral/mistral-embed\",\n    byok_api_key=\"your-mistral-api-key-here\"\n)\n</code></pre>"},{"location":"usage/embeddings/#byok-benefits-for-embeddings","title":"BYOK Benefits for Embeddings","text":"<ul> <li>No Credit Deduction: Your IndoxRouter credits remain unchanged</li> <li>No Rate Limiting: Bypass platform rate limits</li> <li>Direct Provider Access: Connect directly to your provider accounts</li> <li>Cost Control: Pay providers directly at their rates</li> <li>Full Features: Access to all provider-specific embedding features</li> <li>Higher Quality: Use provider's native embedding capabilities</li> </ul>"},{"location":"usage/embeddings/#response-format","title":"Response Format","text":"<p>The response from the embeddings method follows this structure:</p> <pre><code>{\n    \"id\": \"embd-123456789\",\n    \"object\": \"embedding\",\n    \"created\": 1684936116,\n    \"model\": \"openai/text-embedding-3-small\",\n    \"data\": [\n        {\n            \"embedding\": [0.002345, -0.012345, 0.123456, ...],  # Vector of n dimensions\n            \"index\": 0\n        },\n        # More items if multiple texts were provided\n    ],\n    \"usage\": {\n        \"prompt_tokens\": 10,\n        \"total_tokens\": 10\n    }\n}\n</code></pre>"},{"location":"usage/embeddings/#working-with-embeddings","title":"Working with Embeddings","text":""},{"location":"usage/embeddings/#calculating-similarity","title":"Calculating Similarity","text":"<p>Once you have embeddings, you can calculate similarity between them using cosine similarity:</p> <pre><code>import numpy as np\nfrom scipy.spatial.distance import cosine\n\ndef cosine_similarity(a, b):\n    return 1 - cosine(a, b)\n\n# Get embeddings for two texts\nresponse = client.embeddings(\n    text=[\n        \"The weather is quite nice today.\",\n        \"Today's weather is pleasant.\"\n    ],\n    model=\"openai/text-embedding-3-small\"\n)\n\n# Extract the embedding vectors\nembedding1 = response[\"data\"][0][\"embedding\"]\nembedding2 = response[\"data\"][1][\"embedding\"]\n\n# Calculate similarity\nsimilarity = cosine_similarity(embedding1, embedding2)\nprint(f\"Similarity: {similarity:.4f}\")  # Higher value means more similar\n</code></pre>"},{"location":"usage/embeddings/#building-a-simple-rag-system","title":"Building a Simple RAG System","text":"<p>Here's a basic example of using embeddings for a simple retrieval-augmented generation (RAG) system:</p> <pre><code>import numpy as np\nfrom sklearn.metrics.pairwise import cosine_similarity\n\n# Sample knowledge base\ndocuments = [\n    \"Machine learning is a field of study that gives computers the ability to learn without being explicitly programmed.\",\n    \"Natural language processing is a subfield of linguistics, computer science, and artificial intelligence.\",\n    \"Neural networks are computing systems inspired by the biological neural networks in animal brains.\",\n    \"Deep learning is part of a broader family of machine learning methods based on artificial neural networks.\",\n    \"Reinforcement learning is the training of machine learning models to make a sequence of decisions.\"\n]\n\nclient = Client(api_key=\"your_api_key\")\n\n# Step 1: Generate embeddings for our knowledge base\ndocs_response = client.embeddings(\n    text=documents,\n    model=\"openai/text-embedding-3-small\"\n)\n\n# Extract embeddings\ndoc_embeddings = np.array([item[\"embedding\"] for item in docs_response[\"data\"]])\n\n# Step 2: Process a query\nquery = \"How do computers learn without explicit programming?\"\n\n# Generate embedding for the query\nquery_response = client.embeddings(\n    text=query,\n    model=\"openai/text-embedding-3-small\"\n)\nquery_embedding = np.array(query_response[\"data\"][0][\"embedding\"])\n\n# Step 3: Find the most similar document\nsimilarities = cosine_similarity([query_embedding], doc_embeddings)[0]\nmost_similar_index = np.argmax(similarities)\nmost_similar_doc = documents[most_similar_index]\n\nprint(f\"Query: {query}\")\nprint(f\"Most relevant document: {most_similar_doc}\")\nprint(f\"Similarity score: {similarities[most_similar_index]:.4f}\")\n\n# Step 4: Generate an answer using the most relevant document as context\nresponse = client.chat(\n    messages=[\n        {\"role\": \"system\", \"content\": \"You are a helpful assistant. Use the provided context to answer the question.\"},\n        {\"role\": \"user\", \"content\": f\"Context: {most_similar_doc}\\n\\nQuestion: {query}\"}\n    ],\n    model=\"openai/gpt-4o-mini\"\n)\n\nprint(\"\\nGenerated Answer:\")\nprint(response[\"choices\"][0][\"message\"][\"content\"])\n</code></pre>"},{"location":"usage/embeddings/#advanced-usage","title":"Advanced Usage","text":""},{"location":"usage/embeddings/#chunking-large-documents","title":"Chunking Large Documents","text":"<p>For practical applications, you'll often need to chunk large documents before creating embeddings:</p> <pre><code>def chunk_text(text, chunk_size=1000, overlap=100):\n    \"\"\"Split text into overlapping chunks.\"\"\"\n    chunks = []\n    for i in range(0, len(text), chunk_size - overlap):\n        chunk = text[i:i + chunk_size]\n        if len(chunk) &lt; 100:  # Skip very small chunks at the end\n            continue\n        chunks.append(chunk)\n    return chunks\n\n# Example usage\nlarge_document = \"\"\"\n[Your long document text here...]\n\"\"\"\n\nchunks = chunk_text(large_document)\nprint(f\"Document split into {len(chunks)} chunks\")\n\n# Generate embeddings for each chunk\nchunk_response = client.embeddings(\n    text=chunks,\n    model=\"openai/text-embedding-3-small\"\n)\n\nchunk_embeddings = [item[\"embedding\"] for item in chunk_response[\"data\"]]\n</code></pre>"},{"location":"usage/embeddings/#storing-embeddings","title":"Storing Embeddings","text":"<p>For production applications, you would typically store embeddings in a vector database:</p> <pre><code># Pseudocode for storing embeddings in a vector database\n# Replace with actual implementation for your chosen database\n\n# Generate embeddings\nresponse = client.embeddings(\n    text=documents,\n    model=\"openai/text-embedding-3-small\"\n)\n\n# Store in vector database\nfor i, doc in enumerate(documents):\n    vector = response[\"data\"][i][\"embedding\"]\n    doc_id = f\"doc_{i}\"\n    vector_db.insert(\n        id=doc_id,\n        vector=vector,\n        metadata={\"text\": doc}\n    )\n</code></pre>"},{"location":"usage/embeddings/#best-practices","title":"Best Practices","text":"<ol> <li>Choose the right model: Different embedding models have different dimensions and performance characteristics</li> <li>Normalize text: Clean and normalize text before generating embeddings</li> <li>Chunk large documents: Split large texts into smaller chunks</li> <li>Cache embeddings: Store embeddings to avoid regenerating them for the same content</li> <li>Use appropriate similarity metrics: Cosine similarity is common, but other metrics might be better for specific use cases</li> <li>Consider dimensionality reduction: For very large collections, consider techniques like PCA to reduce embedding dimensions</li> </ol>"},{"location":"usage/images/","title":"Image Generation","text":"<p>IndoxRouter provides a unified interface for generating images from text prompts across various AI providers. This guide covers how to use the image generation capabilities.</p>"},{"location":"usage/images/#basic-usage","title":"Basic Usage","text":"<p>The simplest way to generate images is with the <code>images()</code> method:</p> <pre><code>from indoxrouter import Client\n\nclient = Client(api_key=\"your_api_key\")\n\n# Generate an image\nresponse = client.images(\n    prompt=\"A serene mountain landscape with a lake at sunset\",\n    model=\"openai/dall-e-3\"\n)\n\n# Get the image URL\nimage_url = response[\"data\"][0][\"url\"]\nprint(f\"Generated image URL: {image_url}\")\n</code></pre>"},{"location":"usage/images/#model-selection","title":"Model Selection","text":"<p>You can use different image generation models from various providers:</p> <pre><code># OpenAI DALL-E 3\ndalle3_response = client.images(\n    prompt=\"A futuristic city with flying cars\",\n    model=\"openai/dall-e-3\"\n)\n\n# OpenAI DALL-E 2\ndalle2_response = client.images(\n    prompt=\"A futuristic city with flying cars\",\n    model=\"openai/dall-e-2\"\n)\n\n# Stability AI\nstability_response = client.images(\n    prompt=\"A futuristic city with flying cars\",\n    model=\"stability/stable-diffusion-xl\"\n)\n</code></pre>"},{"location":"usage/images/#image-parameters","title":"Image Parameters","text":"<p>The image generation method accepts several parameters to control the output:</p> <pre><code>response = client.images(\n    prompt=\"A photorealistic portrait of a cyberpunk character\",\n    model=\"openai/dall-e-3\",\n    size=\"1024x1024\",  # Image dimensions\n    n=1,               # Number of images to generate\n    quality=\"hd\",      # Image quality (standard or hd)\n    style=\"vivid\"      # Image style (vivid or natural)\n)\n</code></pre>"},{"location":"usage/images/#common-parameters","title":"Common Parameters","text":"<ul> <li><code>prompt</code>: The text description of the image to generate</li> <li><code>model</code>: The model to use in format <code>provider/model_name</code></li> <li><code>size</code>: Image dimensions in format <code>widthxheight</code> (e.g., \"1024x1024\", \"512x512\")</li> <li><code>n</code>: Number of images to generate</li> <li><code>quality</code>: Image quality level (model dependent)</li> <li><code>style</code>: Image style (model dependent)</li> </ul>"},{"location":"usage/images/#byok-bring-your-own-key-support","title":"BYOK (Bring Your Own Key) Support","text":"<p>IndoxRouter supports BYOK for image generation, allowing you to use your own API keys for AI providers:</p> <pre><code># Use your own OpenAI API key for DALL-E 3\nresponse = client.images(\n    prompt=\"A futuristic cityscape at sunset\",\n    model=\"openai/dall-e-3\",\n    size=\"1024x1024\",\n    byok_api_key=\"sk-your-openai-key-here\"\n)\n</code></pre>"},{"location":"usage/images/#byok-benefits-for-image-generation","title":"BYOK Benefits for Image Generation","text":"<ul> <li>No Credit Deduction: Your IndoxRouter credits remain unchanged</li> <li>No Rate Limiting: Bypass platform rate limits</li> <li>Direct Provider Access: Connect directly to your provider accounts</li> <li>Cost Control: Pay providers directly at their rates</li> <li>Full Features: Access to all provider-specific image generation features</li> <li>Higher Quality: Use provider's native image generation capabilities</li> </ul>"},{"location":"usage/images/#response-format","title":"Response Format","text":"<p>The response from the images method follows this structure:</p> <pre><code>{\n    \"created\": 1684939249,\n    \"data\": [\n        {\n            \"url\": \"https://example.com/generated-image-123.png\",\n            \"revised_prompt\": \"A serene mountain landscape with a crystal-clear lake reflecting the sunset colors, surrounded by pine trees and snow-capped peaks\",\n            \"index\": 0\n        }\n        # More items if n &gt; 1\n    ]\n}\n</code></pre>"},{"location":"usage/images/#saving-generated-images","title":"Saving Generated Images","text":"<p>To save the generated images locally:</p> <pre><code>import requests\nimport os\n\ndef save_image(url, filename):\n    \"\"\"Download and save an image from URL to a local file.\"\"\"\n    response = requests.get(url)\n    if response.status_code == 200:\n        with open(filename, 'wb') as f:\n            f.write(response.content)\n        print(f\"Image saved as {filename}\")\n    else:\n        print(f\"Failed to download image: {response.status_code}\")\n\n# Generate an image\nresponse = client.images(\n    prompt=\"A colorful abstract painting with geometric shapes\",\n    model=\"openai/dall-e-3\"\n)\n\n# Save each generated image\nos.makedirs(\"generated_images\", exist_ok=True)\nfor i, item in enumerate(response[\"data\"]):\n    image_url = item[\"url\"]\n    filename = f\"generated_images/image_{i}.png\"\n    save_image(image_url, filename)\n</code></pre>"},{"location":"usage/images/#advanced-usage","title":"Advanced Usage","text":""},{"location":"usage/images/#generating-variations","title":"Generating Variations","text":"<p>Some models support generating variations of existing images. This feature may be added in the future.</p>"},{"location":"usage/images/#image-to-image-generation","title":"Image-to-Image Generation","text":"<p>Some models support image-to-image generation, where an input image is transformed based on a prompt. This feature may be added in the future.</p>"},{"location":"usage/images/#examples","title":"Examples","text":""},{"location":"usage/images/#detailed-art-generation","title":"Detailed Art Generation","text":"<pre><code># Generate a detailed art piece\nart_response = client.images(\n    prompt=(\n        \"An intricate fantasy illustration of an ancient library filled with \"\n        \"magical books, glowing orbs of light floating through the air, towering \"\n        \"bookshelves reaching into a starry sky ceiling, and a wizard studying \"\n        \"at an ornate desk\"\n    ),\n    model=\"openai/dall-e-3\",\n    size=\"1024x1024\",\n    quality=\"hd\",\n    style=\"vivid\"\n)\n\nprint(f\"Image URL: {art_response['data'][0]['url']}\")\nprint(f\"Revised prompt: {art_response['data'][0].get('revised_prompt', 'Not available')}\")\n</code></pre>"},{"location":"usage/images/#product-visualization","title":"Product Visualization","text":"<pre><code># Generate a product visualization\nproduct_response = client.images(\n    prompt=(\n        \"A professional product photography shot of a minimalist smartwatch \"\n        \"with a sleek black band and circular face displaying a digital time \"\n        \"against a clean white background, studio lighting\"\n    ),\n    model=\"openai/dall-e-3\",\n    size=\"1024x1024\",\n    quality=\"hd\",\n    style=\"natural\"\n)\n\nprint(f\"Product image URL: {product_response['data'][0]['url']}\")\n</code></pre>"},{"location":"usage/images/#architectural-concept","title":"Architectural Concept","text":"<pre><code># Generate an architectural concept\narchitecture_response = client.images(\n    prompt=(\n        \"A modern, sustainable treehouse design integrated into a forest canopy, \"\n        \"featuring large windows, solar panels, natural wood materials, \"\n        \"and connected walkways between tree platforms\"\n    ),\n    model=\"openai/dall-e-3\",\n    size=\"1024x1024\"\n)\n\nprint(f\"Architecture concept URL: {architecture_response['data'][0]['url']}\")\n</code></pre>"},{"location":"usage/images/#best-practices","title":"Best Practices","text":"<ol> <li>Be detailed and specific: The more detailed your prompt, the better the results</li> <li>Consider the style: Specify the artistic style, medium, lighting, and mood</li> <li>Experiment with parameters: Try different models, sizes, and quality settings</li> <li>Use appropriate models: Choose models based on your needs and budget</li> <li>Review revised prompts: Some models provide revised prompts that can help you understand how your prompt was interpreted</li> </ol>"},{"location":"usage/images/#limitations","title":"Limitations","text":"<ul> <li>Image generation capabilities may vary depending on the provider and model</li> <li>Some providers may have content filters that restrict certain types of content</li> <li>Image quality and adherence to the prompt varies across different models</li> <li>Costs for image generation can be higher than text generation</li> </ul>"},{"location":"usage/rate-limits/","title":"Rate Limits","text":"<p>IndoxRouter implements a three-tier rate limiting system to ensure fair usage and optimal performance. Understanding these limits helps you optimize your application's AI usage.</p>"},{"location":"usage/rate-limits/#rate-limit-tiers","title":"Rate Limit Tiers","text":"<p>IndoxRouter has three subscription tiers with different rate limits:</p> <pre><code># Rate limits by tier\nRATE_LIMITS = {\n    \"free\": {\n        \"requests_per_minute\": 10,\n        \"tokens_per_hour\": 10000\n    },\n    \"standard\": {\n        \"requests_per_minute\": 60,\n        \"tokens_per_hour\": 100000\n    },\n    \"enterprise\": {\n        \"requests_per_minute\": 500,\n        \"tokens_per_hour\": 1000000\n    }\n}\n</code></pre>"},{"location":"usage/rate-limits/#tier-comparison","title":"Tier Comparison","text":"Tier Requests/Minute Tokens/Hour Best For Free 10 10,000 Testing, prototyping, learning Standard 60 100,000 Production apps, small businesses Enterprise 500 1,000,000 High-volume applications, enterprises"},{"location":"usage/rate-limits/#what-counts-toward-limits","title":"What Counts Toward Limits","text":""},{"location":"usage/rate-limits/#request-limits","title":"Request Limits","text":"<p>Every API call counts as one request:</p> <ul> <li><code>client.chat()</code> = 1 request</li> <li><code>client.completions()</code> = 1 request</li> <li><code>client.embeddings()</code> = 1 request</li> <li><code>client.images()</code> = 1 request</li> <li><code>client.models()</code> = 1 request (but doesn't count toward token limits)</li> </ul>"},{"location":"usage/rate-limits/#token-limits","title":"Token Limits","text":"<p>Tokens are counted for text-based operations:</p> <ul> <li>Chat &amp; Completions: Input tokens + output tokens</li> <li>Embeddings: Input tokens only</li> <li>Images: No tokens counted (separate limits may apply)</li> <li>Models/Info calls: No tokens counted</li> </ul>"},{"location":"usage/rate-limits/#rate-limit-headers","title":"Rate Limit Headers","text":"<p>Every response includes rate limit information in the headers and response:</p> <pre><code>response = client.chat(\n    messages=[{\"role\": \"user\", \"content\": \"Hello\"}],\n    model=\"openai/gpt-4o-mini\"\n)\n\n# Rate limit info is included in response metadata\nprint(f\"Request ID: {response['request_id']}\")\nprint(f\"Duration: {response['duration_ms']}ms\")\n\n# Check your current usage\nusage = client.get_usage()\nprint(f\"Remaining credits: ${usage['remaining_credits']}\")\n</code></pre>"},{"location":"usage/rate-limits/#handling-rate-limits","title":"Handling Rate Limits","text":""},{"location":"usage/rate-limits/#rate-limit-errors","title":"Rate Limit Errors","text":"<p>When you exceed rate limits, you'll receive an error response:</p> <pre><code>{\n    'success': False,\n    'error': 'RateLimitError',\n    'message': 'Rate limit exceeded: 10 requests per minute',\n    'status_code': 429,\n    'request_id': 'req_rate_limit_123',\n    'details': {\n        'limit_type': 'requests_per_minute',\n        'limit': 10,\n        'reset_time': '2025-05-19T10:35:00Z',\n        'retry_after': 45  # seconds\n    }\n}\n</code></pre>"},{"location":"usage/rate-limits/#error-handling-example","title":"Error Handling Example","text":"<pre><code>from indoxrouter import Client, RateLimitError\nimport time\n\nclient = Client(api_key=\"your_api_key\")\n\ndef make_request_with_retry(messages, model, max_retries=3):\n    \"\"\"Make request with automatic retry on rate limit.\"\"\"\n\n    for attempt in range(max_retries):\n        try:\n            response = client.chat(messages=messages, model=model)\n\n            if response['success']:\n                return response\n            else:\n                # Handle other errors\n                print(f\"Request failed: {response['message']}\")\n                return response\n\n        except RateLimitError as e:\n            print(f\"Rate limit hit (attempt {attempt + 1}/{max_retries})\")\n\n            if attempt &lt; max_retries - 1:\n                # Extract retry delay from error details\n                retry_after = getattr(e, 'retry_after', 60)\n                print(f\"Waiting {retry_after} seconds before retry...\")\n                time.sleep(retry_after)\n            else:\n                print(\"Max retries exceeded\")\n                raise\n\n        except Exception as e:\n            print(f\"Request failed with error: {e}\")\n            raise\n\n    return None\n\n# Usage\nresponse = make_request_with_retry(\n    messages=[{\"role\": \"user\", \"content\": \"Hello\"}],\n    model=\"openai/gpt-4o-mini\"\n)\n</code></pre>"},{"location":"usage/rate-limits/#rate-limit-management","title":"Rate Limit Management","text":""},{"location":"usage/rate-limits/#request-batching","title":"Request Batching","text":"<p>For high-volume applications, batch your requests efficiently:</p> <pre><code>import time\nfrom datetime import datetime, timedelta\n\nclass RateLimitManager:\n    \"\"\"Manage requests within rate limits.\"\"\"\n\n    def __init__(self, client, requests_per_minute=60, tokens_per_hour=100000):\n        self.client = client\n        self.requests_per_minute = requests_per_minute\n        self.tokens_per_hour = tokens_per_hour\n\n        # Tracking\n        self.request_timestamps = []\n        self.token_usage_hourly = []\n\n    def can_make_request(self, estimated_tokens=100):\n        \"\"\"Check if we can make a request within limits.\"\"\"\n        now = datetime.now()\n\n        # Clean old request timestamps (older than 1 minute)\n        minute_ago = now - timedelta(minutes=1)\n        self.request_timestamps = [ts for ts in self.request_timestamps if ts &gt; minute_ago]\n\n        # Clean old token usage (older than 1 hour)\n        hour_ago = now - timedelta(hours=1)\n        self.token_usage_hourly = [(ts, tokens) for ts, tokens in self.token_usage_hourly if ts &gt; hour_ago]\n\n        # Check request limit\n        if len(self.request_timestamps) &gt;= self.requests_per_minute:\n            return False, \"Request rate limit would be exceeded\"\n\n        # Check token limit\n        current_hourly_tokens = sum(tokens for _, tokens in self.token_usage_hourly)\n        if current_hourly_tokens + estimated_tokens &gt; self.tokens_per_hour:\n            return False, \"Token rate limit would be exceeded\"\n\n        return True, \"OK\"\n\n    def make_request(self, request_func, estimated_tokens=100, **kwargs):\n        \"\"\"Make request with rate limit checking.\"\"\"\n        can_request, reason = self.can_make_request(estimated_tokens)\n\n        if not can_request:\n            # Calculate wait time\n            if \"request\" in reason.lower():\n                wait_time = 60 - (datetime.now() - min(self.request_timestamps)).total_seconds()\n            else:  # token limit\n                wait_time = 3600 - (datetime.now() - min(ts for ts, _ in self.token_usage_hourly)).total_seconds()\n\n            print(f\"Rate limit hit: {reason}\")\n            print(f\"Estimated wait time: {wait_time:.0f} seconds\")\n            return None\n\n        # Make the request\n        try:\n            response = request_func(**kwargs)\n\n            # Track the request\n            now = datetime.now()\n            self.request_timestamps.append(now)\n\n            # Track token usage if successful\n            if response.get('success') and 'usage' in response:\n                actual_tokens = response['usage']['tokens_total']\n                self.token_usage_hourly.append((now, actual_tokens))\n\n            return response\n\n        except Exception as e:\n            print(f\"Request failed: {e}\")\n            return None\n\n    def wait_for_rate_limit_reset(self):\n        \"\"\"Wait for rate limits to reset.\"\"\"\n        now = datetime.now()\n\n        # Find when we can make the next request\n        if self.request_timestamps:\n            next_request_time = min(self.request_timestamps) + timedelta(minutes=1)\n            if next_request_time &gt; now:\n                wait_seconds = (next_request_time - now).total_seconds()\n                print(f\"Waiting {wait_seconds:.0f} seconds for request limit reset...\")\n                time.sleep(wait_seconds)\n\n        # Find when we have token capacity\n        if self.token_usage_hourly:\n            next_token_time = min(ts for ts, _ in self.token_usage_hourly) + timedelta(hours=1)\n            if next_token_time &gt; now:\n                wait_seconds = (next_token_time - now).total_seconds()\n                print(f\"Waiting {wait_seconds:.0f} seconds for token limit reset...\")\n                time.sleep(wait_seconds)\n\n# Usage example\nrate_manager = RateLimitManager(client, requests_per_minute=60, tokens_per_hour=100000)\n\n# Make a managed request\nresponse = rate_manager.make_request(\n    client.chat,\n    estimated_tokens=150,\n    messages=[{\"role\": \"user\", \"content\": \"Hello\"}],\n    model=\"openai/gpt-4o-mini\"\n)\n\nif response:\n    print(f\"Response: {response['data']}\")\n    print(f\"Actual tokens used: {response['usage']['tokens_total']}\")\n</code></pre>"},{"location":"usage/rate-limits/#batch-processing","title":"Batch Processing","text":"<p>For processing multiple items efficiently:</p> <pre><code>def process_batch_with_rate_limits(client, items, batch_size=10):\n    \"\"\"Process items in batches respecting rate limits.\"\"\"\n\n    rate_manager = RateLimitManager(client)\n    results = []\n\n    for i in range(0, len(items), batch_size):\n        batch = items[i:i + batch_size]\n        batch_results = []\n\n        print(f\"Processing batch {i//batch_size + 1}/{(len(items)-1)//batch_size + 1}\")\n\n        for item in batch:\n            # Check if we can make request\n            can_request, reason = rate_manager.can_make_request(estimated_tokens=100)\n\n            if not can_request:\n                print(f\"Rate limit hit, waiting...\")\n                rate_manager.wait_for_rate_limit_reset()\n\n            # Make the request\n            response = rate_manager.make_request(\n                client.chat,\n                estimated_tokens=100,\n                messages=[{\"role\": \"user\", \"content\": f\"Process this item: {item}\"}],\n                model=\"openai/gpt-4o-mini\"\n            )\n\n            if response and response['success']:\n                batch_results.append({\n                    'item': item,\n                    'result': response['data'],\n                    'cost': response['usage']['cost'],\n                    'tokens': response['usage']['tokens_total']\n                })\n            else:\n                batch_results.append({\n                    'item': item,\n                    'result': None,\n                    'error': response.get('message', 'Unknown error') if response else 'Request failed'\n                })\n\n        results.extend(batch_results)\n\n        # Brief pause between batches\n        time.sleep(1)\n\n    return results\n\n# Process a list of items\nitems_to_process = [\n    \"Translate this to French: Hello world\",\n    \"Summarize: The quick brown fox...\",\n    \"Generate a haiku about rain\",\n    # ... more items\n]\n\nresults = process_batch_with_rate_limits(client, items_to_process)\n\n# Analyze results\nsuccessful = [r for r in results if r['result'] is not None]\nfailed = [r for r in results if r['result'] is None]\n\nprint(f\"Successfully processed: {len(successful)}/{len(results)}\")\nprint(f\"Total cost: ${sum(r.get('cost', 0) for r in successful):.4f}\")\nprint(f\"Total tokens: {sum(r.get('tokens', 0) for r in successful):,}\")\n</code></pre>"},{"location":"usage/rate-limits/#optimization-strategies","title":"Optimization Strategies","text":""},{"location":"usage/rate-limits/#1-token-efficient-prompts","title":"1. Token-Efficient Prompts","text":"<p>Reduce token usage to stay within hourly limits:</p> <pre><code>def optimize_prompt_tokens(original_prompt, target_tokens=100):\n    \"\"\"Optimize prompts to use fewer tokens.\"\"\"\n\n    # Estimate tokens (rough approximation: 1 token \u2248 4 characters)\n    estimated_tokens = len(original_prompt) // 4\n\n    if estimated_tokens &lt;= target_tokens:\n        return original_prompt\n\n    # Optimization strategies\n    optimized_prompt = original_prompt\n\n    # Remove extra whitespace\n    optimized_prompt = ' '.join(optimized_prompt.split())\n\n    # Use abbreviations\n    replacements = {\n        'please': 'pls',\n        'because': 'bc',\n        'without': 'w/o',\n        'with': 'w/',\n        'and': '&amp;',\n        'you are': \"you're\",\n        'do not': \"don't\",\n        'cannot': \"can't\"\n    }\n\n    for old, new in replacements.items():\n        optimized_prompt = optimized_prompt.replace(old, new)\n\n    # If still too long, truncate with ellipsis\n    target_chars = target_tokens * 4\n    if len(optimized_prompt) &gt; target_chars:\n        optimized_prompt = optimized_prompt[:target_chars-3] + \"...\"\n\n    return optimized_prompt\n\n# Example usage\nlong_prompt = \"\"\"\nPlease analyze this text very carefully and provide a comprehensive summary\nthat includes all the key points, main arguments, supporting evidence, and\nconclusions. Make sure to maintain the original tone and style.\n\"\"\"\n\noptimized = optimize_prompt_tokens(long_prompt, target_tokens=50)\nprint(f\"Original: {len(long_prompt)} chars\")\nprint(f\"Optimized: {len(optimized)} chars\")\nprint(f\"Optimized prompt: {optimized}\")\n</code></pre>"},{"location":"usage/rate-limits/#2-smart-model-selection","title":"2. Smart Model Selection","text":"<p>Choose models based on rate limits and requirements:</p> <pre><code>def select_optimal_model(task_complexity, urgency=\"normal\", tier=\"standard\"):\n    \"\"\"Select the best model based on constraints.\"\"\"\n\n    # Model efficiency ratings (cost per token)\n    model_efficiency = {\n        \"openai/gpt-3.5-turbo\": {\"cost_per_token\": 0.000001, \"quality\": 7},\n        \"openai/gpt-4o-mini\": {\"cost_per_token\": 0.000002, \"quality\": 8},\n        \"openai/gpt-4o\": {\"cost_per_token\": 0.000030, \"quality\": 10},\n        \"anthropic/claude-3-haiku-20240307\": {\"cost_per_token\": 0.000002, \"quality\": 8},\n        \"anthropic/claude-3-sonnet-20240229\": {\"cost_per_token\": 0.000008, \"quality\": 9},\n        \"anthropic/claude-3-opus-20240229\": {\"cost_per_token\": 0.000020, \"quality\": 10},\n        \"deepseek/deepseek-chat\": {\"cost_per_token\": 0.0000005, \"quality\": 7}\n    }\n\n    # Filter models based on tier token limits\n    tier_limits = {\n        \"free\": 10000,      # 10K tokens/hour\n        \"standard\": 100000,  # 100K tokens/hour\n        \"enterprise\": 1000000 # 1M tokens/hour\n    }\n\n    hourly_limit = tier_limits.get(tier, 100000)\n\n    if task_complexity == \"simple\" and hourly_limit &lt; 50000:\n        # Use most efficient models for high-volume, low-complexity tasks\n        recommended = [\"deepseek/deepseek-chat\", \"openai/gpt-3.5-turbo\"]\n    elif task_complexity == \"moderate\":\n        recommended = [\"openai/gpt-4o-mini\", \"anthropic/claude-3-haiku-20240307\"]\n    else:  # complex\n        if urgency == \"high\" or hourly_limit &gt; 500000:\n            recommended = [\"openai/gpt-4o\", \"anthropic/claude-3-opus-20240229\"]\n        else:\n            recommended = [\"anthropic/claude-3-sonnet-20240229\", \"openai/gpt-4o-mini\"]\n\n    # Return best option with reasoning\n    best_model = recommended[0]\n    model_info = model_efficiency[best_model]\n\n    return {\n        'model': best_model,\n        'reasoning': f\"Selected for {task_complexity} task with {tier} tier limits\",\n        'cost_per_token': model_info['cost_per_token'],\n        'quality_rating': model_info['quality'],\n        'alternatives': recommended[1:] if len(recommended) &gt; 1 else []\n    }\n\n# Example usage\nselection = select_optimal_model(\"simple\", \"normal\", \"standard\")\nprint(f\"Recommended model: {selection['model']}\")\nprint(f\"Reasoning: {selection['reasoning']}\")\nprint(f\"Cost per token: ${selection['cost_per_token']:.7f}\")\n</code></pre>"},{"location":"usage/rate-limits/#3-request-scheduling","title":"3. Request Scheduling","text":"<p>Distribute requests to avoid rate limit peaks:</p> <pre><code>import time\nfrom datetime import datetime, timedelta\nimport random\n\nclass RequestScheduler:\n    \"\"\"Schedule requests to optimize rate limit usage.\"\"\"\n\n    def __init__(self, client, requests_per_minute=60):\n        self.client = client\n        self.requests_per_minute = requests_per_minute\n        self.request_queue = []\n\n    def add_request(self, request_func, priority=1, **kwargs):\n        \"\"\"Add request to queue with priority.\"\"\"\n        self.request_queue.append({\n            'func': request_func,\n            'kwargs': kwargs,\n            'priority': priority,\n            'added_at': datetime.now()\n        })\n\n        # Sort by priority (higher number = higher priority)\n        self.request_queue.sort(key=lambda x: x['priority'], reverse=True)\n\n    def process_queue(self, max_concurrent=5):\n        \"\"\"Process queued requests respecting rate limits.\"\"\"\n\n        # Calculate optimal delay between requests\n        min_delay = 60 / self.requests_per_minute  # seconds between requests\n\n        results = []\n        processed = 0\n\n        while self.request_queue and processed &lt; max_concurrent:\n            request = self.request_queue.pop(0)\n\n            try:\n                # Add small random delay to avoid synchronized requests\n                jitter = random.uniform(0, min_delay * 0.1)\n                time.sleep(min_delay + jitter)\n\n                print(f\"Processing request {processed + 1}/{min(len(self.request_queue) + 1, max_concurrent)}\")\n\n                response = request['func'](**request['kwargs'])\n                results.append({\n                    'request': request,\n                    'response': response,\n                    'processed_at': datetime.now()\n                })\n\n                processed += 1\n\n            except Exception as e:\n                print(f\"Request failed: {e}\")\n                results.append({\n                    'request': request,\n                    'response': None,\n                    'error': str(e),\n                    'processed_at': datetime.now()\n                })\n\n        return results\n\n# Usage example\nscheduler = RequestScheduler(client, requests_per_minute=60)\n\n# Add requests with different priorities\nscheduler.add_request(\n    client.chat,\n    priority=3,  # High priority\n    messages=[{\"role\": \"user\", \"content\": \"Urgent: Translate 'Hello' to French\"}],\n    model=\"openai/gpt-4o-mini\"\n)\n\nscheduler.add_request(\n    client.chat,\n    priority=1,  # Low priority\n    messages=[{\"role\": \"user\", \"content\": \"Generate a fun fact about cats\"}],\n    model=\"openai/gpt-3.5-turbo\"\n)\n\nscheduler.add_request(\n    client.chat,\n    priority=2,  # Medium priority\n    messages=[{\"role\": \"user\", \"content\": \"Summarize the latest news\"}],\n    model=\"openai/gpt-4o-mini\"\n)\n\n# Process the queue\nresults = scheduler.process_queue(max_concurrent=3)\n\nprint(f\"Processed {len(results)} requests\")\nfor i, result in enumerate(results, 1):\n    if result['response'] and result['response']['success']:\n        print(f\"{i}. Success: {result['response']['data'][:50]}...\")\n    else:\n        print(f\"{i}. Failed: {result.get('error', 'Unknown error')}\")\n</code></pre>"},{"location":"usage/rate-limits/#monitoring-rate-limits","title":"Monitoring Rate Limits","text":""},{"location":"usage/rate-limits/#real-time-rate-limit-tracking","title":"Real-time Rate Limit Tracking","text":"<pre><code>class RateLimitMonitor:\n    \"\"\"Monitor rate limit usage in real-time.\"\"\"\n\n    def __init__(self, requests_per_minute=60, tokens_per_hour=100000):\n        self.requests_per_minute = requests_per_minute\n        self.tokens_per_hour = tokens_per_hour\n        self.request_timestamps = []\n        self.token_usage = []\n\n    def log_request(self, response):\n        \"\"\"Log a request and its token usage.\"\"\"\n        now = datetime.now()\n        self.request_timestamps.append(now)\n\n        if response and response.get('success') and 'usage' in response:\n            tokens = response['usage']['tokens_total']\n            self.token_usage.append((now, tokens))\n\n    def get_current_usage(self):\n        \"\"\"Get current rate limit usage.\"\"\"\n        now = datetime.now()\n\n        # Clean old data\n        minute_ago = now - timedelta(minutes=1)\n        hour_ago = now - timedelta(hours=1)\n\n        self.request_timestamps = [ts for ts in self.request_timestamps if ts &gt; minute_ago]\n        self.token_usage = [(ts, tokens) for ts, tokens in self.token_usage if ts &gt; hour_ago]\n\n        # Calculate current usage\n        current_requests = len(self.request_timestamps)\n        current_tokens = sum(tokens for _, tokens in self.token_usage)\n\n        return {\n            'requests': {\n                'current': current_requests,\n                'limit': self.requests_per_minute,\n                'percentage': (current_requests / self.requests_per_minute * 100) if self.requests_per_minute &gt; 0 else 0,\n                'remaining': max(0, self.requests_per_minute - current_requests)\n            },\n            'tokens': {\n                'current': current_tokens,\n                'limit': self.tokens_per_hour,\n                'percentage': (current_tokens / self.tokens_per_hour * 100) if self.tokens_per_hour &gt; 0 else 0,\n                'remaining': max(0, self.tokens_per_hour - current_tokens)\n            }\n        }\n\n    def print_status(self):\n        \"\"\"Print current rate limit status.\"\"\"\n        usage = self.get_current_usage()\n\n        print(f\"\ud83d\udcca Rate Limit Status\")\n        print(f\"   Requests: {usage['requests']['current']}/{usage['requests']['limit']} ({usage['requests']['percentage']:.1f}%)\")\n        print(f\"   Tokens: {usage['tokens']['current']:,}/{usage['tokens']['limit']:,} ({usage['tokens']['percentage']:.1f}%)\")\n\n        if usage['requests']['percentage'] &gt; 80:\n            print(f\"   \u26a0\ufe0f  High request usage\")\n        if usage['tokens']['percentage'] &gt; 80:\n            print(f\"   \u26a0\ufe0f  High token usage\")\n\n# Usage\nmonitor = RateLimitMonitor(requests_per_minute=60, tokens_per_hour=100000)\n\n# Make some requests\nfor i in range(5):\n    response = client.chat(\n        messages=[{\"role\": \"user\", \"content\": f\"Request {i+1}\"}],\n        model=\"openai/gpt-4o-mini\"\n    )\n    monitor.log_request(response)\n\n    # Show status every few requests\n    if (i + 1) % 2 == 0:\n        monitor.print_status()\n        print()\n</code></pre>"},{"location":"usage/rate-limits/#best-practices","title":"Best Practices","text":""},{"location":"usage/rate-limits/#1-respect-rate-limits","title":"1. Respect Rate Limits","text":"<pre><code># Always implement retry logic with exponential backoff\ndef make_request_with_backoff(client, request_func, max_retries=3, **kwargs):\n    \"\"\"Make request with exponential backoff on rate limits.\"\"\"\n\n    for attempt in range(max_retries):\n        try:\n            response = request_func(**kwargs)\n            return response\n\n        except RateLimitError as e:\n            if attempt &lt; max_retries - 1:\n                wait_time = (2 ** attempt) * 60  # Exponential backoff: 1min, 2min, 4min\n                print(f\"Rate limited, waiting {wait_time} seconds...\")\n                time.sleep(wait_time)\n            else:\n                raise\n\n    return None\n</code></pre>"},{"location":"usage/rate-limits/#2-optimize-token-usage","title":"2. Optimize Token Usage","text":"<pre><code># Use efficient prompting techniques\ndef create_efficient_prompt(task, context=\"\", max_tokens=100):\n    \"\"\"Create token-efficient prompts.\"\"\"\n\n    # Use concise language\n    efficient_prompt = f\"Task: {task}\"\n\n    if context:\n        # Limit context to essential information\n        context_limit = max_tokens // 2\n        if len(context) &gt; context_limit * 4:  # Rough token estimation\n            context = context[:context_limit * 4] + \"...\"\n        efficient_prompt += f\"\\nContext: {context}\"\n\n    efficient_prompt += \"\\nResponse:\"\n\n    return efficient_prompt\n\n# Example\ntask = \"Summarize the main points\"\ncontext = \"Long document text here...\"\nprompt = create_efficient_prompt(task, context, max_tokens=150)\n</code></pre>"},{"location":"usage/rate-limits/#3-monitor-and-alert","title":"3. Monitor and Alert","text":"<pre><code># Set up monitoring for your application\ndef setup_rate_limit_alerts(client, alert_threshold=0.8):\n    \"\"\"Set up alerts for rate limit usage.\"\"\"\n\n    monitor = RateLimitMonitor()\n\n    def check_and_alert():\n        usage = monitor.get_current_usage()\n\n        if usage['requests']['percentage'] &gt; alert_threshold * 100:\n            print(f\"\ud83d\udea8 Request rate limit alert: {usage['requests']['percentage']:.1f}% used\")\n\n        if usage['tokens']['percentage'] &gt; alert_threshold * 100:\n            print(f\"\ud83d\udea8 Token rate limit alert: {usage['tokens']['percentage']:.1f}% used\")\n\n    return monitor, check_and_alert\n\n# Use monitoring\nmonitor, alert_check = setup_rate_limit_alerts(client)\n\n# In your application loop\nresponse = client.chat(messages=[...], model=\"openai/gpt-4o-mini\")\nmonitor.log_request(response)\nalert_check()\n</code></pre>"},{"location":"usage/responses/","title":"Response Format","text":"<p>IndoxRouter provides detailed response information for every API call, including usage statistics, costs, and performance metrics. This helps you monitor and optimize your AI application usage.</p>"},{"location":"usage/responses/#standard-response-structure","title":"Standard Response Structure","text":"<p>Every IndoxRouter response follows this consistent format:</p> <pre><code>{\n    'request_id': 'c08cc108-6b0d-48bd-a660-546143f1b9fa',\n    'created_at': '2025-05-19T06:07:38.077269',\n    'duration_ms': 9664.651870727539,\n    'provider': 'deepseek',\n    'model': 'deepseek-chat',\n    'success': True,\n    'message': '',\n    'usage': {\n        'tokens_prompt': 15,\n        'tokens_completion': 107,\n        'tokens_total': 122,\n        'cost': 0.000229,\n        'latency': 9.487398862838745,\n        'timestamp': '2025-05-19T06:07:38.065330',\n        'cache_read_tokens': 0,\n        'cache_write_tokens': 0,\n        'reasoning_tokens': 0,\n        'web_search_count': 0,\n        'request_count': 1,\n        'cost_breakdown': {\n            'input_tokens': 0.000025,\n            'output_tokens': 0.000204,\n            'cache_read': 0.0,\n            'cache_write': 0.0,\n            'reasoning': 0.0,\n            'web_search': 0.0,\n            'request': 0.0\n        }\n    },\n    'raw_response': None,\n    'data': 'Your AI response content here...',\n    'finish_reason': None,\n    'images': None\n}\n</code></pre>"},{"location":"usage/responses/#response-fields","title":"Response Fields","text":""},{"location":"usage/responses/#metadata-fields","title":"Metadata Fields","text":"Field Type Description <code>request_id</code> string Unique identifier for this request <code>created_at</code> string ISO timestamp when request was processed <code>duration_ms</code> float Total request duration in milliseconds <code>provider</code> string AI provider used (openai, anthropic, etc.) <code>model</code> string Specific model used <code>success</code> boolean Whether the request succeeded <code>message</code> string Success message or additional info"},{"location":"usage/responses/#usage-statistics","title":"Usage Statistics","text":"<p>The <code>usage</code> object contains detailed usage and cost information:</p> Field Type Description <code>tokens_prompt</code> integer Tokens used in the input/prompt <code>tokens_completion</code> integer Tokens generated in the response <code>tokens_total</code> integer Total tokens used (prompt + completion) <code>cost</code> float Total cost in USD for this request <code>latency</code> float Provider response time in seconds <code>timestamp</code> string ISO timestamp of the request <code>cache_read_tokens</code> integer Tokens read from cache <code>cache_write_tokens</code> integer Tokens written to cache <code>reasoning_tokens</code> integer Tokens used for internal reasoning <code>web_search_count</code> integer Number of web searches performed <code>request_count</code> integer Number of requests made (usually 1) <code>cost_breakdown</code> object Detailed cost breakdown by component"},{"location":"usage/responses/#cost-breakdown","title":"Cost Breakdown","text":"<p>The <code>cost_breakdown</code> object provides detailed cost information:</p> Field Type Description <code>input_tokens</code> float Cost for input/prompt tokens <code>output_tokens</code> float Cost for output/completion tokens <code>cache_read</code> float Cost for cache read operations <code>cache_write</code> float Cost for cache write operations <code>reasoning</code> float Cost for reasoning tokens <code>web_search</code> float Cost for web search operations <code>request</code> float Base request cost"},{"location":"usage/responses/#content-fields","title":"Content Fields","text":"Field Type Description <code>data</code> string/array The actual AI response content <code>finish_reason</code> string Why the response ended (stop, length, etc.) <code>images</code> array/null Generated images with URLs (null if none) <code>raw_response</code> object Original provider response (optional)"},{"location":"usage/responses/#response-examples-by-operation","title":"Response Examples by Operation","text":""},{"location":"usage/responses/#chat-completion-response","title":"Chat Completion Response","text":"<pre><code>response = client.chat(\n    messages=[{\"role\": \"user\", \"content\": \"What is the capital of France?\"}],\n    model=\"openai/gpt-4o-mini\"\n)\n\n# Response structure:\n{\n    'request_id': 'b881942c-e21d-4f9d-ad82-47344945c642',\n    'created_at': '2025-06-15T09:53:26.130868',\n    'duration_ms': 1737.612247467041,\n    'provider': 'openai',\n    'model': 'gpt-4o-mini',\n    'success': True,\n    'message': '',\n    'usage': {\n        'tokens_prompt': 24,\n        'tokens_completion': 7,\n        'tokens_total': 31,\n        'cost': 7.8e-06,\n        'latency': 1.629077672958374,\n        'timestamp': '2025-06-15T09:53:26.114626',\n        'cache_read_tokens': 0,\n        'cache_write_tokens': 0,\n        'reasoning_tokens': 0,\n        'web_search_count': 0,\n        'request_count': 1,\n        'cost_breakdown': {\n            'input_tokens': 3.6e-06,\n            'output_tokens': 4.2e-06,\n            'cache_read': 0.0,\n            'cache_write': 0.0,\n            'reasoning': 0.0,\n            'web_search': 0.0,\n            'request': 0.0\n        }\n    },\n    'raw_response': None,\n    'data': 'The capital of France is Paris.',\n    'finish_reason': None\n}\n</code></pre>"},{"location":"usage/responses/#text-completion-response","title":"Text Completion Response","text":"<pre><code>response = client.completions(\n    prompt=\"Tell me a story\",\n    model=\"openai/gpt-4o-mini\",\n    max_tokens=500\n)\n\n# Response structure:\n{\n    'request_id': '0fecd9af-0ba8-47a4-852f-029b3a5bfa18',\n    'created_at': '2025-06-15T09:54:51.393591',\n    'duration_ms': 6939.460754394531,\n    'provider': 'openai',\n    'model': 'gpt-4o-mini',\n    'success': True,\n    'message': '',\n    'usage': {\n        'tokens_prompt': 11,\n        'tokens_completion': 530,\n        'tokens_total': 541,\n        'cost': 0.00031965,\n        'latency': 6.794795513153076,\n        'timestamp': '2025-06-15T09:54:51.362423',\n        'cache_read_tokens': 0,\n        'cache_write_tokens': 0,\n        'reasoning_tokens': 0,\n        'web_search_count': 0,\n        'request_count': 1,\n        'cost_breakdown': {\n            'input_tokens': 1.6499999999999999e-06,\n            'output_tokens': 0.000318,\n            'cache_read': 0.0,\n            'cache_write': 0.0,\n            'reasoning': 0.0,\n            'web_search': 0.0,\n            'request': 0.0\n        }\n    },\n    'raw_response': None,\n    'data': 'Once upon a time, in a small village nestled between rolling hills and a sparkling river, there lived a young girl named Elara. She was known throughout the village for her kindness and her love for nature...',\n    'finish_reason': None,\n    'images': None\n}\n</code></pre>"},{"location":"usage/responses/#text-completion-response-with-images","title":"Text Completion Response with Images","text":"<p>For models that support image generation (like <code>gemini-2.5-flash-image</code>), the response may include generated images:</p> <pre><code>response = client.completions(\n    prompt=\"Describe a cat and draw a picture of it\",\n    model=\"google/gemini-2.5-flash-image\"\n)\n\n# Response structure:\n{\n    'request_id': '48c93623-286e-4e03-807b-938e53cb5076',\n    'created_at': '2025-10-26T16:48:59.574195',\n    'duration_ms': 10853.046178817749,\n    'provider': 'google',\n    'model': 'gemini-2.5-flash-image',\n    'success': True,\n    'message': '',\n    'usage': {\n        'tokens_prompt': 8,\n        'tokens_completion': 1377,\n        'tokens_total': 1385,\n        'cost': 0.0034449000000000003,\n        'latency': 9.228402614593506,\n        'timestamp': '2025-10-26T16:48:58.009249',\n        'cache_read_tokens': 0,\n        'cache_write_tokens': 0,\n        'reasoning_tokens': 0,\n        'web_search_count': 0,\n        'request_count': 1,\n        'cost_breakdown': {\n            'input_tokens': 2.4e-06,\n            'output_tokens': 0.0034425000000000002,\n            'cache_read': 0.0,\n            'cache_write': 0.0,\n            'reasoning': 0.0,\n            'web_search': 0.0,\n            'request': 0.0\n        }\n    },\n    'raw_response': None,\n    'data': 'A cat is a small, domesticated carnivorous mammal... Here\\'s a drawing of a cat for you:',\n    'finish_reason': 'STOP',\n    'images': [\n        {\n            'url': 'https://indoxrouter.s3.amazonaws.com/dev_user_4/image/d0847065-2f2b-4529-8484-0e98e19b7318_20251026_164858.png?...',\n            'index': 0\n        }\n    ]\n}\n</code></pre>"},{"location":"usage/responses/#embedding-response","title":"Embedding Response","text":"<pre><code>response = client.embeddings(\n    text=\"Hello world\",\n    model=\"openai/text-embedding-3-small\"\n)\n\n# Response structure:\n{\n    'request_id': 'req_ghi789',\n    'created_at': '2025-05-19T10:40:15.456789',\n    'duration_ms': 456.78,\n    'provider': 'openai',\n    'model': 'text-embedding-3-small',\n    'success': True,\n    'message': '',\n    'usage': {\n        'tokens_prompt': 2,\n        'tokens_completion': 0,\n        'tokens_total': 2,\n        'cost': 0.000001,\n        'latency': 0.3,\n        'timestamp': '2025-05-19T10:40:15.400000'\n    },\n    'data': [\n        [0.123, -0.456, 0.789, ...]  # 1536-dimensional vector\n    ],\n    'dimensions': 1536\n}\n</code></pre>"},{"location":"usage/responses/#image-generation-response","title":"Image Generation Response","text":"<pre><code>response = client.images(\n    prompt=\"A beautiful sunset over the ocean\",\n    model=\"openai/dall-e-3\",\n    size=\"1024x1024\"\n)\n\n# Response structure:\n{\n    'request_id': '0bc89954-f5cc-4efc-a055-4e5624aa2a81',\n    'created_at': '2025-05-29T11:39:24.621706',\n    'duration_ms': 12340.412378311157,\n    'provider': 'openai',\n    'model': 'dall-e-3',\n    'success': True,\n    'message': '',\n    'usage': {\n        'tokens_prompt': 0,\n        'tokens_completion': 0,\n        'tokens_total': 0,\n        'cost': 0.016,\n        'latency': 12.240789651870728,\n        'timestamp': '2025-05-29T11:39:24.612377',\n        'cache_read_tokens': 0,\n        'cache_write_tokens': 0,\n        'reasoning_tokens': 0,\n        'web_search_count': 0,\n        'request_count': 1,\n        'cost_breakdown': None\n    },\n    'raw_response': None,\n    'data': [\n        {\n            'url': 'https://....image_12345.jpg',\n            'revised_prompt': 'A beautiful sunset over the ocean with golden clouds...'\n        }\n    ]\n}\n</code></pre>"},{"location":"usage/responses/#working-with-responses","title":"Working with Responses","text":""},{"location":"usage/responses/#accessing-response-data","title":"Accessing Response Data","text":"<pre><code>response = client.chat(\n    messages=[{\"role\": \"user\", \"content\": \"Hello\"}],\n    model=\"openai/gpt-4o-mini\"\n)\n\n# Get the AI response text\ncontent = response['data']\nprint(content)\n\n# Get usage information\nusage = response['usage']\nprint(f\"Tokens used: {usage['tokens_total']}\")\nprint(f\"Cost: ${usage['cost']:.6f}\")\nprint(f\"Latency: {usage['latency']:.2f}s\")\n\n# Get detailed cost breakdown\nif usage['cost_breakdown']:\n    breakdown = usage['cost_breakdown']\n    print(f\"Input token cost: ${breakdown['input_tokens']:.6f}\")\n    print(f\"Output token cost: ${breakdown['output_tokens']:.6f}\")\n    print(f\"Cache read cost: ${breakdown['cache_read']:.6f}\")\n\n# Get metadata\nprint(f\"Provider: {response['provider']}\")\nprint(f\"Model: {response['model']}\")\nprint(f\"Request ID: {response['request_id']}\")\n</code></pre>"},{"location":"usage/responses/#handling-image-responses","title":"Handling Image Responses","text":"<pre><code>response = client.images(\n    prompt=\"A beautiful sunset\",\n    model=\"openai/dall-e-3\",\n    size=\"1024x1024\"\n)\n\n# Get image URL\nif response['data']:\n    image_url = response['data'][0]['url']\n    print(f\"Image URL: {image_url}\")\n\n    # Download and display the image\n    import requests\n    from PIL import Image\n    from io import BytesIO\n\n    img_response = requests.get(image_url)\n    img = Image.open(BytesIO(img_response.content))\n    img.show()  # Or save: img.save(\"generated_image.png\")\n\n    # Optional: Get revised prompt if available\n    if 'revised_prompt' in response['data'][0]:\n        print(f\"Revised prompt: {response['data'][0]['revised_prompt']}\")\n</code></pre>"},{"location":"usage/responses/#handling-images-in-text-completions","title":"Handling Images in Text Completions","text":"<p>For text completion models that can generate images (like Gemini models), check for the <code>images</code> field:</p> <pre><code>response = client.completions(\n    prompt=\"Describe a sunset and create an image\",\n    model=\"google/gemini-2.5-flash-image\"\n)\n\n# Check if images were generated\nif response['images'] and len(response['images']) &gt; 0:\n    print(f\"Generated {len(response['images'])} image(s)\")\n\n    for image in response['images']:\n        image_url = image['url']\n        image_index = image['index']\n        print(f\"Image {image_index}: {image_url}\")\n\n        # Download and save the image\n        import requests\n\n        img_response = requests.get(image_url)\n        if img_response.status_code == 200:\n            filename = f\"generated_image_{image_index}.png\"\n            with open(filename, 'wb') as f:\n                f.write(img_response.content)\n            print(f\"Saved image to {filename}\")\nelse:\n    print(\"No images were generated in this response\")\n\n# The text content is still available in response['data']\nprint(f\"Text response: {response['data']}\")\n</code></pre>"},{"location":"usage/responses/#cost-tracking","title":"Cost Tracking","text":"<pre><code>def track_costs(response):\n    \"\"\"Extract and log cost information from response.\"\"\"\n    usage = response['usage']\n\n    print(f\"Request Cost Breakdown:\")\n    print(f\"  Model: {response['provider']}/{response['model']}\")\n    print(f\"  Prompt tokens: {usage['tokens_prompt']}\")\n    print(f\"  Completion tokens: {usage['tokens_completion']}\")\n    print(f\"  Total tokens: {usage['tokens_total']}\")\n    print(f\"  Cost: ${usage['cost']:.6f}\")\n    print(f\"  Latency: {usage['latency']:.2f}s\")\n\n    return usage['cost']\n\n# Use with any request\nresponse = client.chat(messages=[...], model=\"openai/gpt-4o\")\ncost = track_costs(response)\n</code></pre>"},{"location":"usage/responses/#performance-monitoring","title":"Performance Monitoring","text":"<pre><code>def analyze_performance(response):\n    \"\"\"Analyze request performance metrics.\"\"\"\n    duration = response['duration_ms']\n    latency = response['usage']['latency'] * 1000  # Convert to ms\n\n    # Network + processing overhead\n    overhead = duration - latency\n\n    print(f\"Performance Analysis:\")\n    print(f\"  Total duration: {duration:.2f}ms\")\n    print(f\"  Provider latency: {latency:.2f}ms\")\n    print(f\"  Network overhead: {overhead:.2f}ms\")\n\n    if overhead &gt; 1000:  # &gt; 1 second overhead\n        print(\"  \u26a0\ufe0f  High network overhead detected\")\n\n    return {\n        'total_duration': duration,\n        'provider_latency': latency,\n        'overhead': overhead\n    }\n</code></pre>"},{"location":"usage/responses/#error-responses","title":"Error Responses","text":"<p>When an error occurs, the response format changes:</p> <pre><code>{\n    'success': False,\n    'error': 'ModelNotFoundError',\n    'message': 'Model \"gpt-5\" not found for provider \"openai\"',\n    'status_code': 404,\n    'request_id': 'req_error123',\n    'details': {\n        'provider': 'openai',\n        'requested_model': 'gpt-5',\n        'available_models': ['gpt-4o', 'gpt-4o-mini', 'gpt-3.5-turbo']\n    }\n}\n</code></pre>"},{"location":"usage/responses/#handling-error-responses","title":"Handling Error Responses","text":"<pre><code>try:\n    response = client.chat(\n        messages=[{\"role\": \"user\", \"content\": \"Hello\"}],\n        model=\"invalid/model\"\n    )\n\n    if response['success']:\n        print(response['data'])\n    else:\n        print(f\"Error: {response['error']}\")\n        print(f\"Message: {response['message']}\")\n\n        # Get suggested alternatives\n        if 'details' in response and 'available_models' in response['details']:\n            print(\"Available models:\", response['details']['available_models'])\n\nexcept Exception as e:\n    print(f\"Request failed: {e}\")\n</code></pre>"},{"location":"usage/responses/#streaming-responses","title":"Streaming Responses","text":"<p>For streaming requests, responses come in chunks:</p> <pre><code>response_stream = client.chat(\n    messages=[{\"role\": \"user\", \"content\": \"Tell me a story\"}],\n    model=\"openai/gpt-4o-mini\",\n    stream=True\n)\n\nfull_response = \"\"\ntotal_cost = 0\n\nfor chunk in response_stream:\n    if chunk['success']:\n        # Accumulate the response\n        full_response += chunk['data']\n\n        # Track costs (final chunk has complete usage info)\n        if 'usage' in chunk:\n            total_cost = chunk['usage']['cost']\n\n        print(chunk['data'], end='', flush=True)\n\nprint(f\"\\n\\nTotal cost: ${total_cost:.6f}\")\n</code></pre>"},{"location":"usage/responses/#response-validation","title":"Response Validation","text":"<pre><code>def validate_response(response):\n    \"\"\"Validate IndoxRouter response format.\"\"\"\n    required_fields = ['request_id', 'success', 'provider', 'model']\n\n    for field in required_fields:\n        if field not in response:\n            raise ValueError(f\"Missing required field: {field}\")\n\n    if response['success']:\n        if 'data' not in response:\n            raise ValueError(\"Success response missing 'data' field\")\n        if 'usage' not in response:\n            raise ValueError(\"Success response missing 'usage' field\")\n    else:\n        if 'error' not in response:\n            raise ValueError(\"Error response missing 'error' field\")\n\n    return True\n\n# Use with responses\nresponse = client.chat(messages=[...], model=\"openai/gpt-4o-mini\")\nif validate_response(response):\n    print(\"Response format is valid\")\n</code></pre>"},{"location":"usage/responses/#best-practices","title":"Best Practices","text":""},{"location":"usage/responses/#1-always-check-success-status","title":"1. Always Check Success Status","text":"<pre><code>response = client.chat(messages=[...], model=\"openai/gpt-4o-mini\")\n\nif response['success']:\n    content = response['data']\n    cost = response['usage']['cost']\n    # Process successful response\nelse:\n    error = response['error']\n    message = response['message']\n    # Handle error\n</code></pre>"},{"location":"usage/responses/#2-monitor-costs","title":"2. Monitor Costs","text":"<pre><code># Set up cost alerts\ndef check_cost_threshold(response, max_cost=0.01):\n    \"\"\"Alert if single request exceeds cost threshold.\"\"\"\n    cost = response['usage']['cost']\n    if cost &gt; max_cost:\n        print(f\"\u26a0\ufe0f  High cost request: ${cost:.4f}\")\n        print(f\"   Model: {response['provider']}/{response['model']}\")\n        print(f\"   Tokens: {response['usage']['tokens_total']}\")\n\n    return cost\n\nresponse = client.chat(messages=[...], model=\"openai/gpt-4o\")\ncheck_cost_threshold(response)\n</code></pre>"},{"location":"usage/responses/#3-track-performance","title":"3. Track Performance","text":"<pre><code># Performance monitoring\ndef log_performance(response):\n    \"\"\"Log performance metrics for monitoring.\"\"\"\n    metrics = {\n        'request_id': response['request_id'],\n        'model': f\"{response['provider']}/{response['model']}\",\n        'duration_ms': response['duration_ms'],\n        'latency_ms': response['usage']['latency'] * 1000,\n        'tokens': response['usage']['tokens_total'],\n        'cost': response['usage']['cost']\n    }\n\n    # Log to your monitoring system\n    print(f\"METRICS: {metrics}\")\n\n    return metrics\n</code></pre>"},{"location":"usage/responses/#4-store-request-ids","title":"4. Store Request IDs","text":"<pre><code># For debugging and support\ndef save_request_info(response, query_description):\n    \"\"\"Save request information for debugging.\"\"\"\n    info = {\n        'timestamp': response['created_at'],\n        'request_id': response['request_id'],\n        'description': query_description,\n        'model': f\"{response['provider']}/{response['model']}\",\n        'success': response['success'],\n        'cost': response.get('usage', {}).get('cost', 0)\n    }\n\n    # Save to logs or database\n    print(f\"REQUEST_LOG: {info}\")\n\n    return info\n\nresponse = client.chat(messages=[...], model=\"openai/gpt-4o-mini\")\nsave_request_info(response, \"User greeting response\")\n</code></pre>"},{"location":"usage/stt/","title":"Speech-to-Text","text":"<p>IndoxRouter provides a unified interface for transcribing audio to text using speech-to-text models across various AI providers. This guide covers how to use the speech-to-text capabilities.</p>"},{"location":"usage/stt/#basic-usage","title":"Basic Usage","text":"<p>The simplest way to transcribe audio to text is with the <code>speech_to_text()</code> method:</p> <pre><code>from indoxrouter import Client\n\nclient = Client(api_key=\"your_api_key\")\n\n# Transcribe audio from file path\nresponse = client.speech_to_text(\"path/to/audio.mp3\")\n\n# Check if transcription was successful\nif response[\"success\"]:\n    print(\"Transcription:\", response[\"text\"])\nelse:\n    print(f\"Error: {response['message']}\")\n</code></pre>"},{"location":"usage/stt/#file-input-options","title":"File Input Options","text":"<p>You can provide audio in two ways:</p>"},{"location":"usage/stt/#file-path","title":"File Path","text":"<pre><code># Using a file path\nresponse = client.speech_to_text(\"path/to/audio.mp3\")\n</code></pre>"},{"location":"usage/stt/#audio-bytes","title":"Audio Bytes","text":"<pre><code># Using audio data as bytes\nwith open(\"audio.mp3\", \"rb\") as f:\n    audio_data = f.read()\n\nresponse = client.speech_to_text(audio_data)\n</code></pre>"},{"location":"usage/stt/#model-selection","title":"Model Selection","text":"<p>You can use different speech-to-text models from various providers:</p> <pre><code># OpenAI Whisper-1 (default)\nresponse = client.speech_to_text(\n    \"audio.mp3\",\n    model=\"openai/whisper-1\"\n)\n\n# Other providers (when available)\nresponse = client.speech_to_text(\n    \"audio.mp3\",\n    model=\"provider/model-name\"\n)\n</code></pre>"},{"location":"usage/stt/#language-specification","title":"Language Specification","text":"<p>Specify the language of the audio for better accuracy:</p> <pre><code># English audio\nresponse = client.speech_to_text(\n    \"english_audio.mp3\",\n    model=\"openai/whisper-1\",\n    language=\"en\"\n)\n\n# Spanish audio\nresponse = client.speech_to_text(\n    \"spanish_audio.mp3\",\n    model=\"openai/whisper-1\",\n    language=\"es\"\n)\n\n# French audio\nresponse = client.speech_to_text(\n    \"french_audio.mp3\",\n    model=\"openai/whisper-1\",\n    language=\"fr\"\n)\n</code></pre>"},{"location":"usage/stt/#response-formats","title":"Response Formats","text":"<p>Choose different output formats for your transcription:</p> <pre><code># JSON format (default) - returns structured data\njson_response = client.speech_to_text(\n    \"audio.mp3\",\n    model=\"openai/whisper-1\",\n    response_format=\"json\"\n)\n\n# Plain text format - returns just the text\ntext_response = client.speech_to_text(\n    \"audio.mp3\",\n    model=\"openai/whisper-1\",\n    response_format=\"text\"\n)\n\n# SRT subtitle format\nsrt_response = client.speech_to_text(\n    \"audio.mp3\",\n    model=\"openai/whisper-1\",\n    response_format=\"srt\"\n)\n\n# Verbose JSON - includes detailed timing and metadata\nverbose_response = client.speech_to_text(\n    \"audio.mp3\",\n    model=\"openai/whisper-1\",\n    response_format=\"verbose_json\"\n)\n\n# VTT subtitle format\nvtt_response = client.speech_to_text(\n    \"audio.mp3\",\n    model=\"openai/whisper-1\",\n    response_format=\"vtt\"\n)\n</code></pre>"},{"location":"usage/stt/#timestamps-and-segmentation","title":"Timestamps and Segmentation","text":"<p>Get detailed timing information with timestamp granularities:</p> <pre><code># Get word-level timestamps\nresponse = client.speech_to_text(\n    \"audio.mp3\",\n    model=\"openai/whisper-1\",\n    response_format=\"verbose_json\",\n    timestamp_granularities=[\"word\"]\n)\n\n# Get segment-level timestamps\nresponse = client.speech_to_text(\n    \"audio.mp3\",\n    model=\"openai/whisper-1\",\n    response_format=\"verbose_json\",\n    timestamp_granularities=[\"segment\"]\n)\n\n# Get both word and segment timestamps\nresponse = client.speech_to_text(\n    \"audio.mp3\",\n    model=\"openai/whisper-1\",\n    response_format=\"verbose_json\",\n    timestamp_granularities=[\"word\", \"segment\"]\n)\n</code></pre>"},{"location":"usage/stt/#temperature-control","title":"Temperature Control","text":"<p>Adjust the randomness/consistency of the transcription:</p> <pre><code># More consistent transcription (lower temperature)\nresponse = client.speech_to_text(\n    \"audio.mp3\",\n    model=\"openai/whisper-1\",\n    temperature=0.0\n)\n\n# More creative transcription (higher temperature)\nresponse = client.speech_to_text(\n    \"audio.mp3\",\n    model=\"openai/whisper-1\",\n    temperature=0.7\n)\n</code></pre>"},{"location":"usage/stt/#prompt-guidance","title":"Prompt Guidance","text":"<p>Use prompts to guide the transcription style:</p> <pre><code># Formal style prompt\nresponse = client.speech_to_text(\n    \"audio.mp3\",\n    model=\"openai/whisper-1\",\n    prompt=\"This is a formal business meeting transcript.\"\n)\n\n# Technical content prompt\nresponse = client.speech_to_text(\n    \"technical_audio.mp3\",\n    model=\"openai/whisper-1\",\n    prompt=\"This audio contains technical programming terms and code discussions.\"\n)\n</code></pre>"},{"location":"usage/stt/#audio-translation","title":"Audio Translation","text":"<p>Translate foreign language audio directly to English:</p> <pre><code># Translate Spanish audio to English\nresponse = client.translate_audio(\n    \"spanish_audio.mp3\",\n    model=\"openai/whisper-1\"\n)\n\n# Translate with specific format\nresponse = client.translate_audio(\n    \"french_audio.mp3\",\n    model=\"openai/whisper-1\",\n    response_format=\"text\"\n)\n</code></pre>"},{"location":"usage/stt/#using-byok-bring-your-own-key","title":"Using BYOK (Bring Your Own Key)","text":"<p>Use your own provider API keys:</p> <pre><code># Use your own OpenAI API key\nresponse = client.speech_to_text(\n    \"audio.mp3\",\n    model=\"openai/whisper-1\",\n    byok_api_key=\"sk-your-openai-key-here\"\n)\n\n# Translation with BYOK\nresponse = client.translate_audio(\n    \"audio.mp3\",\n    model=\"openai/whisper-1\",\n    byok_api_key=\"sk-your-openai-key-here\"\n)\n</code></pre>"},{"location":"usage/stt/#supported-audio-formats","title":"Supported Audio Formats","text":"<p>The speech-to-text API supports various audio formats:</p> <ul> <li>MP3 - Most common compressed format</li> <li>WAV - Uncompressed format (good quality)</li> <li>FLAC - Lossless compression</li> <li>M4A - Apple audio format</li> <li>OGG - Open-source compressed format</li> <li>WEBM - Web-optimized format</li> </ul> <pre><code># Examples with different formats\nmp3_response = client.speech_to_text(\"audio.mp3\")\nwav_response = client.speech_to_text(\"audio.wav\")\nflac_response = client.speech_to_text(\"audio.flac\")\n</code></pre>"},{"location":"usage/stt/#response-format","title":"Response Format","text":"<p>The speech-to-text response includes several fields:</p> <pre><code>{\n    \"request_id\": \"uuid-string\",\n    \"created_at\": \"2024-01-15T10:30:00.000Z\",\n    \"duration_ms\": 2500,\n    \"provider\": \"openai\",\n    \"model\": \"whisper-1\",\n    \"success\": true,\n    \"message\": \"Audio transcribed successfully\",\n    \"text\": \"Hello, this is the transcribed text.\",\n    \"language\": \"en\",\n    \"usage\": {\n        \"duration_seconds\": 45.2,\n        \"cost\": 0.0068\n    },\n    \"raw_response\": {...}\n}\n</code></pre>"},{"location":"usage/stt/#response-fields","title":"Response Fields","text":"<ul> <li>request_id: Unique identifier for the request</li> <li>created_at: Timestamp when the transcription was created</li> <li>duration_ms: Time taken to process the audio in milliseconds</li> <li>provider: AI provider used (e.g., \"openai\")</li> <li>model: Specific model used (e.g., \"whisper-1\")</li> <li>success: Boolean indicating if the transcription was successful</li> <li>message: Human-readable status message</li> <li>text: The transcribed text</li> <li>language: Detected or specified language</li> <li>usage: Usage statistics including duration and cost</li> <li>raw_response: Raw response from the provider</li> </ul>"},{"location":"usage/stt/#error-handling","title":"Error Handling","text":"<p>Handle common errors that may occur during transcription:</p> <pre><code>from indoxrouter.exceptions import (\n    ModelNotAvailableError,\n    InsufficientCreditsError,\n    ValidationError,\n    InvalidParametersError\n)\n\ntry:\n    response = client.speech_to_text(\n        \"audio.mp3\",\n        model=\"openai/whisper-1\"\n    )\n\n    if response[\"success\"]:\n        text = response[\"text\"]\n        # Process the transcribed text\n    else:\n        print(f\"Transcription failed: {response['message']}\")\n\nexcept ModelNotAvailableError as e:\n    print(f\"Model is not available: {e}\")\nexcept InsufficientCreditsError as e:\n    print(f\"Insufficient credits: {e}\")\nexcept InvalidParametersError as e:\n    print(f\"Invalid parameters: {e}\")\nexcept ValidationError as e:\n    print(f\"Validation error: {e}\")\nexcept Exception as e:\n    print(f\"Unexpected error: {e}\")\n</code></pre>"},{"location":"usage/stt/#best-practices","title":"Best Practices","text":""},{"location":"usage/stt/#audio-quality","title":"Audio Quality","text":"<ul> <li>Use high-quality audio files for better transcription accuracy</li> <li>Minimize background noise when possible</li> <li>Ensure clear speech and appropriate volume levels</li> </ul>"},{"location":"usage/stt/#file-size","title":"File Size","text":"<ul> <li>Keep audio files under 25MB for optimal processing</li> <li>For longer audio, consider splitting into smaller segments</li> </ul>"},{"location":"usage/stt/#language-specification_1","title":"Language Specification","text":"<ul> <li>Always specify the language when known for better accuracy</li> <li>Use the correct ISO language code (e.g., \"en\", \"es\", \"fr\")</li> </ul>"},{"location":"usage/stt/#response-format-selection","title":"Response Format Selection","text":"<ul> <li>Use \"json\" for most applications requiring structured data</li> <li>Use \"text\" when you only need the transcribed text</li> <li>Use \"verbose_json\" when you need detailed timing information</li> <li>Use \"srt\" or \"vtt\" for subtitle generation</li> </ul>"},{"location":"usage/stt/#cost-optimization","title":"Cost Optimization","text":"<ul> <li>Use appropriate temperature settings (lower values are more consistent)</li> <li>Consider using BYOK for high-volume transcriptions</li> <li>Monitor usage through the usage endpoint</li> </ul>"},{"location":"usage/stt/#complete-example","title":"Complete Example","text":"<p>Here's a comprehensive example that demonstrates various features:</p> <pre><code>from indoxrouter import Client\n\ndef transcribe_audio_file(file_path, language=None):\n    client = Client(api_key=\"your_api_key\")\n\n    try:\n        # Transcribe with detailed options\n        response = client.speech_to_text(\n            file_path,\n            model=\"openai/whisper-1\",\n            language=language,\n            response_format=\"verbose_json\",\n            temperature=0.2,\n            timestamp_granularities=[\"word\", \"segment\"]\n        )\n\n        if response[\"success\"]:\n            print(f\"\u2705 Transcription successful!\")\n            print(f\"\ud83d\udcdd Text: {response['text']}\")\n            print(f\"\ud83c\udf0d Language: {response.get('language', 'Unknown')}\")\n            print(f\"\u23f1\ufe0f  Duration: {response['usage']['duration_seconds']} seconds\")\n            print(f\"\ud83d\udcb0 Cost: ${response['usage']['cost']}\")\n\n            # Process segments if available\n            if \"segments\" in response:\n                print(f\"\ud83d\udcca Segments: {len(response['segments'])}\")\n                for i, segment in enumerate(response['segments'][:3]):  # First 3 segments\n                    start = segment.get('start', 0)\n                    end = segment.get('end', 0)\n                    text = segment.get('text', '')\n                    print(f\"   {i+1}. [{start:.1f}s - {end:.1f}s]: {text}\")\n\n            return response['text']\n        else:\n            print(f\"\u274c Error: {response.get('message', 'Unknown error')}\")\n            return None\n\n    except Exception as e:\n        print(f\"\ud83d\udca5 Exception: {e}\")\n        return None\n    finally:\n        client.close()\n\n# Usage\ntranscribed_text = transcribe_audio_file(\"meeting_recording.mp3\", language=\"en\")\n</code></pre>"},{"location":"usage/tracking/","title":"Usage Tracking","text":"<p>IndoxRouter provides comprehensive usage tracking to help you monitor your API consumption, costs, and performance metrics.</p>"},{"location":"usage/tracking/#getting-usage-statistics","title":"Getting Usage Statistics","text":"<p>Use the <code>get_usage()</code> method to retrieve detailed usage information:</p> <pre><code>from indoxrouter import IndoxRouter\n\nclient = IndoxRouter(api_key=\"your-api-key\")\n\n# Get current usage statistics\nusage_stats = client.get_usage()\nprint(usage_stats)\n</code></pre>"},{"location":"usage/tracking/#usage-response-format","title":"Usage Response Format","text":"<p>The <code>get_usage()</code> method returns detailed statistics including:</p> <pre><code>{\n    \"total_requests\": 1250,\n    \"total_tokens\": 45000,\n    \"total_cost\": 12.50,\n    \"current_period\": {\n        \"requests\": 150,\n        \"tokens\": 5500,\n        \"cost\": 1.75,\n        \"period_start\": \"2024-01-01T00:00:00Z\",\n        \"period_end\": \"2024-01-31T23:59:59Z\"\n    },\n    \"by_provider\": {\n        \"openai\": {\n            \"requests\": 800,\n            \"tokens\": 28000,\n            \"cost\": 8.40\n        },\n        \"anthropic\": {\n            \"requests\": 300,\n            \"tokens\": 12000,\n            \"cost\": 2.88\n        },\n        \"google\": {\n            \"requests\": 150,\n            \"tokens\": 5000,\n            \"cost\": 1.22\n        }\n    },\n    \"by_model\": {\n        \"gpt-4\": {\n            \"requests\": 400,\n            \"tokens\": 15000,\n            \"cost\": 4.50\n        },\n        \"claude-3-sonnet\": {\n            \"requests\": 300,\n            \"tokens\": 12000,\n            \"cost\": 2.88\n        },\n        \"gemini-pro\": {\n            \"requests\": 150,\n            \"tokens\": 5000,\n            \"cost\": 1.22\n        }\n    }\n}\n</code></pre>"},{"location":"usage/tracking/#real-time-usage-in-responses","title":"Real-time Usage in Responses","text":"<p>Every API response includes detailed usage information:</p>"},{"location":"usage/tracking/#chat-completion-usage","title":"Chat Completion Usage","text":"<pre><code>response = client.chat_completions(\n    provider=\"openai\",\n    model=\"gpt-4\",\n    messages=[{\"role\": \"user\", \"content\": \"Hello!\"}]\n)\n\n# Access usage information\nusage = response.usage\nprint(f\"Prompt tokens: {usage.prompt_tokens}\")\nprint(f\"Completion tokens: {usage.completion_tokens}\")\nprint(f\"Total tokens: {usage.total_tokens}\")\nprint(f\"Cost: ${usage.cost_breakdown.total_cost}\")\n</code></pre>"},{"location":"usage/tracking/#detailed-usage-fields","title":"Detailed Usage Fields","text":"<p>Each response includes comprehensive usage tracking:</p> <ul> <li><code>prompt_tokens</code>: Tokens used in the input</li> <li><code>completion_tokens</code>: Tokens generated in the response</li> <li><code>total_tokens</code>: Sum of prompt and completion tokens</li> <li><code>cache_read_tokens</code>: Tokens read from cache (if applicable)</li> <li><code>cache_write_tokens</code>: Tokens written to cache (if applicable)</li> <li><code>reasoning_tokens</code>: Tokens used for reasoning (for reasoning models)</li> <li><code>web_search_count</code>: Number of web searches performed</li> <li><code>request_count</code>: Number of API requests made</li> <li><code>cost_breakdown</code>: Detailed cost information</li> </ul>"},{"location":"usage/tracking/#cost-breakdown","title":"Cost Breakdown","text":"<p>The <code>cost_breakdown</code> object provides detailed pricing information:</p> <pre><code>{\n    \"prompt_cost\": 0.015,\n    \"completion_cost\": 0.030,\n    \"cache_read_cost\": 0.0015,\n    \"cache_write_cost\": 0.0075,\n    \"reasoning_cost\": 0.240,\n    \"web_search_cost\": 0.001,\n    \"total_cost\": 0.2935\n}\n</code></pre>"},{"location":"usage/tracking/#monitoring-best-practices","title":"Monitoring Best Practices","text":""},{"location":"usage/tracking/#1-regular-usage-checks","title":"1. Regular Usage Checks","text":"<pre><code># Check usage before making expensive requests\nusage = client.get_usage()\nif usage[\"current_period\"][\"cost\"] &gt; 50.0:\n    print(\"Warning: High usage this period\")\n</code></pre>"},{"location":"usage/tracking/#2-provider-cost-optimization","title":"2. Provider Cost Optimization","text":"<pre><code># Compare costs across providers\nusage = client.get_usage()\nfor provider, stats in usage[\"by_provider\"].items():\n    cost_per_token = stats[\"cost\"] / stats[\"tokens\"]\n    print(f\"{provider}: ${cost_per_token:.6f} per token\")\n</code></pre>"},{"location":"usage/tracking/#3-model-performance-tracking","title":"3. Model Performance Tracking","text":"<pre><code># Track model efficiency\nusage = client.get_usage()\nfor model, stats in usage[\"by_model\"].items():\n    avg_tokens_per_request = stats[\"tokens\"] / stats[\"requests\"]\n    print(f\"{model}: {avg_tokens_per_request:.1f} tokens per request\")\n</code></pre>"},{"location":"usage/tracking/#rate-limit-monitoring","title":"Rate Limit Monitoring","text":"<p>Usage tracking also helps monitor rate limit consumption:</p> <pre><code># Check current rate limit status\nusage = client.get_usage()\ncurrent_requests = usage[\"current_period\"][\"requests\"]\nprint(f\"Requests this period: {current_requests}\")\n\n# Rate limits vary by tier:\n# - Free: 10 requests/minute, 10K tokens/hour\n# - Standard: 60 requests/minute, 100K tokens/hour\n# - Enterprise: 500 requests/minute, 1M tokens/hour\n</code></pre>"},{"location":"usage/tracking/#export-usage-data","title":"Export Usage Data","text":"<p>For detailed analysis, you can export usage data:</p> <pre><code>import json\nfrom datetime import datetime\n\n# Get usage data\nusage = client.get_usage()\n\n# Save to file with timestamp\ntimestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\nfilename = f\"usage_report_{timestamp}.json\"\n\nwith open(filename, 'w') as f:\n    json.dump(usage, f, indent=2)\n\nprint(f\"Usage report saved to {filename}\")\n</code></pre>"},{"location":"usage/tracking/#integration-with-analytics","title":"Integration with Analytics","text":"<p>Usage data can be integrated with analytics platforms:</p> <pre><code># Example: Send to analytics service\ndef track_usage_metrics(usage_data):\n    metrics = {\n        'total_requests': usage_data['total_requests'],\n        'total_cost': usage_data['total_cost'],\n        'avg_cost_per_request': usage_data['total_cost'] / usage_data['total_requests']\n    }\n\n    # Send to your analytics platform\n    # analytics_client.track('api_usage', metrics)\n\ntrack_usage_metrics(client.get_usage())\n</code></pre>"},{"location":"usage/tts/","title":"Text-to-Speech","text":"<p>IndoxRouter provides a unified interface for generating audio from text using text-to-speech models across various AI providers. This guide covers how to use the text-to-speech capabilities.</p>"},{"location":"usage/tts/#basic-usage","title":"Basic Usage","text":"<p>The simplest way to generate audio from text is with the <code>text_to_speech()</code> method:</p> <pre><code>from indoxrouter import Client\n\nclient = Client(api_key=\"your_api_key\")\n\n# Generate audio from text\nresponse = client.text_to_speech(\n    input=\"Hello, welcome to IndoxRouter!\",\n    model=\"openai/tts-1\"\n)\n\n# Check if audio was generated successfully\nif response[\"success\"]:\n    print(\"Audio generated successfully!\")\n    audio_url = response[\"data\"][\"url\"]\n    print(f\"Audio URL: {audio_url}\")\nelse:\n    print(f\"Error: {response['message']}\")\n</code></pre>"},{"location":"usage/tts/#model-selection","title":"Model Selection","text":"<p>You can use different text-to-speech models from various providers:</p> <pre><code># OpenAI TTS-1 (faster, lower quality)\ntts1_response = client.text_to_speech(\n    input=\"This is generated with TTS-1\",\n    model=\"openai/tts-1\"\n)\n\n# OpenAI TTS-1-HD (slower, higher quality)\ntts1_hd_response = client.text_to_speech(\n    input=\"This is generated with TTS-1-HD\",\n    model=\"openai/tts-1-hd\"\n)\n</code></pre>"},{"location":"usage/tts/#byok-bring-your-own-key-support","title":"BYOK (Bring Your Own Key) Support","text":"<p>IndoxRouter supports BYOK for text-to-speech, allowing you to use your own API keys for AI providers:</p> <pre><code># Use your own OpenAI API key for TTS\nresponse = client.text_to_speech(\n    input=\"Hello, this is generated with my own OpenAI API key\",\n    model=\"openai/tts-1\",\n    voice=\"alloy\",\n    byok_api_key=\"sk-your-openai-key-here\"\n)\n</code></pre>"},{"location":"usage/tts/#byok-benefits-for-text-to-speech","title":"BYOK Benefits for Text-to-Speech","text":"<ul> <li>No Credit Deduction: Your IndoxRouter credits remain unchanged</li> <li>No Rate Limiting: Bypass platform rate limits</li> <li>Direct Provider Access: Connect directly to your provider accounts</li> <li>Cost Control: Pay providers directly at their rates</li> <li>Full Features: Access to all provider-specific TTS features</li> <li>Higher Quality: Use provider's native text-to-speech capabilities</li> </ul>"},{"location":"usage/tts/#voice-selection","title":"Voice Selection","text":"<p>Different providers offer different voices. For OpenAI, you can choose from several voices:</p> <pre><code># Available OpenAI voices: alloy, echo, fable, onyx, nova, shimmer\nvoices = [\"alloy\", \"echo\", \"fable\", \"onyx\", \"nova\", \"shimmer\"]\n\nfor voice in voices:\n    response = client.text_to_speech(\n        input=f\"This is the {voice} voice.\",\n        model=\"openai/tts-1\",\n        voice=voice\n    )\n    print(f\"Generated audio with {voice} voice\")\n</code></pre>"},{"location":"usage/tts/#voice-characteristics","title":"Voice Characteristics","text":"<ul> <li>alloy: Balanced, neutral voice</li> <li>echo: Clear, professional voice</li> <li>fable: Warm, storytelling voice</li> <li>onyx: Deep, authoritative voice</li> <li>nova: Bright, energetic voice</li> <li>shimmer: Soft, gentle voice</li> </ul>"},{"location":"usage/tts/#audio-format-options","title":"Audio Format Options","text":"<p>You can specify different audio output formats:</p> <pre><code># MP3 format (default)\nmp3_response = client.text_to_speech(\n    input=\"This will be in MP3 format\",\n    model=\"openai/tts-1\",\n    response_format=\"mp3\"\n)\n\n# Opus format (good for real-time applications)\nopus_response = client.text_to_speech(\n    input=\"This will be in Opus format\",\n    model=\"openai/tts-1\",\n    response_format=\"opus\"\n)\n\n# AAC format (good for mobile devices)\naac_response = client.text_to_speech(\n    input=\"This will be in AAC format\",\n    model=\"openai/tts-1\",\n    response_format=\"aac\"\n)\n\n# FLAC format (lossless quality)\nflac_response = client.text_to_speech(\n    input=\"This will be in FLAC format\",\n    model=\"openai/tts-1\",\n    response_format=\"flac\"\n)\n</code></pre>"},{"location":"usage/tts/#speed-control","title":"Speed Control","text":"<p>Adjust the playback speed of the generated audio:</p> <pre><code># Slower speech (0.25x speed)\nslow_response = client.text_to_speech(\n    input=\"This will be spoken very slowly.\",\n    model=\"openai/tts-1\",\n    speed=0.25\n)\n\n# Normal speed (1.0x - default)\nnormal_response = client.text_to_speech(\n    input=\"This will be spoken at normal speed.\",\n    model=\"openai/tts-1\",\n    speed=1.0\n)\n\n# Faster speech (2.0x speed)\nfast_response = client.text_to_speech(\n    input=\"This will be spoken quickly.\",\n    model=\"openai/tts-1\",\n    speed=2.0\n)\n\n# Maximum speed (4.0x speed)\nmax_speed_response = client.text_to_speech(\n    input=\"This will be spoken very quickly.\",\n    model=\"openai/tts-1\",\n    speed=4.0\n)\n</code></pre> <p>Speed Range</p> <p>The speed parameter accepts values from 0.25 to 4.0, where:</p> <pre><code>- 0.25 = Quarter speed (very slow)\n- 1.0 = Normal speed\n- 4.0 = Quadruple speed (very fast)\n</code></pre>"},{"location":"usage/tts/#advanced-usage","title":"Advanced Usage","text":""},{"location":"usage/tts/#with-instructions","title":"With Instructions","text":"<p>Some providers may support additional instructions for fine-tuning the speech generation:</p> <pre><code>response = client.text_to_speech(\n    input=\"Welcome to our premium service!\",\n    model=\"openai/tts-1\",\n    voice=\"nova\",\n    instructions=\"Speak with enthusiasm and excitement\"\n)\n</code></pre>"},{"location":"usage/tts/#complete-example","title":"Complete Example","text":"<p>Here's a comprehensive example that demonstrates all the parameters:</p> <pre><code>from indoxrouter import Client\n\nclient = Client(api_key=\"your_api_key\")\n\ndef generate_speech_sample():\n    response = client.text_to_speech(\n        input=\"Welcome to IndoxRouter, your unified AI API gateway. \"\n               \"We provide seamless access to multiple AI providers \"\n               \"through a single, consistent interface.\",\n        model=\"openai/tts-1-hd\",\n        voice=\"alloy\",\n        response_format=\"mp3\",\n        speed=1.1,\n        instructions=\"Speak clearly and professionally\"\n    )\n\n    if response[\"success\"]:\n        print(f\"\u2705 Audio generated successfully!\")\n        print(f\"\ud83d\udcca Usage: {response.get('usage', {})}\")\n        print(f\"\ud83c\udfd7\ufe0f Provider: {response.get('provider', 'N/A')}\")\n        print(f\"\ud83e\udd16 Model: {response.get('model', 'N/A')}\")\n        print(f\"\u23f1\ufe0f Duration: {response.get('duration_ms', 0)}ms\")\n\n        # The audio URL is available in response[\"data\"][\"url\"]\n        audio_url = response[\"data\"][\"url\"]\n        print(f\"\ud83d\udd17 Audio URL: {audio_url}\")\n\n        # You can download the audio file using the URL\n        # import requests\n        # audio_response = requests.get(audio_url)\n        # with open(\"generated_speech.mp3\", \"wb\") as f:\n        #     f.write(audio_response.content)\n\n        return audio_url\n    else:\n        print(f\"\u274c Error: {response.get('message', 'Unknown error')}\")\n        return None\n\n# Generate the speech\naudio_data = generate_speech_sample()\n</code></pre>"},{"location":"usage/tts/#response-format","title":"Response Format","text":"<p>The text-to-speech response includes several fields:</p> <pre><code>{\n    \"request_id\": \"uuid-string\",\n    \"created_at\": \"2024-01-15T10:30:00.000Z\",\n    \"duration_ms\": 1250,\n    \"provider\": \"openai\",\n    \"model\": \"tts-1\",\n    \"success\": true,\n    \"message\": \"Audio generated successfully\",\n    \"data\": {\n        \"url\": \"https://generated-audio.example.com/audio_12345.mp3\"\n    },\n    \"usage\": {\n        \"characters\": 150,\n        \"cost\": 0.0075\n    },\n    \"raw_response\": {...}\n}\n</code></pre>"},{"location":"usage/tts/#response-fields","title":"Response Fields","text":"<ul> <li>request_id: Unique identifier for the request</li> <li>created_at: Timestamp when the audio was generated</li> <li>duration_ms: Time taken to generate the audio in milliseconds</li> <li>provider: AI provider used (e.g., \"openai\")</li> <li>model: Specific model used (e.g., \"tts-1\")</li> <li>success: Boolean indicating if the generation was successful</li> <li>message: Human-readable status message</li> <li>data: Object containing the URL to the generated audio file</li> <li>usage: Usage statistics including character count and cost</li> <li>raw_response: Raw response from the provider</li> </ul>"},{"location":"usage/tts/#error-handling","title":"Error Handling","text":"<p>Handle common errors that may occur during text-to-speech generation:</p> <pre><code>from indoxrouter.exceptions import (\n    ModelNotAvailableError,\n    InsufficientCreditsError,\n    ValidationError\n)\n\ntry:\n    response = client.text_to_speech(\n        input=\"Text to convert to speech\",\n        model=\"openai/tts-1\",\n        voice=\"alloy\"\n    )\n\n    if response[\"success\"]:\n        audio_url = response[\"data\"][\"url\"]\n        # Process the audio URL\n    else:\n        print(f\"Generation failed: {response['message']}\")\n\nexcept ModelNotAvailableError as e:\n    print(f\"Model is not available: {e}\")\nexcept InsufficientCreditsError as e:\n    print(f\"Insufficient credits: {e}\")\nexcept ValidationError as e:\n    print(f\"Invalid parameters: {e}\")\nexcept Exception as e:\n    print(f\"Unexpected error: {e}\")\n</code></pre>"},{"location":"usage/tts/#best-practices","title":"Best Practices","text":""},{"location":"usage/tts/#text-optimization","title":"Text Optimization","text":"<ul> <li>Keep it concise: Shorter texts generally produce better audio quality</li> <li>Use punctuation: Proper punctuation helps with natural speech rhythm</li> <li>Avoid special characters: Some symbols may not be pronounced correctly</li> </ul> <pre><code># Good example\ngood_text = \"Hello! Welcome to our service. How can we help you today?\"\n\n# Less optimal example\npoor_text = \"hello welcome 2 our service how can we help u 2day???\"\n\nresponse = client.text_to_speech(\n    input=good_text,\n    model=\"openai/tts-1\",\n    voice=\"alloy\"\n)\n</code></pre>"},{"location":"usage/tts/#voice-selection-guidelines","title":"Voice Selection Guidelines","text":"<ul> <li>Presentations: Use \"echo\" or \"onyx\" for professional content</li> <li>Storytelling: Use \"fable\" for narrative content</li> <li>Casual content: Use \"alloy\" or \"nova\" for friendly interactions</li> <li>Customer service: Use \"shimmer\" for gentle, helpful responses</li> </ul>"},{"location":"usage/tts/#performance-considerations","title":"Performance Considerations","text":"<ul> <li>Use TTS-1 for real-time applications where speed is important</li> <li>Use TTS-1-HD for high-quality content where audio fidelity matters</li> <li>Cache generated audio when possible to avoid repeated API calls</li> <li>Monitor usage costs as TTS can be more expensive than text generation</li> </ul>"},{"location":"usage/tts/#provider-specific-features","title":"Provider-Specific Features","text":""},{"location":"usage/tts/#openai-features","title":"OpenAI Features","text":"<p>OpenAI's TTS models support:</p> <ul> <li>Multiple high-quality voices optimized for different use cases</li> <li>Variable speed control from 0.25x to 4.0x</li> <li>Multiple output formats (MP3, Opus, AAC, FLAC)</li> <li>Consistent quality across different text lengths</li> </ul>"},{"location":"usage/tts/#future-provider-support","title":"Future Provider Support","text":"<p>IndoxRouter is designed to support multiple TTS providers. As new providers are added, they may offer:</p> <ul> <li>Different voice options and characteristics</li> <li>Unique audio processing capabilities</li> <li>Provider-specific parameters and optimizations</li> <li>Various pricing models and usage limits</li> </ul>"},{"location":"usage/tts/#troubleshooting","title":"Troubleshooting","text":""},{"location":"usage/tts/#common-issues","title":"Common Issues","text":"<p>Audio not generating:</p> <ul> <li>Check that your API key has TTS permissions</li> <li>Verify you have sufficient credits</li> <li>Ensure the model name is correct</li> </ul> <p>Poor audio quality:</p> <ul> <li>Try using TTS-1-HD instead of TTS-1</li> <li>Adjust the speed parameter</li> <li>Use proper punctuation in your text</li> </ul> <p>Unsupported parameters:</p> <ul> <li>Some parameters may not be supported by all providers</li> <li>Check provider documentation for supported features</li> <li>Use <code>additional_params</code> for provider-specific options</li> </ul>"},{"location":"usage/tts/#getting-help","title":"Getting Help","text":"<p>If you encounter issues:</p> <ol> <li>Check the error message in the response</li> <li>Verify your API key and credits</li> <li>Review the parameter documentation</li> <li>Contact support with your request ID for specific issues</li> </ol> <p>Rate Limits</p> <p>Text-to-speech requests may have different rate limits than text generation. Monitor your usage and implement appropriate retry logic for production applications.</p>"},{"location":"usage/video/","title":"Video Generation","text":"<p>IndoxRouter provides a unified interface for generating videos from text prompts across various AI providers. This guide covers how to use the video generation capabilities with support for different models from Google, OpenAI, and Amazon.</p>"},{"location":"usage/video/#basic-usage","title":"Basic Usage","text":"<p>The simplest way to generate videos is with the <code>videos()</code> method:</p> <pre><code>from indoxrouter import Client\n\nclient = Client(api_key=\"your_api_key\")\n\n# Generate a video (async operation)\nresponse = client.videos(\n    prompt=\"A serene mountain landscape with a lake at sunset, cinematic camera movement\",\n    model=\"openai/sora-2\"\n)\n\n# Get the job status\nprint(f\"Job ID: {response['data']['job_id']}\")\nprint(f\"Status: {response['data']['status']}\")\n</code></pre>"},{"location":"usage/video/#understanding-video-generation","title":"Understanding Video Generation","text":"<p>Video generation is typically an asynchronous operation. Unlike image generation which returns results immediately, video generation creates a job that you can monitor for completion:</p> <pre><code># Check job status\nstatus_response = client.jobs(job_id=\"your_job_id\")\nprint(f\"Status: {status_response['status']}\")\nprint(f\"Progress: {status_response['progress']}%\")\n\n# Once completed, get the video URL\nif status_response['status'] == 'completed':\n    video_url = status_response['result']['data'][0]['url']\n    print(f\"Video URL: {video_url}\")\n</code></pre>"},{"location":"usage/video/#model-selection","title":"Model Selection","text":"<p>You can use different video generation models from various providers:</p>"},{"location":"usage/video/#openai-sora-models","title":"OpenAI Sora Models","text":"<pre><code># Sora 2 (recommended for most use cases)\nsora_response = client.videos(\n    prompt=\"A cat playing piano in a jazz club\",\n    model=\"openai/sora-2\",\n    size=\"1280x720\",\n    seconds=4\n)\n\n# Sora 2 Pro (higher quality, more expensive)\nsora_pro_response = client.videos(\n    prompt=\"A cinematic battle scene with dramatic lighting\",\n    model=\"openai/sora-2-pro\",\n    size=\"1792x1024\",\n    seconds=8\n)\n</code></pre>"},{"location":"usage/video/#google-veo-models","title":"Google Veo Models","text":"<pre><code># Veo 2 (text-to-video and image-to-video)\nveo2_response = client.videos(\n    prompt=\"A butterfly emerging from its chrysalis\",\n    model=\"google/veo-2.0-generate-001\",\n    aspect_ratio=\"16:9\",\n    duration=6\n)\n\n# Veo 3 (with optional audio generation)\nveo3_response = client.videos(\n    prompt=\"A chef preparing pasta in an Italian kitchen\",\n    model=\"google/veo-3.0-generate-001\",\n    aspect_ratio=\"16:9\",\n    duration=8,\n    generate_audio=True  # Include synchronized audio\n)\n\n# Veo 3.1 (advanced features)\nveo31_response = client.videos(\n    prompt=\"A futuristic city with flying cars\",\n    model=\"google/veo-3.1-generate-preview\",\n    aspect_ratio=\"16:9\",\n    duration=8,\n    generate_audio=True,\n    negative_prompt=\"blurry, low quality, static\",\n    person_generation=\"allow_all\"\n)\n</code></pre>"},{"location":"usage/video/#amazon-nova-reel","title":"Amazon Nova Reel","text":"<pre><code># Amazon Nova Reel (cost-effective option)\nnova_response = client.videos(\n    prompt=\"A timelapse of a flower blooming\",\n    model=\"amazon/nova-reel\",\n    duration=6\n)\n</code></pre>"},{"location":"usage/video/#video-parameters","title":"Video Parameters","text":"<p>The video generation method accepts several parameters to control the output:</p>"},{"location":"usage/video/#core-parameters","title":"Core Parameters","text":"<pre><code>response = client.videos(\n    prompt=\"A majestic eagle soaring over mountain peaks\",  # Required: text description\n    model=\"openai/sora-2\",                                # Required: provider/model format\n    size=\"1280x720\",                                      # Video resolution/dimensions\n    duration=4,                                           # Video duration in seconds\n    n=1,                                                  # Number of videos to generate\n    aspect_ratio=\"16:9\",                                  # Video aspect ratio\n    fps=24,                                               # Frames per second\n    seed=42                                               # Random seed for reproducibility\n)\n</code></pre>"},{"location":"usage/video/#provider-specific-parameters","title":"Provider-Specific Parameters","text":""},{"location":"usage/video/#openai-sora-parameters","title":"OpenAI Sora Parameters","text":"<pre><code>sora_response = client.videos(\n    prompt=\"A ballet dancer performing in a spotlight\",\n    model=\"openai/sora-2\",\n    size=\"1280x720\",        # 1280x720 or 720x1280\n    seconds=4,              # 4, 8, or 12 seconds\n    input_image=\"data:image/png;base64,...\",  # Optional: base64 image for image-to-video\n    input_reference=\"data:image/png;base64,...\"  # Optional: reference image\n)\n</code></pre>"},{"location":"usage/video/#google-veo-parameters","title":"Google Veo Parameters","text":"<pre><code>veo_response = client.videos(\n    prompt=\"A serene underwater scene with colorful fish\",\n    model=\"google/veo-3.0-generate-001\",\n    aspect_ratio=\"16:9\",                    # 16:9 or 9:16\n    resolution=\"720p\",                      # 720p or 1080p (Veo 3+)\n    duration=8,                             # 4-8 seconds depending on model\n    generate_audio=True,                    # Generate synchronized audio\n    input_image=\"data:image/png;base64,...\", # Image-to-video input\n    reference_image=\"data:image/png;base64,...\", # Reference image for style\n    reference_images=[\"data:image/png;base64,...\", \"data:image/png;base64,...\"], # Multiple references (Veo 3.1)\n    negative_prompt=\"blurry, distorted, low quality\", # What to avoid (Veo 3.1)\n    person_generation=\"allow_adult\",         # Person generation control (Veo 3.1)\n    last_frame=\"data:image/png;base64,...\", # Frame interpolation (Veo 3.1)\n    video=\"data:video/mp4;base64,...\"       # Video extension input (future)\n)\n</code></pre>"},{"location":"usage/video/#amazon-nova-reel-parameters","title":"Amazon Nova Reel Parameters","text":"<pre><code>nova_response = client.videos(\n    prompt=\"A cup of coffee steaming on a wooden table\",\n    model=\"amazon/nova-reel\",\n    dimension=\"1280x720\",    # Fixed: 1280x720\n    duration_seconds=6,      # Fixed: 6 seconds\n    fps=24,                  # Fixed: 24 fps\n    seed=123,                # Random seed\n    input_image=\"data:image/png;base64,...\" # Optional: base64 image input\n)\n</code></pre>"},{"location":"usage/video/#job-monitoring-and-results","title":"Job Monitoring and Results","text":""},{"location":"usage/video/#checking-job-status","title":"Checking Job Status","text":"<pre><code># Get job status\njob_response = client.jobs(job_id=\"your_job_id\")\n\nprint(f\"Job Status: {job_response['status']}\")\nprint(f\"Progress: {job_response['progress']}%\")\nprint(f\"Created: {job_response['created_at']}\")\n\nif job_response['status'] == 'completed':\n    # Access the generated video\n    video_data = job_response['result']['data'][0]\n    video_url = video_data['url']\n    print(f\"Video URL: {video_url}\")\nelif job_response['status'] == 'failed':\n    print(f\"Error: {job_response.get('error', 'Unknown error')}\")\n</code></pre>"},{"location":"usage/video/#response-structure","title":"Response Structure","text":"<p>Completed video generation returns:</p> <pre><code>{\n    \"job_id\": \"video_job_123456\",\n    \"status\": \"completed\",\n    \"progress\": 100,\n    \"created_at\": \"2024-01-15T10:30:00Z\",\n    \"result\": {\n        \"data\": [\n            {\n                \"url\": \"https://example.com/generated-video-123.mp4\",\n                \"s3_key\": \"user_123/video/video_456.mp4\",\n                \"file_size\": 5242880,\n                \"content_type\": \"video/mp4\",\n                \"file_name\": \"video_456.mp4\"\n            }\n        ],\n        \"model\": \"openai/sora-2\",\n        \"created\": 1705312200\n    }\n}\n</code></pre>"},{"location":"usage/video/#saving-generated-videos","title":"Saving Generated Videos","text":"<p>To save the generated videos locally:</p> <pre><code>import requests\nimport os\n\ndef save_video(url, filename):\n    \"\"\"Download and save a video from URL to a local file.\"\"\"\n    response = requests.get(url, stream=True)\n    if response.status_code == 200:\n        with open(filename, 'wb') as f:\n            for chunk in response.iter_content(chunk_size=8192):\n                f.write(chunk)\n        print(f\"Video saved as {filename}\")\n    else:\n        print(f\"Failed to download video: {response.status_code}\")\n\n# Check if job is completed\njob_response = client.jobs(job_id=\"your_job_id\")\n\nif job_response['status'] == 'completed':\n    # Save each generated video\n    os.makedirs(\"generated_videos\", exist_ok=True)\n    for i, video_item in enumerate(job_response['result']['data']):\n        video_url = video_item['url']\n        filename = f\"generated_videos/video_{i}.mp4\"\n        save_video(video_url, filename)\nelse:\n    print(\"Video generation not yet completed\")\n</code></pre>"},{"location":"usage/video/#advanced-features","title":"Advanced Features","text":""},{"location":"usage/video/#image-to-video-generation","title":"Image-to-Video Generation","text":"<p>Transform existing images into videos:</p> <pre><code># OpenAI Sora with image input\nimage_video_response = client.videos(\n    prompt=\"Make this image come alive with gentle movement and flowing water\",\n    model=\"openai/sora-2\",\n    input_image=\"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAA...\",  # Base64 encoded image\n    size=\"1280x720\",\n    seconds=4\n)\n\n# Google Veo with image input\nveo_image_response = client.videos(\n    prompt=\"Animate this landscape with a gentle breeze and moving clouds\",\n    model=\"google/veo-3.0-generate-001\",\n    input_image=\"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAA...\",\n    aspect_ratio=\"16:9\",\n    duration=8,\n    generate_audio=True\n)\n</code></pre>"},{"location":"usage/video/#reference-images-and-style-control","title":"Reference Images and Style Control","text":"<p>Use reference images to guide generation:</p> <pre><code># Google Veo 3.1 with multiple reference images\nstyled_response = client.videos(\n    prompt=\"A ballet dancer in the style of these reference images\",\n    model=\"google/veo-3.1-generate-preview\",\n    reference_images=[\n        \"data:image/png;base64,...\",  # Reference image 1\n        \"data:image/png;base64,...\",  # Reference image 2\n        \"data:image/png;base64,...\"   # Reference image 3\n    ],\n    aspect_ratio=\"9:16\",\n    duration=8,\n    generate_audio=True\n)\n</code></pre>"},{"location":"usage/video/#audio-generation","title":"Audio Generation","text":"<p>Generate videos with synchronized audio:</p> <pre><code># Google Veo 3+ with audio\naudio_video_response = client.videos(\n    prompt=\"A musician playing guitar on a beach at sunset\",\n    model=\"google/veo-3.0-generate-001\",\n    aspect_ratio=\"16:9\",\n    duration=8,\n    generate_audio=True  # Generates music and sound effects\n)\n</code></pre>"},{"location":"usage/video/#frame-interpolation-and-video-extension","title":"Frame Interpolation and Video Extension","text":"<p>Advanced features in Google Veo 3.1:</p> <pre><code># Frame interpolation between two frames\ninterpolation_response = client.videos(\n    prompt=\"Smooth transition between these two poses\",\n    model=\"google/veo-3.1-fast-generate-preview\",\n    input_image=\"data:image/png;base64,...\",  # Starting frame\n    last_frame=\"data:image/png;base64,...\",   # Ending frame\n    duration=8,\n    generate_audio=True\n)\n</code></pre>"},{"location":"usage/video/#byok-bring-your-own-key-support","title":"BYOK (Bring Your Own Key) Support","text":"<p>IndoxRouter supports BYOK for video generation:</p> <pre><code># Use your own OpenAI API key\nresponse = client.videos(\n    prompt=\"A rocket launching into space\",\n    model=\"openai/sora-2\",\n    size=\"1280x720\",\n    seconds=4,\n    byok_api_key=\"sk-your-openai-key-here\"\n)\n\n# Use your own Google API key\ngoogle_response = client.videos(\n    prompt=\"A butterfly emerging from its chrysalis\",\n    model=\"google/veo-3.0-generate-001\",\n    aspect_ratio=\"16:9\",\n    duration=8,\n    byok_api_key=\"your-google-api-key-here\"\n)\n</code></pre>"},{"location":"usage/video/#byok-benefits-for-video-generation","title":"BYOK Benefits for Video Generation","text":"<ul> <li>No Credit Deduction: Your IndoxRouter credits remain unchanged</li> <li>No Rate Limiting: Bypass platform rate limits</li> <li>Direct Provider Access: Connect directly to your provider accounts</li> <li>Cost Control: Pay providers directly at their rates</li> <li>Full Features: Access to all provider-specific video generation features</li> </ul>"},{"location":"usage/video/#examples","title":"Examples","text":""},{"location":"usage/video/#cinematic-scene-generation","title":"Cinematic Scene Generation","text":"<pre><code># Create a cinematic video scene\ncinematic_response = client.videos(\n    prompt=(\n        \"A dramatic cinematic scene: a lone warrior standing on a cliff \"\n        \"overlooking a vast ocean at sunset, wind blowing through their \"\n        \"hair, camera slowly dollying in for an emotional close-up, \"\n        \"epic orchestral music swelling in the background\"\n    ),\n    model=\"openai/sora-2-pro\",\n    size=\"1792x1024\",\n    seconds=8\n)\n</code></pre>"},{"location":"usage/video/#product-showcase-video","title":"Product Showcase Video","text":"<pre><code># Generate a product showcase\nproduct_video = client.videos(\n    prompt=(\n        \"A sleek smartphone rotating slowly on a pedestal, \"\n        \"highlighting its premium design and features, \"\n        \"soft studio lighting, professional product photography style\"\n    ),\n    model=\"google/veo-3.0-generate-001\",\n    aspect_ratio=\"9:16\",  # Vertical video for social media\n    duration=6,\n    generate_audio=False\n)\n</code></pre>"},{"location":"usage/video/#educational-animation","title":"Educational Animation","text":"<pre><code># Create an educational animation\neducational_video = client.videos(\n    prompt=(\n        \"Animated explanation of photosynthesis: show a green plant \"\n        \"absorbing sunlight, water molecules moving up through roots, \"\n        \"carbon dioxide entering leaves, oxygen and glucose being produced, \"\n        \"simple animated style suitable for children\"\n    ),\n    model=\"amazon/nova-reel\",\n    duration=6\n)\n</code></pre>"},{"location":"usage/video/#best-practices","title":"Best Practices","text":"<ol> <li>Be descriptive: Provide detailed, vivid descriptions of scenes, actions, camera movements, and atmosphere</li> <li>Consider timing: Shorter videos (4-6 seconds) often work better than longer ones</li> <li>Specify camera work: Include camera movement directions like \"slow pan\", \"dolly in\", \"tracking shot\"</li> <li>Think about audio: For models with audio generation, consider what sounds would enhance the scene</li> <li>Use appropriate aspect ratios: 16:9 for horizontal, 9:16 for vertical/social media</li> <li>Start simple: Begin with basic prompts and gradually add complexity</li> <li>Monitor progress: Video generation can take time - use job monitoring to track progress</li> </ol>"},{"location":"usage/video/#limitations-and-considerations","title":"Limitations and Considerations","text":""},{"location":"usage/video/#general-limitations","title":"General Limitations","text":"<ul> <li>Asynchronous processing: Video generation is not immediate - jobs can take minutes to hours</li> <li>Cost: Video generation is significantly more expensive than image generation</li> <li>File sizes: Generated videos can be large (5-50MB depending on duration and resolution)</li> <li>Content filtering: All providers have safety filters that may reject certain content</li> <li>Quality variation: Results can vary between providers and even between runs</li> </ul>"},{"location":"usage/video/#provider-specific-considerations","title":"Provider-Specific Considerations","text":""},{"location":"usage/video/#openai-sora","title":"OpenAI Sora","text":"<ul> <li>Limited to 4, 8, or 12 second videos</li> <li>Best at cinematic and realistic scenes</li> <li>Supports image-to-video input</li> </ul>"},{"location":"usage/video/#google-veo","title":"Google Veo","text":"<ul> <li>Excellent at smooth motion and realistic videos</li> <li>Supports audio generation with music and sound effects</li> <li>Advanced features like reference images and frame interpolation (Veo 3.1)</li> <li>Best for high-quality, cinematic content</li> </ul>"},{"location":"usage/video/#amazon-nova-reel_1","title":"Amazon Nova Reel","text":"<ul> <li>Most cost-effective option</li> <li>Fixed 6-second, 720p videos</li> <li>Good for simple animations and product showcases</li> <li>Fastest generation times</li> </ul>"},{"location":"usage/video/#troubleshooting","title":"Troubleshooting","text":""},{"location":"usage/video/#common-issues","title":"Common Issues","text":"<p>Job Stuck in \"queued\" or \"processing\": This is normal - video generation takes time. Continue checking the job status.</p> <p>Generation Failed: Check the error message in the job response. Common causes:</p> <ul> <li>Prompt violates safety filters</li> <li>Invalid parameters for the selected model</li> <li>Provider service issues</li> </ul> <p>Low Quality Results: Try:</p> <ul> <li>More detailed prompts</li> <li>Different models</li> <li>Adjusting parameters like duration or resolution</li> </ul> <p>Audio Not Generated: Ensure you're using a model that supports audio generation (Google Veo 3+) and have <code>generate_audio=True</code>.</p>"},{"location":"usage/video/#getting-help","title":"Getting Help","text":"<p>If you encounter issues:</p> <ol> <li>Check the job error message: <code>client.jobs(job_id)[\"error\"]</code></li> <li>Verify your parameters match the model's specifications</li> <li>Try a different model or simplify your prompt</li> <li>Contact support if the issue persists</li> </ol>"},{"location":"usage/vision/","title":"Vision &amp; Multimodal Capabilities","text":"<p>IndoxRouter supports vision-capable models that can understand and analyze images alongside text. This guide covers how to send images to AI models and work with multimodal content.</p>"},{"location":"usage/vision/#supported-models","title":"Supported Models","text":"<p>Many models across different providers support image inputs. Here are some popular vision-capable models:</p>"},{"location":"usage/vision/#openai","title":"OpenAI","text":"<ul> <li><code>gpt-4o</code> - Most capable multimodal model</li> <li><code>gpt-4o-mini</code> - Fast and cost-effective vision model</li> <li><code>gpt-4.1</code> - Latest GPT-4 with vision</li> <li><code>gpt-4.1-mini</code> - Efficient GPT-4.1 with vision</li> <li><code>chatgpt-4o</code> - ChatGPT optimized vision model</li> <li><code>o1</code> - Advanced reasoning with vision</li> </ul>"},{"location":"usage/vision/#anthropic","title":"Anthropic","text":"<ul> <li><code>claude-sonnet-4.5</code> - Sonnet 4.5 with vision</li> <li><code>claude-sonnet-4</code> - Sonnet 4 with vision</li> <li><code>claude-opus-4.1</code> - Most capable Claude with vision</li> <li><code>claude-3-opus-20240229</code> - Claude 3 Opus</li> <li><code>claude-3-sonnet-20240229</code> - Claude 3 Sonnet</li> <li><code>claude-3-5-sonnet-20241022</code> - Claude 3.5 Sonnet</li> <li><code>claude-3-haiku-20240307</code> - Fast and efficient with vision</li> </ul>"},{"location":"usage/vision/#google","title":"Google","text":"<ul> <li><code>gemini-2.5-flash-preview</code> - Latest Gemini Flash</li> <li><code>gemini-2.5-pro-preview</code> - Latest Gemini Pro</li> <li><code>gemini-2.0-flash</code> - Gemini 2.0 Flash</li> <li><code>gemini-1.5-pro</code> - Gemini 1.5 Pro</li> <li><code>gemini-1.5-flash</code> - Fast Gemini with vision</li> <li><code>gemma-3-27b</code> - Open Gemma model with vision</li> </ul>"},{"location":"usage/vision/#amazon-bedrock","title":"Amazon Bedrock","text":"<ul> <li><code>amazon.nova-lite-v1:0</code> - Nova Lite with vision</li> <li><code>amazon.nova-pro-v1:0</code> - Nova Pro with vision</li> </ul>"},{"location":"usage/vision/#meta","title":"Meta","text":"<ul> <li><code>meta.llama3-2-11b-instruct-v1:0</code> - Llama 3.2 11B Vision</li> <li><code>meta.llama3-2-90b-instruct-v1:0</code> - Llama 3.2 90B Vision</li> </ul>"},{"location":"usage/vision/#image-input-format","title":"Image Input Format","text":"<p>IndoxRouter uses a standardized format for multimodal content. Images are sent as part of the message content array.</p>"},{"location":"usage/vision/#basic-image-with-text","title":"Basic Image with Text","text":"<pre><code>from indoxrouter import Client\nimport base64\n\n# Initialize client\nclient = Client(api_key=\"your_api_key\")\n\n# Read and encode image\nwith open(\"image.jpg\", \"rb\") as f:\n    image_base64 = base64.b64encode(f.read()).decode('utf-8')\n\n# Send request with image\nresponse = client.chat(\n    messages=[\n        {\n            \"role\": \"user\",\n            \"content\": [\n                {\n                    \"type\": \"text\",\n                    \"text\": \"What's in this image?\"\n                },\n                {\n                    \"type\": \"image\",\n                    \"image\": {\n                        \"data\": image_base64,\n                        \"media_type\": \"image/jpeg\"\n                    }\n                }\n            ]\n        }\n    ],\n    model=\"openai/gpt-4o\"\n)\n\nprint(response[\"data\"])\n</code></pre>"},{"location":"usage/vision/#image-from-url","title":"Image from URL","text":"<p>You can also send images via URL (supported by most providers):</p> <pre><code>response = client.chat(\n    messages=[\n        {\n            \"role\": \"user\",\n            \"content\": [\n                {\n                    \"type\": \"text\",\n                    \"text\": \"Describe this image\"\n                },\n                {\n                    \"type\": \"image\",\n                    \"image\": {\n                        \"url\": \"https://example.com/image.jpg\"\n                    }\n                }\n            ]\n        }\n    ],\n    model=\"anthropic/claude-sonnet-4.5\"\n)\n</code></pre>"},{"location":"usage/vision/#multiple-images","title":"Multiple Images","text":"<p>You can include multiple images in a single request:</p> <pre><code>import base64\n\n# Read multiple images\nwith open(\"image1.jpg\", \"rb\") as f:\n    image1_base64 = base64.b64encode(f.read()).decode('utf-8')\n\nwith open(\"image2.jpg\", \"rb\") as f:\n    image2_base64 = base64.b64encode(f.read()).decode('utf-8')\n\nresponse = client.chat(\n    messages=[\n        {\n            \"role\": \"user\",\n            \"content\": [\n                {\n                    \"type\": \"text\",\n                    \"text\": \"Compare these two images. What are the main differences?\"\n                },\n                {\n                    \"type\": \"image\",\n                    \"image\": {\n                        \"data\": image1_base64,\n                        \"media_type\": \"image/jpeg\"\n                    }\n                },\n                {\n                    \"type\": \"image\",\n                    \"image\": {\n                        \"data\": image2_base64,\n                        \"media_type\": \"image/jpeg\"\n                    }\n                }\n            ]\n        }\n    ],\n    model=\"google/gemini-2.5-pro-preview\"\n)\n\nprint(response[\"data\"])\n</code></pre>"},{"location":"usage/vision/#supported-image-formats","title":"Supported Image Formats","text":"<p>The following image formats are supported:</p> <ul> <li>JPEG (<code>image/jpeg</code>)</li> <li>PNG (<code>image/png</code>)</li> <li>WebP (<code>image/webp</code>)</li> <li>GIF (<code>image/gif</code>) - Some models only</li> </ul> <p>!!! note \"Image Format Recommendations\" - Use JPEG for photographs and complex images - Use PNG for screenshots, diagrams, and images with text - Use WebP for smaller file sizes with good quality - Convert images to JPEG or PNG for best compatibility</p>"},{"location":"usage/vision/#image-size-limits","title":"Image Size Limits","text":"<p>IndoxRouter enforces the following limits to ensure optimal performance:</p> <ul> <li>Maximum single image size: 20MB (raw bytes)</li> <li>Maximum total images per request: 100MB (all images combined)</li> </ul> <p>Size Limit Enforcement</p> <p>If your images exceed these limits, you'll receive a <code>400 Bad Request</code> error. Compress or resize images before sending.</p>"},{"location":"usage/vision/#image-optimization-tips","title":"Image Optimization Tips","text":"<pre><code>from PIL import Image\nimport io\nimport base64\n\ndef optimize_image(image_path, max_size_mb=2):\n    \"\"\"\n    Optimize an image to reduce size while maintaining quality.\n\n    Args:\n        image_path: Path to the image file\n        max_size_mb: Maximum size in megabytes (default: 2MB)\n\n    Returns:\n        Base64 encoded optimized image\n    \"\"\"\n    img = Image.open(image_path)\n\n    # Convert to RGB if necessary\n    if img.mode in ('RGBA', 'LA', 'P'):\n        img = img.convert('RGB')\n\n    # Resize if image is very large\n    max_dimension = 2048\n    if max(img.size) &gt; max_dimension:\n        ratio = max_dimension / max(img.size)\n        new_size = tuple(int(dim * ratio) for dim in img.size)\n        img = img.resize(new_size, Image.Resampling.LANCZOS)\n\n    # Save with optimization\n    buffer = io.BytesIO()\n    quality = 85\n\n    while True:\n        buffer.seek(0)\n        buffer.truncate()\n        img.save(buffer, format='JPEG', quality=quality, optimize=True)\n        size_mb = buffer.tell() / (1024 * 1024)\n\n        if size_mb &lt;= max_size_mb or quality &lt;= 20:\n            break\n        quality -= 5\n\n    # Encode to base64\n    buffer.seek(0)\n    return base64.b64encode(buffer.read()).decode('utf-8')\n\n# Use the optimized image\noptimized_image = optimize_image(\"large_image.jpg\", max_size_mb=2)\n\nresponse = client.chat(\n    messages=[\n        {\n            \"role\": \"user\",\n            \"content\": [\n                {\"type\": \"text\", \"text\": \"What's in this image?\"},\n                {\n                    \"type\": \"image\",\n                    \"image\": {\n                        \"data\": optimized_image,\n                        \"media_type\": \"image/jpeg\"\n                    }\n                }\n            ]\n        }\n    ],\n    model=\"openai/gpt-4o-mini\"\n)\n</code></pre>"},{"location":"usage/vision/#multi-turn-conversations-with-images","title":"Multi-Turn Conversations with Images","text":"<p>You can maintain conversations with images across multiple turns:</p> <pre><code>import base64\n\n# First turn with image\nwith open(\"diagram.png\", \"rb\") as f:\n    image_base64 = base64.b64encode(f.read()).decode('utf-8')\n\nmessages = [\n    {\n        \"role\": \"user\",\n        \"content\": [\n            {\"type\": \"text\", \"text\": \"What does this diagram show?\"},\n            {\n                \"type\": \"image\",\n                \"image\": {\n                    \"data\": image_base64,\n                    \"media_type\": \"image/png\"\n                }\n            }\n        ]\n    }\n]\n\nresponse = client.chat(messages=messages, model=\"anthropic/claude-opus-4.1\")\nassistant_message = response[\"choices\"][0][\"message\"][\"content\"]\nprint(f\"Assistant: {assistant_message}\")\n\n# Add assistant response to conversation\nmessages.append({\n    \"role\": \"assistant\",\n    \"content\": assistant_message\n})\n\n# Follow-up question (no need to resend the image)\nmessages.append({\n    \"role\": \"user\",\n    \"content\": \"Can you explain the flow in more detail?\"\n})\n\nresponse = client.chat(messages=messages, model=\"anthropic/claude-opus-4.1\")\nprint(f\"Assistant: {response['choices'][0]['message']['content']}\")\n</code></pre> <p>Image Memory</p> <p>Once an image is sent in a conversation, the model remembers it for subsequent turns. You don't need to resend the image unless you want to reference a different image.</p>"},{"location":"usage/vision/#common-use-cases","title":"Common Use Cases","text":""},{"location":"usage/vision/#1-document-analysis","title":"1. Document Analysis","text":"<pre><code>import base64\n\nwith open(\"invoice.pdf.jpg\", \"rb\") as f:\n    invoice_image = base64.b64encode(f.read()).decode('utf-8')\n\nresponse = client.chat(\n    messages=[\n        {\n            \"role\": \"user\",\n            \"content\": [\n                {\n                    \"type\": \"text\",\n                    \"text\": \"Extract all the line items, quantities, and prices from this invoice.\"\n                },\n                {\n                    \"type\": \"image\",\n                    \"image\": {\n                        \"data\": invoice_image,\n                        \"media_type\": \"image/jpeg\"\n                    }\n                }\n            ]\n        }\n    ],\n    model=\"openai/gpt-4o\"\n)\n\nprint(response[\"data\"])\n</code></pre>"},{"location":"usage/vision/#2-image-description-for-accessibility","title":"2. Image Description for Accessibility","text":"<pre><code>import base64\n\nwith open(\"photo.jpg\", \"rb\") as f:\n    photo_base64 = base64.b64encode(f.read()).decode('utf-8')\n\nresponse = client.chat(\n    messages=[\n        {\n            \"role\": \"system\",\n            \"content\": \"You are an accessibility assistant. Provide detailed, accurate descriptions of images for visually impaired users.\"\n        },\n        {\n            \"role\": \"user\",\n            \"content\": [\n                {\n                    \"type\": \"text\",\n                    \"text\": \"Please provide a detailed description of this image for a screen reader.\"\n                },\n                {\n                    \"type\": \"image\",\n                    \"image\": {\n                        \"data\": photo_base64,\n                        \"media_type\": \"image/jpeg\"\n                    }\n                }\n            ]\n        }\n    ],\n    model=\"google/gemini-2.0-flash\"\n)\n\nprint(response[\"data\"])\n</code></pre>"},{"location":"usage/vision/#3-chart-and-graph-analysis","title":"3. Chart and Graph Analysis","text":"<pre><code>import base64\n\nwith open(\"sales_chart.png\", \"rb\") as f:\n    chart_image = base64.b64encode(f.read()).decode('utf-8')\n\nresponse = client.chat(\n    messages=[\n        {\n            \"role\": \"user\",\n            \"content\": [\n                {\n                    \"type\": \"text\",\n                    \"text\": \"Analyze this sales chart and provide insights about trends, peaks, and anomalies.\"\n                },\n                {\n                    \"type\": \"image\",\n                    \"image\": {\n                        \"data\": chart_image,\n                        \"media_type\": \"image/png\"\n                    }\n                }\n            ]\n        }\n    ],\n    model=\"anthropic/claude-sonnet-4.5\"\n)\n\nprint(response[\"data\"])\n</code></pre>"},{"location":"usage/vision/#4-code-screenshot-analysis","title":"4. Code Screenshot Analysis","text":"<pre><code>import base64\n\nwith open(\"code_screenshot.png\", \"rb\") as f:\n    code_image = base64.b64encode(f.read()).decode('utf-8')\n\nresponse = client.chat(\n    messages=[\n        {\n            \"role\": \"user\",\n            \"content\": [\n                {\n                    \"type\": \"text\",\n                    \"text\": \"Review this code and suggest improvements. Identify any bugs or security issues.\"\n                },\n                {\n                    \"type\": \"image\",\n                    \"image\": {\n                        \"data\": code_image,\n                        \"media_type\": \"image/png\"\n                    }\n                }\n            ]\n        }\n    ],\n    model=\"openai/gpt-4o\"\n)\n\nprint(response[\"data\"])\n</code></pre>"},{"location":"usage/vision/#error-handling","title":"Error Handling","text":""},{"location":"usage/vision/#model-doesnt-support-images","title":"Model Doesn't Support Images","text":"<p>If you try to send an image to a text-only model, you'll receive an error:</p> <pre><code>try:\n    response = client.chat(\n        messages=[\n            {\n                \"role\": \"user\",\n                \"content\": [\n                    {\"type\": \"text\", \"text\": \"What's in this image?\"},\n                    {\n                        \"type\": \"image\",\n                        \"image\": {\"data\": image_base64, \"media_type\": \"image/jpeg\"}\n                    }\n                ]\n            }\n        ],\n        model=\"openai/gpt-3.5-turbo\"  # Text-only model\n    )\nexcept Exception as e:\n    print(f\"Error: {e}\")\n    # Error: Model 'gpt-3.5-turbo' from provider 'openai' does not support image inputs.\n    # Supported input modalities: text.\n    # Please use a vision-capable model like 'gpt-4o', 'claude-3-sonnet-20240229', or 'gemini-1.5-pro'.\n</code></pre>"},{"location":"usage/vision/#image-too-large","title":"Image Too Large","text":"<pre><code>try:\n    # Very large image\n    response = client.chat(\n        messages=[\n            {\n                \"role\": \"user\",\n                \"content\": [\n                    {\"type\": \"text\", \"text\": \"Analyze this image\"},\n                    {\n                        \"type\": \"image\",\n                        \"image\": {\"data\": huge_image_base64, \"media_type\": \"image/jpeg\"}\n                    }\n                ]\n            }\n        ],\n        model=\"openai/gpt-4o\"\n    )\nexcept Exception as e:\n    print(f\"Error: {e}\")\n    # Error: Image size exceeds maximum allowed size of 20MB.\n    # Please reduce the image size or resolution.\n</code></pre>"},{"location":"usage/vision/#best-practices","title":"Best Practices","text":""},{"location":"usage/vision/#1-choose-the-right-model","title":"1. Choose the Right Model","text":"<p>Different models have different strengths:</p> <ul> <li>GPT-4o: Best for general-purpose vision tasks, excellent text extraction</li> <li>Claude Opus 4.1: Excellent for detailed analysis and reasoning about images</li> <li>Gemini 2.5 Pro: Strong at multi-image comparison and video frames</li> <li>GPT-4o-mini: Fast and cost-effective for simple vision tasks</li> </ul>"},{"location":"usage/vision/#2-optimize-images","title":"2. Optimize Images","text":"<ul> <li>Resize large images to reasonable dimensions (e.g., 2048x2048 max)</li> <li>Compress images to reduce file size</li> <li>Use JPEG for photos, PNG for diagrams/screenshots</li> <li>Remove unnecessary metadata</li> </ul>"},{"location":"usage/vision/#3-provide-clear-instructions","title":"3. Provide Clear Instructions","text":"<p>Be specific about what you want the model to analyze:</p> <pre><code># \u274c Vague\n{\"type\": \"text\", \"text\": \"What is this?\"}\n\n# \u2705 Specific\n{\"type\": \"text\", \"text\": \"Identify all the people in this image and describe what they're doing.\"}\n</code></pre>"},{"location":"usage/vision/#4-use-system-messages","title":"4. Use System Messages","text":"<p>Guide the model's behavior with system messages:</p> <pre><code>messages = [\n    {\n        \"role\": \"system\",\n        \"content\": \"You are an expert medical image analyst. Provide detailed, technical descriptions.\"\n    },\n    {\n        \"role\": \"user\",\n        \"content\": [\n            {\"type\": \"text\", \"text\": \"Analyze this X-ray image.\"},\n            {\"type\": \"image\", \"image\": {\"data\": xray_base64, \"media_type\": \"image/jpeg\"}}\n        ]\n    }\n]\n</code></pre>"},{"location":"usage/vision/#5-handle-errors-gracefully","title":"5. Handle Errors Gracefully","text":"<p>Always wrap vision API calls in try-except blocks:</p> <pre><code>try:\n    response = client.chat(\n        messages=[...],\n        model=\"openai/gpt-4o\"\n    )\n    print(response[\"data\"])\nexcept Exception as e:\n    print(f\"Failed to analyze image: {e}\")\n    # Implement fallback or retry logic\n</code></pre>"},{"location":"usage/vision/#pricing-considerations","title":"Pricing Considerations","text":"<p>Vision models typically cost more than text-only models due to image processing:</p> <ul> <li>Images are tokenized and counted toward your usage</li> <li>Larger images use more tokens</li> <li>Multiple images increase costs proportionally</li> </ul> <p>!!! tip \"Cost Optimization\" - Use smaller images when possible - Choose cost-effective models like <code>gpt-4o-mini</code> or <code>gemini-1.5-flash</code> for simple tasks - Use BYOK (Bring Your Own Key) for direct provider pricing</p>"},{"location":"usage/vision/#byok-with-vision-models","title":"BYOK with Vision Models","text":"<p>You can use your own API keys with vision models:</p> <pre><code>response = client.chat(\n    messages=[\n        {\n            \"role\": \"user\",\n            \"content\": [\n                {\"type\": \"text\", \"text\": \"What's in this image?\"},\n                {\"type\": \"image\", \"image\": {\"data\": image_base64, \"media_type\": \"image/jpeg\"}}\n            ]\n        }\n    ],\n    model=\"openai/gpt-4o\",\n    byok_api_key=\"sk-your-openai-key\"\n)\n</code></pre>"},{"location":"usage/vision/#streaming-with-images","title":"Streaming with Images","text":"<p>Vision models support streaming just like text-only models:</p> <pre><code>import base64\n\nwith open(\"image.jpg\", \"rb\") as f:\n    image_base64 = base64.b64encode(f.read()).decode('utf-8')\n\nfor chunk in client.chat(\n    messages=[\n        {\n            \"role\": \"user\",\n            \"content\": [\n                {\"type\": \"text\", \"text\": \"Describe this image in detail.\"},\n                {\"type\": \"image\", \"image\": {\"data\": image_base64, \"media_type\": \"image/jpeg\"}}\n            ]\n        }\n    ],\n    model=\"anthropic/claude-sonnet-4.5\",\n    stream=True\n):\n    if isinstance(chunk, dict) and \"data\" in chunk:\n        print(chunk[\"data\"], end=\"\", flush=True)\n</code></pre>"},{"location":"usage/vision/#limitations","title":"Limitations","text":""},{"location":"usage/vision/#per-provider-limitations","title":"Per-Provider Limitations","text":"<p>Different providers have different limitations:</p> <ul> <li>OpenAI: Up to 20 images per request (depending on model)</li> <li>Anthropic: Up to 20 images per request</li> <li>Google: Varies by model, generally supports multiple images</li> <li>Image size: Most providers limit images to ~20MB per image</li> </ul>"},{"location":"usage/vision/#quality-considerations","title":"Quality Considerations","text":"<ul> <li>Very small images may not be analyzed accurately</li> <li>Blurry or low-quality images may produce less accurate results</li> <li>Some models have difficulty with:</li> <li>Very small text in images</li> <li>Complex diagrams with many elements</li> <li>Highly compressed or artifacted images</li> </ul>"},{"location":"usage/vision/#next-steps","title":"Next Steps","text":"<ul> <li>Explore Chat Completions for general chat functionality</li> <li>Learn about BYOK Support to use your own API keys</li> <li>Check Rate Limits for usage constraints</li> <li>See Basic Examples for more code samples</li> </ul>"},{"location":"use-cases/chatbots/","title":"Building Chatbots with IndoxRouter","text":"<p>This guide shows how to build a simple but effective chatbot using the IndoxRouter client. By following these examples, you can create chatbots that leverage different AI models through a consistent interface.</p>"},{"location":"use-cases/chatbots/#basic-chatbot","title":"Basic Chatbot","text":"<p>Here's a simple example of a command-line chatbot:</p> <pre><code>from indoxrouter import Client\n\ndef simple_chatbot():\n    \"\"\"A simple command-line chatbot using IndoxRouter.\"\"\"\n\n    print(\"Welcome to IndoxRouter Chatbot!\")\n    print(\"Type 'exit' to end the conversation.\\n\")\n\n    # Initialize the client\n    with Client(api_key=\"your_api_key\") as client:\n        # Set up the conversation with a system message\n        messages = [\n            {\"role\": \"system\", \"content\": \"You are a helpful assistant that provides concise answers.\"}\n        ]\n\n        while True:\n            # Get user input\n            user_input = input(\"You: \")\n\n            # Check if user wants to exit\n            if user_input.lower() in (\"exit\", \"quit\", \"bye\"):\n                print(\"Assistant: Goodbye!\")\n                break\n\n            # Add user message to conversation\n            messages.append({\"role\": \"user\", \"content\": user_input})\n\n            try:\n                # Get response from the model\n                response = client.chat(\n                    messages=messages,\n                    model=\"openai/gpt-4o-mini\",\n                    temperature=0.7\n                )\n\n                # Extract and print the assistant's response\n                assistant_response = response[\"data\"]\n                print(f\"Assistant: {assistant_response}\")\n\n                # Add assistant response to conversation history\n                messages.append({\"role\": \"assistant\", \"content\": assistant_response})\n\n            except Exception as e:\n                print(f\"Error: {str(e)}\")\n                messages.pop()\n\nif __name__ == \"__main__\":\n    simple_chatbot()\n</code></pre>"},{"location":"use-cases/chatbots/#multi-provider-chatbot","title":"Multi-Provider Chatbot","text":"<p>One of the key benefits of IndoxRouter is the ability to use multiple AI providers through a consistent interface. Here's an example of a chatbot that can switch between different providers:</p> <pre><code>from indoxrouter import Client\nimport argparse\n\ndef multi_provider_chatbot():\n    \"\"\"A chatbot that can use different AI providers.\"\"\"\n\n    # Parse command line arguments\n    parser = argparse.ArgumentParser(description=\"Multi-provider chatbot\")\n    parser.add_argument(\"--provider\", type=str, default=\"openai\", help=\"Provider to use (openai, anthropic, google, mistral)\")\n    parser.add_argument(\"--model\", type=str, help=\"Specific model to use\")\n    args = parser.parse_args()\n\n    # Set up provider and model\n    provider = args.provider.lower()\n\n    # Default models for each provider\n    provider_models = {\n        \"openai\": \"gpt-4o-mini\",\n        \"anthropic\": \"claude-3-haiku-20240307\",\n        \"google\": \"gemini-1.5-pro\",\n        \"mistral\": \"mistral-small-latest\",\n        \"deepseek\": \"deepseek-chat\"\n    }\n\n    if args.model:\n        model = args.model\n    elif provider in provider_models:\n        model = provider_models[provider]\n    else:\n        print(f\"Unknown provider: {provider}. Using OpenAI as default.\")\n        provider = \"openai\"\n        model = provider_models[provider]\n\n    full_model = f\"{provider}/{model}\"\n    print(f\"Welcome to IndoxRouter Multi-Provider Chatbot!\")\n    print(f\"Using model: {full_model}\")\n    print(\"Type 'exit' to end the conversation.\")\n    print(\"Type 'switch provider model' to change the AI model.\\n\")\n\n    # Initialize the client\n    with Client(api_key=\"your_api_key\") as client:\n        # Set up the conversation with a system message\n        messages = [\n            {\"role\": \"system\", \"content\": \"You are a helpful assistant that provides concise answers.\"}\n        ]\n\n        while True:\n            # Get user input\n            user_input = input(\"You: \")\n\n            # Check if user wants to exit\n            if user_input.lower() in (\"exit\", \"quit\", \"bye\"):\n                print(\"Assistant: Goodbye!\")\n                break\n\n            # Check if user wants to switch model\n            if user_input.lower().startswith(\"switch \"):\n                try:\n                    _, new_provider, new_model = user_input.split()\n                    full_model = f\"{new_provider}/{new_model}\"\n                    print(f\"Switching to {full_model}\")\n\n                    # Add system message explaining the switch\n                    messages.append({\"role\": \"system\", \"content\": f\"The conversation will now continue using {full_model}.\"})\n                    continue\n                except ValueError:\n                    print(\"Invalid format. Use 'switch provider model'\")\n                    continue\n\n            # Add user message to conversation\n            messages.append({\"role\": \"user\", \"content\": user_input})\n\n            try:\n                # Get response from the model\n                response = client.chat(\n                    messages=messages,\n                    model=full_model,\n                    temperature=0.7\n                )\n\n                # Extract and print the assistant's response\n                assistant_response = response[\"data\"]\n                print(f\"Assistant: {assistant_response}\")\n\n                # Add assistant response to conversation history\n                messages.append({\"role\": \"assistant\", \"content\": assistant_response})\n\n            except Exception as e:\n                print(f\"Error: {str(e)}\")\n                messages.pop()  # Remove the user message from history\n\nif __name__ == \"__main__\":\n    multi_provider_chatbot()\n</code></pre>"},{"location":"use-cases/chatbots/#web-based-chatbot-with-streamlit","title":"Web-Based Chatbot with Streamlit","text":"<p>You can also create a simple web-based chatbot using Streamlit:</p> <pre><code># Save as chatbot_app.py\nimport streamlit as st\nfrom indoxrouter import Client, ModelNotFoundError, ProviderError\n\n# Set page title and configure\nst.set_page_config(page_title=\"IndoxRouter Chatbot\", page_icon=\"\ud83d\udcac\")\nst.title(\"IndoxRouter Chatbot\")\n\n# Initialize session state for conversation history\nif \"messages\" not in st.session_state:\n    st.session_state.messages = [\n        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"}\n    ]\n\nif \"current_model\" not in st.session_state:\n    st.session_state.current_model = \"openai/gpt-4o-mini\"\n\n# API key input\napi_key = st.sidebar.text_input(\"API Key\", type=\"password\")\n\n# Model selection\nprovider_options = [\"openai\", \"anthropic\", \"google\", \"mistral\", \"deepseek\"]\nselected_provider = st.sidebar.selectbox(\"Provider\", provider_options)\n\nmodel_options = {\n    \"openai\": [\"gpt-4o-mini\", \"gpt-4o\", \"gpt-3.5-turbo\"],\n    \"anthropic\": [\"claude-3-haiku-20240307\", \"claude-3-sonnet-20240229\", \"claude-3-opus-20240229\"],\n    \"google\": [\"gemini-1.5-pro\", \"gemini-1.5-flash\"],\n    \"mistral\": [\"mistral-small-latest\", \"mistral-medium-latest\", \"mistral-large-latest\"],\n    \"deepseek\": [\"deepseek-chat\", \"deepseek-coder\"]\n}\n\nselected_model = st.sidebar.selectbox(\"Model\", model_options[selected_provider])\nst.session_state.current_model = f\"{selected_provider}/{selected_model}\"\n\n# Temperature slider\ntemperature = st.sidebar.slider(\"Temperature\", 0.0, 1.0, 0.7, 0.1)\n\n# Display conversation history\nfor message in st.session_state.messages:\n    if message[\"role\"] != \"system\":\n        with st.chat_message(message[\"role\"]):\n            st.write(message[\"content\"])\n\n# User input\nuser_input = st.chat_input(\"Type your message here...\")\n\nif user_input and api_key:\n    # Add user message to conversation\n    st.session_state.messages.append({\"role\": \"user\", \"content\": user_input})\n\n    # Display user message\n    with st.chat_message(\"user\"):\n        st.write(user_input)\n\n    # Get assistant response\n    with st.chat_message(\"assistant\"):\n        message_placeholder = st.empty()\n        full_response = \"\"\n\n        try:\n            # Initialize client\n            with Client(api_key=api_key) as client:\n                # Stream the response\n                for chunk in client.chat(\n                    messages=st.session_state.messages,\n                    model=st.session_state.current_model,\n                    temperature=temperature,\n                    stream=True\n                ):\n                    if isinstance(chunk, dict) and \"data\" in chunk:\n                        content = chunk[\"data\"]\n                        full_response += content\n                        message_placeholder.write(full_response + \"\u258c\")\n\n                message_placeholder.write(full_response)\n\n            # Add assistant response to conversation\n            st.session_state.messages.append({\"role\": \"assistant\", \"content\": full_response})\n\n        except ModelNotFoundError as e:\n            st.error(f\"Model not found: {e}\")\n        except ProviderError as e:\n            st.error(f\"Provider error: {e}\")\n        except Exception as e:\n            st.error(f\"An error occurred: {str(e)}\")\n\nelif user_input and not api_key:\n    st.warning(\"Please enter your API key in the sidebar.\")\n\n# Run with: streamlit run chatbot_app.py\n</code></pre> <p>To run the Streamlit app, install Streamlit and run:</p> <pre><code>pip install streamlit\nstreamlit run chatbot_app.py\n</code></pre>"},{"location":"use-cases/chatbots/#best-practices-for-chatbots","title":"Best Practices for Chatbots","text":"<ol> <li>Maintain Conversation Context: Keep track of conversation history to provide contextual responses</li> <li>Set Clear System Instructions: Use system messages to define the persona and behavior of your chatbot</li> <li>Handle Errors Gracefully: Implement proper error handling to ensure a smooth user experience</li> <li>Optimize Token Usage: Be mindful of the conversation length to avoid exceeding token limits</li> <li>Implement Rate Limiting: Add rate limiting to prevent abuse and manage costs</li> <li>Consider Privacy: Be transparent about data usage and implement appropriate data retention policies</li> <li>Test Different Models: Experiment with different models to find the best balance of quality and cost</li> <li>Implement Fallbacks: Have fallback mechanisms when a provider is unavailable or returns errors</li> </ol>"},{"location":"use-cases/chatbots/#next-steps","title":"Next Steps","text":"<p>To further enhance your chatbot, consider:</p> <ul> <li>Implementing memory management for long conversations</li> <li>Adding message persistence using a database</li> <li>Implementing functions/tools for more interactive capabilities</li> <li>Creating a feedback mechanism to improve chatbot responses</li> <li>Fine-tuning models for specific use cases</li> </ul>"},{"location":"use-cases/content-generation/","title":"Content Generation with IndoxRouter","text":"<p>IndoxRouter provides access to powerful language models that can be used for a wide range of content generation tasks. This guide demonstrates various content generation use cases and how to implement them.</p>"},{"location":"use-cases/content-generation/#text-generation-basics","title":"Text Generation Basics","text":"<p>At its core, content generation involves prompting a language model to produce text that meets specific requirements. Here's a simple example:</p> <pre><code>from indoxrouter import Client\n\nclient = Client(api_key=\"your_api_key\")\n\n# Generate a short blog post\nresponse = client.chat(\n    messages=[\n        {\"role\": \"system\", \"content\": \"You are a professional blog writer with expertise in technology.\"},\n        {\"role\": \"user\", \"content\": \"Write a 300-word blog post about the impact of AI on content creation.\"}\n    ],\n    model=\"openai/gpt-4o\"\n)\n\nblog_post = response[\"data\"]\nprint(blog_post)\n</code></pre>"},{"location":"use-cases/content-generation/#creative-writing","title":"Creative Writing","text":"<p>Language models can generate creative content like stories, poems, and scripts:</p> <pre><code># Generate a short story\nstory_response = client.chat(\n    messages=[\n        {\"role\": \"system\", \"content\": \"You are a creative fiction writer with a talent for engaging narratives.\"},\n        {\"role\": \"user\", \"content\": (\n            \"Write a short story (around 500 words) about a scientist who discovers \"\n            \"a way to communicate with plants. The story should have a surprising twist ending.\"\n        )}\n    ],\n    model=\"anthropic/claude-3-opus-20240229\",\n    temperature=0.8  # Higher temperature for more creativity\n)\n\nstory = story_response[\"data\"]\n</code></pre>"},{"location":"use-cases/content-generation/#seo-content-creation","title":"SEO Content Creation","text":"<p>Generate SEO-optimized content for websites and marketing:</p> <pre><code># Generate SEO-optimized article\nseo_response = client.chat(\n    messages=[\n        {\"role\": \"system\", \"content\": (\n            \"You are an SEO content expert who writes engaging, informative content \"\n            \"that ranks well in search engines. Include appropriate headings (H2, H3), \"\n            \"bullet points where relevant, and a conclusion.\"\n        )},\n        {\"role\": \"user\", \"content\": (\n            \"Write a comprehensive SEO article (800-1000 words) about 'Best Practices for Machine Learning Deployment' \"\n            \"targeting developers and data scientists. Include these keywords naturally: machine learning operations, \"\n            \"MLOps, model monitoring, deployment pipeline, and containerization.\"\n        )}\n    ],\n    model=\"openai/gpt-4o\",\n    temperature=0.7\n)\n\nseo_article = seo_response[\"data\"]\n</code></pre>"},{"location":"use-cases/content-generation/#product-descriptions","title":"Product Descriptions","text":"<p>Create compelling product descriptions for e-commerce:</p> <pre><code>def generate_product_description(product_name, features, target_audience, word_count=200):\n    \"\"\"Generate a product description based on product details.\"\"\"\n    features_text = \"\\n\".join([f\"- {feature}\" for feature in features])\n\n    prompt = f\"\"\"\n    Product: {product_name}\n    Features:\n    {features_text}\n    Target Audience: {target_audience}\n    Word Count: {word_count}\n\n    Write a compelling product description that highlights the features and benefits.\n    \"\"\"\n\n    response = client.chat(\n        messages=[\n            {\"role\": \"system\", \"content\": \"You are a marketing copywriter who creates compelling product descriptions.\"},\n            {\"role\": \"user\", \"content\": prompt}\n        ],\n        model=\"mistral/mistral-large-latest\",\n        temperature=0.7\n    )\n\n    return response[\"data\"]\n\n# Example usage\nproduct_description = generate_product_description(\n    product_name=\"EcoCharge Solar Power Bank\",\n    features=[\n        \"25,000mAh capacity\",\n        \"Solar charging capability\",\n        \"Fast-charging USB-C port\",\n        \"Built-in LED flashlight\",\n        \"Waterproof (IP67 rating)\"\n    ],\n    target_audience=\"Outdoor enthusiasts and travelers\",\n    word_count=150\n)\n</code></pre>"},{"location":"use-cases/content-generation/#email-marketing-campaigns","title":"Email Marketing Campaigns","text":"<p>Generate email marketing content with different styles:</p> <pre><code>def generate_email_campaign(campaign_type, product_info, audience, call_to_action):\n    \"\"\"Generate email marketing content based on campaign type.\"\"\"\n\n    campaign_prompts = {\n        \"welcome\": \"Write a friendly welcome email for new subscribers.\",\n        \"promotional\": \"Write a promotional email announcing a new product or special offer.\",\n        \"newsletter\": \"Write a newsletter email with updates and valuable content.\",\n        \"abandonment\": \"Write a cart abandonment email to remind customers of items left in their cart.\",\n        \"follow_up\": \"Write a follow-up email after a purchase to thank the customer and suggest next steps.\"\n    }\n\n    if campaign_type not in campaign_prompts:\n        raise ValueError(f\"Campaign type must be one of: {', '.join(campaign_prompts.keys())}\")\n\n    prompt = f\"\"\"\n    Campaign Type: {campaign_type} email\n    Product/Service Information: {product_info}\n    Target Audience: {audience}\n    Call to Action: {call_to_action}\n\n    {campaign_prompts[campaign_type]}\n    Include a subject line, greeting, body, and sign-off.\n    \"\"\"\n\n    response = client.chat(\n        messages=[\n            {\"role\": \"system\", \"content\": \"You are an email marketing specialist who writes compelling emails that convert.\"},\n            {\"role\": \"user\", \"content\": prompt}\n        ],\n        model=\"openai/gpt-4o-mini\",\n        temperature=0.7\n    )\n\n    return response[\"data\"]\n\n# Example usage\nwelcome_email = generate_email_campaign(\n    campaign_type=\"welcome\",\n    product_info=\"TechNest - A productivity app for managing tasks, notes, and projects\",\n    audience=\"New app subscribers, primarily professionals aged 25-45\",\n    call_to_action=\"Download the app and complete the onboarding tour\"\n)\n</code></pre>"},{"location":"use-cases/content-generation/#social-media-content","title":"Social Media Content","text":"<p>Generate content for different social media platforms:</p> <pre><code>def generate_social_media_post(platform, topic, tone, hashtags=None, include_emoji=True):\n    \"\"\"Generate a social media post tailored to a specific platform.\"\"\"\n\n    platform_guidelines = {\n        \"twitter\": \"Write a concise post under 280 characters.\",\n        \"instagram\": \"Write an engaging caption that works well with a visual. Include line breaks for readability.\",\n        \"linkedin\": \"Write a professional post that provides value to a business audience. Can be longer format.\",\n        \"facebook\": \"Write a conversational post that encourages engagement and interaction.\",\n        \"tiktok\": \"Write a catchy, trend-aware caption that would work well with a short video.\"\n    }\n\n    if platform not in platform_guidelines:\n        raise ValueError(f\"Platform must be one of: {', '.join(platform_guidelines.keys())}\")\n\n    hashtag_text = \"\"\n    if hashtags:\n        hashtag_text = f\"\\nSuggested hashtags: {', '.join(hashtags)}\"\n\n    emoji_instruction = \"Include appropriate emojis to increase engagement.\" if include_emoji else \"Don't use emojis.\"\n\n    prompt = f\"\"\"\n    Platform: {platform}\n    Topic: {topic}\n    Tone: {tone}\n    {hashtag_text}\n\n    {platform_guidelines[platform]}\n    {emoji_instruction}\n    \"\"\"\n\n    response = client.chat(\n        messages=[\n            {\"role\": \"system\", \"content\": \"You are a social media content creator who crafts engaging posts.\"},\n            {\"role\": \"user\", \"content\": prompt}\n        ],\n        model=\"anthropic/claude-3-haiku-20240307\",\n        temperature=0.7\n    )\n\n    return response[\"data\"]\n\n# Example usage\nlinkedin_post = generate_social_media_post(\n    platform=\"linkedin\",\n    topic=\"How AI is transforming data analysis in finance\",\n    tone=\"professional and informative\",\n    hashtags=[\"AIinFinance\", \"DataAnalytics\", \"FinTech\", \"MachineLearning\"],\n    include_emoji=True\n)\n</code></pre>"},{"location":"use-cases/content-generation/#content-repurposing","title":"Content Repurposing","text":"<p>Take existing content and repurpose it for different formats:</p> <pre><code>def repurpose_content(original_content, original_format, target_format, target_length=None):\n    \"\"\"Repurpose content from one format to another.\"\"\"\n\n    length_instruction = f\"The target length should be approximately {target_length} words.\" if target_length else \"\"\n\n    prompt = f\"\"\"\n    Original Content ({original_format}):\n\n    {original_content}\n\n    Please repurpose this content into a {target_format} format.\n    {length_instruction}\n    Maintain the key points and message while adapting to the new format's requirements.\n    \"\"\"\n\n    response = client.chat(\n        messages=[\n            {\"role\": \"system\", \"content\": \"You are a content repurposing specialist who can transform content between different formats while preserving the core message.\"},\n            {\"role\": \"user\", \"content\": prompt}\n        ],\n        model=\"openai/gpt-4o\",\n        temperature=0.7\n    )\n\n    return response[\"data\"]\n\n# Example usage\nblog_post = \"\"\"\n# The Future of Remote Work\nRemote work has transformed how businesses operate in the digital age. Since the COVID-19 pandemic, companies have discovered both challenges and benefits of distributed teams. Studies show productivity often increases, while office costs decrease. However, maintaining company culture and collaboration requires intentional strategies and tools. The future likely holds a hybrid model, combining in-person collaboration with remote flexibility.\n\"\"\"\n\n# Repurpose to different formats\ntwitter_thread = repurpose_content(blog_post, \"blog post\", \"Twitter thread (5-7 tweets)\")\nvideo_script = repurpose_content(blog_post, \"blog post\", \"video script\", 300)\nnewsletter = repurpose_content(blog_post, \"blog post\", \"email newsletter\", 250)\n</code></pre>"},{"location":"use-cases/content-generation/#data-driven-content","title":"Data-Driven Content","text":"<p>Generate content based on data and analysis:</p> <pre><code>def generate_data_report(data_summary, key_findings, audience, report_type=\"executive_summary\"):\n    \"\"\"Generate a data-driven report based on findings.\"\"\"\n\n    report_types = {\n        \"executive_summary\": \"Write a concise executive summary highlighting the most important insights.\",\n        \"detailed_analysis\": \"Write a detailed analysis explaining all findings and their implications.\",\n        \"recommendation\": \"Write recommendations based on the data findings.\",\n        \"press_release\": \"Write a press release announcing the key findings.\"\n    }\n\n    if report_type not in report_types:\n        raise ValueError(f\"Report type must be one of: {', '.join(report_types.keys())}\")\n\n    prompt = f\"\"\"\n    Data Summary: {data_summary}\n    Key Findings:\n    {key_findings}\n    Target Audience: {audience}\n\n    {report_types[report_type]}\n    Use a professional tone and focus on actionable insights.\n    \"\"\"\n\n    response = client.chat(\n        messages=[\n            {\"role\": \"system\", \"content\": \"You are a data analyst who creates clear, insightful reports from complex data.\"},\n            {\"role\": \"user\", \"content\": prompt}\n        ],\n        model=\"openai/gpt-4o\",\n        temperature=0.3  # Lower temperature for more factual content\n    )\n\n    return response[\"data\"]\n\n# Example usage\nexecutive_summary = generate_data_report(\n    data_summary=\"Survey of 1,000 consumers about shopping habits\",\n    key_findings=\"\"\"\n    - 67% of consumers prefer shopping online for electronics\n    - 82% read at least 3 reviews before making purchases over $100\n    - Mobile shopping increased by 34% compared to last year\n    - 45% of customers abandon carts due to high shipping costs\n    - Loyalty programs influence 58% of repeat purchases\n    \"\"\",\n    audience=\"E-commerce business executives\",\n    report_type=\"executive_summary\"\n)\n</code></pre>"},{"location":"use-cases/content-generation/#content-translation-and-localization","title":"Content Translation and Localization","text":"<p>Translate and adapt content for different regions:</p> <pre><code>def translate_and_localize(content, source_language, target_language, target_region=None, content_type=\"general\"):\n    \"\"\"Translate and localize content for a specific language and region.\"\"\"\n\n    region_instruction = f\"Adapt for {target_region} region specifically.\" if target_region else \"\"\n\n    content_type_instructions = {\n        \"general\": \"Translate the content while maintaining the original meaning.\",\n        \"marketing\": \"Translate and adapt marketing content to resonate with the target culture.\",\n        \"technical\": \"Translate technical content with precision, maintaining all technical details.\",\n        \"legal\": \"Translate legal content accurately, using appropriate legal terminology.\"\n    }\n\n    if content_type not in content_type_instructions:\n        raise ValueError(f\"Content type must be one of: {', '.join(content_type_instructions.keys())}\")\n\n    prompt = f\"\"\"\n    Original Content ({source_language}):\n\n    {content}\n\n    Please translate this content into {target_language}.\n    {region_instruction}\n\n    Content Type: {content_type}\n    {content_type_instructions[content_type]}\n\n    Note any cultural adaptations made in your translation.\n    \"\"\"\n\n    response = client.chat(\n        messages=[\n            {\"role\": \"system\", \"content\": f\"You are a professional translator fluent in {source_language} and {target_language} with expertise in cultural localization.\"},\n            {\"role\": \"user\", \"content\": prompt}\n        ],\n        model=\"openai/gpt-4o\",\n        temperature=0.4\n    )\n\n    return response[\"data\"]\n\n# Example usage\nenglish_content = \"\"\"\nOur Premium Membership gives you access to all courses, workshops, and resources.\nSign up today and get a 20% discount for the first three months!\n\"\"\"\n\nspanish_translation = translate_and_localize(\n    content=english_content,\n    source_language=\"English\",\n    target_language=\"Spanish\",\n    target_region=\"Mexico\",\n    content_type=\"marketing\"\n)\n</code></pre>"},{"location":"use-cases/content-generation/#best-practices-for-content-generation","title":"Best Practices for Content Generation","text":"<ol> <li>Iterative Refinement: Generate a draft, then refine it with additional prompts</li> <li>Specific Instructions: Provide clear, detailed instructions about tone, style, and format</li> <li>Model Selection: Choose the appropriate model based on the content complexity</li> <li>Temperature Control: Use higher temperature for creative content, lower for factual content</li> <li>Content Verification: Always review AI-generated content for accuracy and appropriateness</li> <li>A/B Testing: Generate multiple versions and test their effectiveness</li> <li>Human Touch: Add human editing to enhance and personalize the content</li> </ol>"},{"location":"use-cases/document-processing/","title":"Document Processing with IndoxRouter","text":"<p>IndoxRouter provides powerful capabilities for processing and analyzing documents using various language models. This guide covers common document processing tasks and patterns.</p>"},{"location":"use-cases/document-processing/#text-extraction-and-summarization","title":"Text Extraction and Summarization","text":""},{"location":"use-cases/document-processing/#document-summarization","title":"Document Summarization","text":"<pre><code>from indoxrouter import Client\n\nclient = Client(api_key=\"your_api_key\")\n\ndef summarize_document(text, max_length=200):\n    \"\"\"Summarize a long document into key points.\"\"\"\n\n    response = client.chat(\n        messages=[\n            {\n                \"role\": \"system\",\n                \"content\": f\"Summarize the following document in {max_length} words or less. Focus on the main points and key information.\"\n            },\n            {\"role\": \"user\", \"content\": text}\n        ],\n        model=\"openai/gpt-4o-mini\",\n        max_tokens=max_length * 2  # Rough estimate for token count\n    )\n\n    return response[\"data\"]\n\n# Example usage\ndocument = \"\"\"\nYour long document text here...\nThis could be a research paper, article, report, etc.\n\"\"\"\n\nsummary = summarize_document(document)\nprint(\"Summary:\", summary)\n</code></pre>"},{"location":"use-cases/document-processing/#extractive-vs-abstractive-summarization","title":"Extractive vs Abstractive Summarization","text":"<pre><code>def extractive_summary(text, num_sentences=3):\n    \"\"\"Extract key sentences from the document.\"\"\"\n\n    response = client.chat(\n        messages=[\n            {\n                \"role\": \"system\",\n                \"content\": f\"Extract the {num_sentences} most important sentences from the following text. Return only the sentences, separated by newlines.\"\n            },\n            {\"role\": \"user\", \"content\": text}\n        ],\n        model=\"openai/gpt-4o-mini\"\n    )\n\n    return response[\"data\"]\n\ndef abstractive_summary(text, style=\"professional\"):\n    \"\"\"Generate a new summary in the specified style.\"\"\"\n\n    response = client.chat(\n        messages=[\n            {\n                \"role\": \"system\",\n                \"content\": f\"Create a {style} summary of the following document. Write it in your own words, capturing the essence and main ideas.\"\n            },\n            {\"role\": \"user\", \"content\": text}\n        ],\n        model=\"openai/gpt-4o-mini\"\n    )\n\n    return response[\"data\"]\n</code></pre>"},{"location":"use-cases/document-processing/#document-classification","title":"Document Classification","text":""},{"location":"use-cases/document-processing/#topic-classification","title":"Topic Classification","text":"<pre><code>def classify_document(text, categories):\n    \"\"\"Classify a document into predefined categories.\"\"\"\n\n    categories_str = \", \".join(categories)\n\n    response = client.chat(\n        messages=[\n            {\n                \"role\": \"system\",\n                \"content\": f\"Classify the following document into one of these categories: {categories_str}. Respond with only the category name and a confidence score (0-1).\"\n            },\n            {\"role\": \"user\", \"content\": text}\n        ],\n        model=\"openai/gpt-4o-mini\"\n    )\n\n    return response[\"data\"]\n\n# Example usage\ncategories = [\"Technology\", \"Finance\", \"Healthcare\", \"Education\", \"Sports\"]\nclassification = classify_document(document, categories)\nprint(\"Classification:\", classification)\n</code></pre>"},{"location":"use-cases/document-processing/#sentiment-analysis","title":"Sentiment Analysis","text":"<pre><code>def analyze_sentiment(text):\n    \"\"\"Analyze the sentiment of a document.\"\"\"\n\n    response = client.chat(\n        messages=[\n            {\n                \"role\": \"system\",\n                \"content\": \"Analyze the sentiment of the following text. Provide: 1) Overall sentiment (positive/negative/neutral), 2) Confidence score (0-1), 3) Key phrases that indicate the sentiment.\"\n            },\n            {\"role\": \"user\", \"content\": text}\n        ],\n        model=\"openai/gpt-4o-mini\"\n    )\n\n    return response[\"data\"]\n</code></pre>"},{"location":"use-cases/document-processing/#information-extraction","title":"Information Extraction","text":""},{"location":"use-cases/document-processing/#named-entity-recognition","title":"Named Entity Recognition","text":"<pre><code>def extract_entities(text, entity_types=None):\n    \"\"\"Extract named entities from text.\"\"\"\n\n    if entity_types:\n        entity_prompt = f\"Focus on these entity types: {', '.join(entity_types)}\"\n    else:\n        entity_prompt = \"Extract all relevant entities\"\n\n    response = client.chat(\n        messages=[\n            {\n                \"role\": \"system\",\n                \"content\": f\"Extract named entities from the following text. {entity_prompt}. Format as JSON with entity type and value.\"\n            },\n            {\"role\": \"user\", \"content\": text}\n        ],\n        model=\"openai/gpt-4o-mini\"\n    )\n\n    return response[\"data\"]\n\n# Example usage\nentities = extract_entities(\n    \"Apple Inc. was founded by Steve Jobs in Cupertino, California in 1976.\",\n    entity_types=[\"PERSON\", \"ORGANIZATION\", \"LOCATION\", \"DATE\"]\n)\nprint(\"Entities:\", entities)\n</code></pre>"},{"location":"use-cases/document-processing/#key-information-extraction","title":"Key Information Extraction","text":"<pre><code>def extract_key_info(text, fields):\n    \"\"\"Extract specific fields from a document.\"\"\"\n\n    fields_str = \", \".join(fields)\n\n    response = client.chat(\n        messages=[\n            {\n                \"role\": \"system\",\n                \"content\": f\"Extract the following information from the document: {fields_str}. Format as JSON. If a field is not found, mark it as null.\"\n            },\n            {\"role\": \"user\", \"content\": text}\n        ],\n        model=\"openai/gpt-4o-mini\"\n    )\n\n    return response[\"data\"]\n\n# Example for invoice processing\ninvoice_fields = [\"invoice_number\", \"date\", \"total_amount\", \"vendor_name\", \"items\"]\nextracted_info = extract_key_info(invoice_text, invoice_fields)\n</code></pre>"},{"location":"use-cases/document-processing/#document-comparison-and-analysis","title":"Document Comparison and Analysis","text":""},{"location":"use-cases/document-processing/#document-similarity","title":"Document Similarity","text":"<pre><code>def compare_documents(doc1, doc2):\n    \"\"\"Compare two documents for similarity and differences.\"\"\"\n\n    response = client.chat(\n        messages=[\n            {\n                \"role\": \"system\",\n                \"content\": \"Compare these two documents. Provide: 1) Similarity score (0-1), 2) Main similarities, 3) Key differences, 4) Summary of comparison.\"\n            },\n            {\n                \"role\": \"user\",\n                \"content\": f\"Document 1:\\n{doc1}\\n\\nDocument 2:\\n{doc2}\"\n            }\n        ],\n        model=\"openai/gpt-4o-mini\"\n    )\n\n    return response[\"data\"]\n</code></pre>"},{"location":"use-cases/document-processing/#change-detection","title":"Change Detection","text":"<pre><code>def detect_changes(original_doc, revised_doc):\n    \"\"\"Detect changes between document versions.\"\"\"\n\n    response = client.chat(\n        messages=[\n            {\n                \"role\": \"system\",\n                \"content\": \"Identify all changes between the original and revised documents. List additions, deletions, and modifications clearly.\"\n            },\n            {\n                \"role\": \"user\",\n                \"content\": f\"Original:\\n{original_doc}\\n\\nRevised:\\n{revised_doc}\"\n            }\n        ],\n        model=\"openai/gpt-4o-mini\"\n    )\n\n    return response[\"data\"]\n</code></pre>"},{"location":"use-cases/document-processing/#batch-document-processing","title":"Batch Document Processing","text":""},{"location":"use-cases/document-processing/#processing-multiple-documents","title":"Processing Multiple Documents","text":"<pre><code>import asyncio\nfrom indoxrouter import AsyncClient\n\nasync def process_documents_batch(documents, processing_function):\n    \"\"\"Process multiple documents concurrently.\"\"\"\n\n    client = AsyncClient(api_key=\"your_api_key\")\n\n    tasks = []\n    for doc in documents:\n        task = processing_function(client, doc)\n        tasks.append(task)\n\n    results = await asyncio.gather(*tasks)\n    return results\n\nasync def summarize_async(client, text):\n    \"\"\"Async version of document summarization.\"\"\"\n\n    response = await client.chat(\n        messages=[\n            {\n                \"role\": \"system\",\n                \"content\": \"Summarize this document in 100 words or less.\"\n            },\n            {\"role\": \"user\", \"content\": text}\n        ],\n        model=\"openai/gpt-4o-mini\"\n    )\n\n    return response[\"data\"]\n\n# Usage\ndocuments = [\"doc1 text...\", \"doc2 text...\", \"doc3 text...\"]\nsummaries = asyncio.run(process_documents_batch(documents, summarize_async))\n</code></pre>"},{"location":"use-cases/document-processing/#document-quality-assessment","title":"Document Quality Assessment","text":""},{"location":"use-cases/document-processing/#readability-analysis","title":"Readability Analysis","text":"<pre><code>def assess_readability(text):\n    \"\"\"Assess document readability and provide improvement suggestions.\"\"\"\n\n    response = client.chat(\n        messages=[\n            {\n                \"role\": \"system\",\n                \"content\": \"Analyze the readability of this text. Provide: 1) Reading level, 2) Clarity score (1-10), 3) Specific suggestions for improvement, 4) Complex sentences that could be simplified.\"\n            },\n            {\"role\": \"user\", \"content\": text}\n        ],\n        model=\"openai/gpt-4o-mini\"\n    )\n\n    return response[\"data\"]\n</code></pre>"},{"location":"use-cases/document-processing/#content-quality-check","title":"Content Quality Check","text":"<pre><code>def check_content_quality(text, criteria=None):\n    \"\"\"Check document quality against specific criteria.\"\"\"\n\n    if criteria:\n        criteria_str = f\"Focus on these criteria: {', '.join(criteria)}\"\n    else:\n        criteria_str = \"Use general quality standards\"\n\n    response = client.chat(\n        messages=[\n            {\n                \"role\": \"system\",\n                \"content\": f\"Evaluate the quality of this document. {criteria_str}. Provide scores and specific feedback.\"\n            },\n            {\"role\": \"user\", \"content\": text}\n        ],\n        model=\"openai/gpt-4o-mini\"\n    )\n\n    return response[\"data\"]\n\n# Example usage\nquality_criteria = [\"accuracy\", \"completeness\", \"clarity\", \"organization\"]\nquality_report = check_content_quality(document, quality_criteria)\n</code></pre>"},{"location":"use-cases/document-processing/#specialized-document-types","title":"Specialized Document Types","text":""},{"location":"use-cases/document-processing/#legal-document-processing","title":"Legal Document Processing","text":"<pre><code>def process_legal_document(text, task=\"summarize\"):\n    \"\"\"Process legal documents with domain-specific understanding.\"\"\"\n\n    tasks = {\n        \"summarize\": \"Summarize this legal document, highlighting key legal points, obligations, and important dates.\",\n        \"extract_clauses\": \"Extract and list all important clauses, terms, and conditions from this legal document.\",\n        \"risk_analysis\": \"Identify potential legal risks, ambiguities, or concerning clauses in this document.\"\n    }\n\n    prompt = tasks.get(task, tasks[\"summarize\"])\n\n    response = client.chat(\n        messages=[\n            {\n                \"role\": \"system\",\n                \"content\": f\"{prompt} Use legal terminology appropriately and be precise.\"\n            },\n            {\"role\": \"user\", \"content\": text}\n        ],\n        model=\"openai/gpt-4o\"  # Use more capable model for legal analysis\n    )\n\n    return response[\"data\"]\n</code></pre>"},{"location":"use-cases/document-processing/#scientific-paper-processing","title":"Scientific Paper Processing","text":"<pre><code>def process_research_paper(text, section=\"abstract\"):\n    \"\"\"Process scientific papers with academic focus.\"\"\"\n\n    sections = {\n        \"abstract\": \"Extract and summarize the abstract, highlighting research objectives, methods, and key findings.\",\n        \"methodology\": \"Analyze and summarize the research methodology, including data collection and analysis methods.\",\n        \"findings\": \"Extract and summarize the key findings, results, and their significance.\",\n        \"citations\": \"Extract all citations and references mentioned in this paper.\"\n    }\n\n    prompt = sections.get(section, sections[\"abstract\"])\n\n    response = client.chat(\n        messages=[\n            {\n                \"role\": \"system\",\n                \"content\": f\"{prompt} Maintain scientific accuracy and use appropriate academic language.\"\n            },\n            {\"role\": \"user\", \"content\": text}\n        ],\n        model=\"openai/gpt-4o\"\n    )\n\n    return response[\"data\"]\n</code></pre>"},{"location":"use-cases/document-processing/#document-generation-and-enhancement","title":"Document Generation and Enhancement","text":""},{"location":"use-cases/document-processing/#document-enhancement","title":"Document Enhancement","text":"<pre><code>def enhance_document(text, enhancement_type=\"improve_clarity\"):\n    \"\"\"Enhance document quality and readability.\"\"\"\n\n    enhancements = {\n        \"improve_clarity\": \"Rewrite this text to improve clarity and readability while maintaining all original information.\",\n        \"professional_tone\": \"Rewrite this text in a more professional and formal tone.\",\n        \"simplify\": \"Simplify this text for a general audience while keeping all important information.\",\n        \"expand\": \"Expand this text with additional relevant details and explanations.\"\n    }\n\n    prompt = enhancements.get(enhancement_type, enhancements[\"improve_clarity\"])\n\n    response = client.chat(\n        messages=[\n            {\n                \"role\": \"system\",\n                \"content\": prompt\n            },\n            {\"role\": \"user\", \"content\": text}\n        ],\n        model=\"openai/gpt-4o-mini\"\n    )\n\n    return response[\"data\"]\n</code></pre>"},{"location":"use-cases/document-processing/#template-based-generation","title":"Template-Based Generation","text":"<pre><code>def generate_from_template(template, data):\n    \"\"\"Generate documents from templates and data.\"\"\"\n\n    response = client.chat(\n        messages=[\n            {\n                \"role\": \"system\",\n                \"content\": \"Fill in the following template with the provided data. Maintain the template structure and format.\"\n            },\n            {\n                \"role\": \"user\",\n                \"content\": f\"Template:\\n{template}\\n\\nData:\\n{data}\"\n            }\n        ],\n        model=\"openai/gpt-4o-mini\"\n    )\n\n    return response[\"data\"]\n\n# Example usage\nemail_template = \"\"\"\nSubject: {subject}\n\nDear {recipient_name},\n\n{opening_paragraph}\n\n{main_content}\n\n{closing_paragraph}\n\nBest regards,\n{sender_name}\n\"\"\"\n\nemail_data = {\n    \"subject\": \"Project Update\",\n    \"recipient_name\": \"John Doe\",\n    \"opening_paragraph\": \"I hope this email finds you well.\",\n    \"main_content\": \"I wanted to provide you with an update on our current project status...\",\n    \"closing_paragraph\": \"Please let me know if you have any questions.\",\n    \"sender_name\": \"Jane Smith\"\n}\n\ngenerated_email = generate_from_template(email_template, str(email_data))\n</code></pre> <p>This comprehensive guide covers the main document processing capabilities available through IndoxRouter, enabling you to build sophisticated document analysis and processing systems.</p>"},{"location":"use-cases/rag-systems/","title":"Building RAG Systems with IndoxRouter","text":"<p>Retrieval-Augmented Generation (RAG) is a powerful approach that combines the benefits of retrieving relevant information from a knowledge base with the capabilities of large language models. This guide demonstrates how to build effective RAG systems using IndoxRouter.</p>"},{"location":"use-cases/rag-systems/#what-is-rag","title":"What is RAG?","text":"<p>RAG enhances language model responses by:</p> <ol> <li>Breaking down documents into smaller chunks</li> <li>Creating vector embeddings for each chunk</li> <li>Storing these embeddings in a vector database</li> <li>When a query is received, finding the most relevant chunks</li> <li>Using these chunks as context for the language model to generate an answer</li> </ol> <p>This approach helps ground model responses in specific knowledge and reduces hallucinations.</p>"},{"location":"use-cases/rag-systems/#basic-rag-implementation","title":"Basic RAG Implementation","text":"<p>Here's a simple implementation of a RAG system using IndoxRouter:</p> <pre><code>import numpy as np\nfrom sklearn.metrics.pairwise import cosine_similarity\nfrom indoxrouter import Client\n\n# Initialize client\nclient = Client(api_key=\"your_api_key\")\n\n# Sample documents - in a real application, you would load these from files\ndocuments = [\n    \"The Python programming language was created by Guido van Rossum and first released in 1991.\",\n    \"Python is known for its readability and simplicity, making it an excellent language for beginners.\",\n    \"Python supports multiple programming paradigms, including procedural, object-oriented, and functional programming.\",\n    \"The name Python comes from Monty Python, not the snake.\",\n    \"Popular Python frameworks include Django and Flask for web development, and NumPy and Pandas for data analysis.\"\n]\n\n# Step 1: Generate embeddings for the documents\ndoc_response = client.embeddings(\n    text=documents,\n    model=\"openai/text-embedding-3-small\"\n)\n\n# Convert to numpy array for easier processing\ndoc_embeddings = np.array([item[\"embedding\"] for item in doc_response[\"data\"]])\n\n# Step 2: Process a user query\nquery = \"Why is Python good for beginners?\"\n\n# Generate embedding for the query\nquery_response = client.embeddings(\n    text=query,\n    model=\"openai/text-embedding-3-small\"\n)\nquery_embedding = np.array(query_response[\"data\"][0][\"embedding\"])\n\n# Step 3: Find the most relevant documents\nsimilarities = cosine_similarity([query_embedding], doc_embeddings)[0]\n\n# Get the top 2 most relevant documents\ntop_indices = np.argsort(similarities)[-2:][::-1]\nrelevant_docs = [documents[i] for i in top_indices]\ncontext = \"\\n\".join(relevant_docs)\n\n# Step 4: Generate an answer using the relevant documents as context\nresponse = client.chat(\n    messages=[\n        {\"role\": \"system\", \"content\": \"You are a helpful assistant. Answer the question based on the provided context only.\"},\n        {\"role\": \"user\", \"content\": f\"Context:\\n{context}\\n\\nQuestion: {query}\"}\n    ],\n    model=\"openai/gpt-4o-mini\"\n)\n\n# Print the answer\nprint(f\"Question: {query}\")\nprint(f\"Answer: {response['data']\")\n</code></pre>"},{"location":"use-cases/rag-systems/#advanced-rag-implementation","title":"Advanced RAG Implementation","text":"<p>For real-world applications, you'll need a more sophisticated approach:</p> <pre><code>import os\nimport numpy as np\nfrom sklearn.metrics.pairwise import cosine_similarity\nfrom indoxrouter import Client\n\nclass RAGSystem:\n    def __init__(self, api_key, embed_model=\"openai/text-embedding-3-small\", llm_model=\"openai/gpt-4o-mini\"):\n        \"\"\"Initialize the RAG system with API key and models.\"\"\"\n        self.client = Client(api_key=api_key)\n        self.embed_model = embed_model\n        self.llm_model = llm_model\n        self.document_store = []\n        self.embeddings = []\n\n    def chunk_text(self, text, chunk_size=500, overlap=50):\n        \"\"\"Split text into overlapping chunks.\"\"\"\n        chunks = []\n        for i in range(0, len(text), chunk_size - overlap):\n            chunk = text[i:i + chunk_size]\n            if len(chunk) &lt; 100:  # Skip very small chunks\n                continue\n            chunks.append(chunk)\n        return chunks\n\n    def add_document(self, doc_id, text, metadata=None):\n        \"\"\"Process and add a document to the RAG system.\"\"\"\n        # Split document into chunks\n        chunks = self.chunk_text(text)\n\n        # Create embeddings for each chunk\n        response = self.client.embeddings(\n            text=chunks,\n            model=self.embed_model\n        )\n\n        # Store chunks and their embeddings\n        for i, chunk in enumerate(chunks):\n            chunk_id = f\"{doc_id}_chunk_{i}\"\n            embedding = response[\"data\"][i][\"embedding\"]\n            chunk_metadata = metadata.copy() if metadata else {}\n            chunk_metadata.update({\n                \"doc_id\": doc_id,\n                \"chunk_id\": chunk_id,\n                \"chunk_index\": i,\n                \"total_chunks\": len(chunks)\n            })\n\n            self.document_store.append({\n                \"id\": chunk_id,\n                \"text\": chunk,\n                \"metadata\": chunk_metadata\n            })\n            self.embeddings.append(embedding)\n\n        print(f\"Added document {doc_id} with {len(chunks)} chunks\")\n\n    def query(self, question, top_k=3):\n        \"\"\"Process a query and return the answer with supporting evidence.\"\"\"\n        # Generate embedding for the query\n        query_response = self.client.embeddings(\n            text=question,\n            model=self.embed_model\n        )\n        query_embedding = np.array(query_response[\"data\"][0][\"embedding\"])\n\n        # Calculate similarities with all document chunks\n        doc_embeddings = np.array(self.embeddings)\n        similarities = cosine_similarity([query_embedding], doc_embeddings)[0]\n\n        # Get top K most relevant chunks\n        top_indices = np.argsort(similarities)[-top_k:][::-1]\n        relevant_chunks = [self.document_store[i] for i in top_indices]\n\n        # Build context from relevant chunks\n        context_parts = []\n        for i, chunk in enumerate(relevant_chunks):\n            context_parts.append(f\"[Document {i+1}] {chunk['text']}\")\n\n        context = \"\\n\\n\".join(context_parts)\n\n        # Generate answer using context\n        system_message = (\n            \"You are a helpful assistant. Answer the user's question based ONLY on the provided context. \"\n            \"If the answer cannot be determined from the context, say 'I don't have enough information to answer that question.'\"\n        )\n\n        response = self.client.chat(\n            messages=[\n                {\"role\": \"system\", \"content\": system_message},\n                {\"role\": \"user\", \"content\": f\"Context:\\n\\n{context}\\n\\nQuestion: {question}\"}\n            ],\n            model=self.llm_model\n        )\n\n        answer = response[\"data\"]\n\n        # Return answer along with supporting evidence\n        return {\n            \"question\": question,\n            \"answer\": answer,\n            \"sources\": relevant_chunks,\n            \"similarities\": [similarities[i] for i in top_indices]\n        }\n\n# Usage example\nrag = RAGSystem(api_key=\"your_api_key\")\n\n# Add some documents\nrag.add_document(\"python_basics\", \"\"\"\nPython is a high-level, interpreted programming language with dynamic semantics.\nIts high-level built in data structures, combined with dynamic typing and dynamic binding,\nmake it very attractive for Rapid Application Development, as well as for use as a\nscripting or glue language to connect existing components together.\n\"\"\")\n\nrag.add_document(\"python_features\", \"\"\"\nPython's simple, easy to learn syntax emphasizes readability and therefore reduces\nthe cost of program maintenance. Python supports modules and packages, which encourages\nprogram modularity and code reuse. The Python interpreter and the extensive standard\nlibrary are available in source or binary form without charge for all major platforms.\n\"\"\")\n\n# Query the system\nresult = rag.query(\"What makes Python good for development?\")\nprint(f\"Question: {result['question']}\")\nprint(f\"Answer: {result['answer']}\")\nprint(\"\\nSources:\")\nfor i, source in enumerate(result['sources']):\n    print(f\"{i+1}. {source['metadata']['doc_id']} (similarity: {result['similarities'][i]:.3f})\")\n</code></pre>"},{"location":"use-cases/rag-systems/#best-practices","title":"Best Practices","text":""},{"location":"use-cases/rag-systems/#1-document-preprocessing","title":"1. Document Preprocessing","text":"<ul> <li>Clean text: Remove unnecessary formatting, headers, footers</li> <li>Normalize text: Handle different encodings, special characters</li> <li>Structure preservation: Maintain important formatting like lists, tables</li> </ul>"},{"location":"use-cases/rag-systems/#2-chunking-strategies","title":"2. Chunking Strategies","text":"<ul> <li>Fixed-size chunking: Simple but may break context</li> <li>Sentence-based chunking: Preserves semantic boundaries</li> <li>Paragraph-based chunking: Good for structured documents</li> <li>Overlapping chunks: Helps maintain context across boundaries</li> </ul>"},{"location":"use-cases/rag-systems/#3-embedding-optimization","title":"3. Embedding Optimization","text":"<ul> <li>Choose appropriate models: Balance quality vs speed</li> <li>Batch processing: Process multiple texts together for efficiency</li> <li>Caching: Store embeddings to avoid recomputation</li> </ul>"},{"location":"use-cases/rag-systems/#4-retrieval-tuning","title":"4. Retrieval Tuning","text":"<ul> <li>Adjust top_k: Find the right balance of context vs noise</li> <li>Similarity thresholds: Filter out irrelevant results</li> <li>Hybrid search: Combine semantic and keyword search</li> </ul>"},{"location":"use-cases/rag-systems/#5-response-generation","title":"5. Response Generation","text":"<ul> <li>Clear instructions: Tell the model how to use the context</li> <li>Context formatting: Structure the retrieved information clearly</li> <li>Fallback handling: Handle cases where no relevant context is found</li> </ul>"},{"location":"use-cases/rag-systems/#integration-with-vector-databases","title":"Integration with Vector Databases","text":"<p>For production systems, consider using dedicated vector databases:</p> <pre><code># Example with Pinecone\nimport pinecone\nfrom indoxrouter import Client\n\nclass ProductionRAG:\n    def __init__(self, api_key, pinecone_api_key, pinecone_env):\n        self.client = Client(api_key=api_key)\n\n        # Initialize Pinecone\n        pinecone.init(api_key=pinecone_api_key, environment=pinecone_env)\n        self.index = pinecone.Index(\"rag-index\")\n\n    def add_document(self, doc_id, text):\n        # Generate embeddings\n        response = self.client.embeddings(\n            text=text,\n            model=\"openai/text-embedding-3-small\"\n        )\n\n        # Store in Pinecone\n        self.index.upsert([(\n            doc_id,\n            response[\"data\"][0][\"embedding\"],\n            {\"text\": text}\n        )])\n\n    def query(self, question, top_k=3):\n        # Generate query embedding\n        query_response = self.client.embeddings(\n            text=question,\n            model=\"openai/text-embedding-3-small\"\n        )\n\n        # Search Pinecone\n        results = self.index.query(\n            vector=query_response[\"data\"][0][\"embedding\"],\n            top_k=top_k,\n            include_metadata=True\n        )\n\n        # Build context\n        context = \"\\n\\n\".join([match.metadata[\"text\"] for match in results.matches])\n\n        # Generate answer\n        response = self.client.chat(\n            messages=[\n                {\"role\": \"system\", \"content\": \"Answer based on the provided context.\"},\n                {\"role\": \"user\", \"content\": f\"Context:\\n{context}\\n\\nQuestion: {question}\"}\n            ],\n            model=\"openai/gpt-4o-mini\"\n        )\n\n        return response[\"data\"]\n</code></pre> <p>This approach provides scalable, production-ready RAG systems with IndoxRouter.</p>"}]}