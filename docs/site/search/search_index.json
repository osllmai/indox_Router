{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"IndoxRouter","text":"<p>A unified Python client for accessing multiple AI providers through a single, consistent API. Switch between OpenAI, Anthropic, Google, Mistral, DeepSeek, XAI, and Qwen models seamlessly without changing your code.</p>"},{"location":"#installation","title":"Installation","text":"<pre><code>pip install indoxrouter\n</code></pre>"},{"location":"#quick-start","title":"Quick Start","text":""},{"location":"#initialize-the-client","title":"Initialize the Client","text":"<pre><code>from indoxrouter import Client\n\n# Initialize with API key\nclient = Client(api_key=\"your_api_key\")\n\n# Or use environment variable INDOX_ROUTER_API_KEY\nclient = Client()\n</code></pre>"},{"location":"#chat-completion-example","title":"Chat Completion Example","text":"<pre><code>response = client.chat(\n    messages=[\n        {\"role\": \"user\", \"content\": \"Tell me a story about a robot in 5 sentences.\"}\n    ],\n    model=\"deepseek/deepseek-chat\"\n)\n\nprint(response['data'])\nprint(f\"Cost: ${response['usage']['cost']}\")\nprint(f\"Tokens used: {response['usage']['tokens_total']}\")\n</code></pre>"},{"location":"#response-format","title":"Response Format","text":"<p>Every response includes detailed usage information:</p> <pre><code>{\n    'request_id': 'c08cc108-6b0d-48bd-a660-546143f1b9fa',\n    'created_at': '2025-05-19T06:07:38.077269',\n    'duration_ms': 9664.651870727539,\n    'provider': 'deepseek',\n    'model': 'deepseek-chat',\n    'success': True,\n    'message': '',\n    'usage': {\n        'tokens_prompt': 15,\n        'tokens_completion': 107,\n        'tokens_total': 122,\n        'cost': 0.000229,\n        'latency': 9.487398862838745,\n        'timestamp': '2025-05-19T06:07:38.065330'\n    },\n    'data': 'Your AI response text here...',\n    'finish_reason': None\n}\n</code></pre>"},{"location":"#usage-tracking","title":"Usage Tracking","text":"<p>Monitor your usage and costs:</p> <pre><code># Get detailed usage statistics\nusage = client.get_usage()\nprint(f\"Total requests: {usage['total_requests']}\")\nprint(f\"Total cost: ${usage['total_cost']}\")\nprint(f\"Remaining credits: ${usage['remaining_credits']}\")\n</code></pre>"},{"location":"#model-information","title":"Model Information","text":"<p>Get detailed information about available models:</p> <pre><code># Get specific model info\nmodel_info = client.get_model_info(provider=\"openai\", model=\"gpt-4o-mini\")\nprint(f\"Context window: {model_info['specs']['context_window']}\")\nprint(f\"Capabilities: {model_info['capabilities']}\")\n\n# List all available models\nmodels = client.models()\nfor provider in models:\n    print(f\"Provider: {provider['name']}\")\n    for model in provider.get('text_completions', []):\n        print(f\"  - {model['modelName']}\")\n</code></pre>"},{"location":"#using-with-openai-sdk","title":"Using with OpenAI SDK","text":"<p>You can also use the OpenAI SDK with IndoxRouter's base URL:</p> <pre><code>from openai import OpenAI\n\nclient = OpenAI(\n    api_key=\"your_indoxrouter_api_key\",\n    base_url=\"https://api.indoxrouter.com\"\n)\n\nresponse = client.chat.completions.create(\n    model=\"anthropic/claude-3-haiku-20240307\",\n    messages=[{\"role\": \"user\", \"content\": \"Hello!\"}]\n)\n</code></pre>"},{"location":"#examples-by-use-case","title":"Examples by Use Case","text":""},{"location":"#cost-optimized-chat","title":"Cost-Optimized Chat","text":"<pre><code># Use fast, cost-effective models for high-volume applications\nresponse = client.chat(\n    messages=[{\"role\": \"user\", \"content\": \"Summarize this text...\"}],\n    model=\"openai/gpt-3.5-turbo\",  # Most cost-effective\n    max_tokens=100\n)\n</code></pre>"},{"location":"#high-quality-analysis","title":"High-Quality Analysis","text":"<pre><code># Use premium models for complex reasoning\nresponse = client.chat(\n    messages=[{\"role\": \"user\", \"content\": \"Analyze this complex problem...\"}],\n    model=\"anthropic/claude-3-opus-20240229\",  # Highest quality\n    temperature=0.1  # More focused responses\n)\n</code></pre>"},{"location":"#code-generation","title":"Code Generation","text":"<pre><code># Use specialized coding models\nresponse = client.chat(\n    messages=[{\"role\": \"user\", \"content\": \"Write a Python function to...\"}],\n    model=\"deepseek/deepseek-coder\",  # Optimized for coding\n    temperature=0.0  # Deterministic code\n)\n</code></pre>"},{"location":"#image-generation","title":"Image Generation","text":"<pre><code># Generate images with different providers\nresponse = client.images(\n    prompt=\"A futuristic cityscape at sunset\",\n    model=\"openai/dall-e-3\",\n    size=\"1024x1024\",\n    style=\"vivid\"\n)\n\nimage_url = response['data'][0]['url']\n</code></pre>"},{"location":"#rate-limits","title":"Rate Limits","text":"<p>IndoxRouter has three tiers with different rate limits:</p> Tier Requests/Minute Tokens/Hour Best For Free 10 10,000 Testing &amp; prototyping Standard 60 100,000 Production applications Enterprise 500 1,000,000 High-volume applications <p>Rate limit information is included in error responses when limits are exceeded.</p>"},{"location":"#next-steps","title":"Next Steps","text":"<ul> <li>Getting Started: Detailed setup guide</li> <li>Usage Examples: Comprehensive usage examples</li> <li>API Reference: Full API documentation</li> </ul>"},{"location":"getting-started/","title":"Getting Started","text":"<p>This guide will help you get started with the IndoxRouter library, showing you how to install the package, set up your API key, and make your first API call.</p>"},{"location":"getting-started/#installation","title":"Installation","text":"<p>To install the IndoxRouter, use pip:</p> <pre><code>pip install indoxrouter\n</code></pre>"},{"location":"getting-started/#setting-up-your-api-key","title":"Setting Up Your API Key","text":"<p>To use IndoxRouter, you need an API key from your IndoxRouter Server instance. There are several ways to configure your API key:</p>"},{"location":"getting-started/#method-1-directly-in-the-client-constructor","title":"Method 1: Directly in the Client constructor","text":"<pre><code>from indoxrouter import Client\n\nclient = Client(api_key=\"your_api_key\")\n</code></pre>"},{"location":"getting-started/#method-2-using-environment-variables","title":"Method 2: Using environment variables","text":"<p>You can set the <code>INDOX_ROUTER_API_KEY</code> environment variable and the client will use it automatically:</p> <pre><code># In your terminal or .env file\n# export INDOX_ROUTER_API_KEY=your_api_key\n\n# In your Python code\nfrom indoxrouter import Client\n\nclient = Client()  # Will use the environment variable\n</code></pre>"},{"location":"getting-started/#method-3-configuration-file","title":"Method 3: Configuration file","text":"<p>Coming soon: Support for loading configuration from a file.</p>"},{"location":"getting-started/#verifying-your-api-key","title":"Verifying Your API Key","text":"<p>You can verify that your API key is working correctly by using the test_connection method:</p> <pre><code>from indoxrouter import Client\n\nclient = Client(api_key=\"your_api_key\")\ntry:\n    result = client.test_connection()\n    print(\"Connection successful!\")\n    print(f\"Connected to server version: {result.get('version', 'unknown')}\")\nexcept Exception as e:\n    print(f\"Connection failed: {str(e)}\")\n</code></pre>"},{"location":"getting-started/#making-your-first-api-call","title":"Making Your First API Call","text":"<p>Here's a simple example of making a chat completion request:</p> <pre><code>from indoxrouter import Client\n\n# Initialize the client\nclient = Client(api_key=\"your_api_key\")\n\n# Make a chat completion request\nresponse = client.chat(\n    messages=[\n        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n        {\"role\": \"user\", \"content\": \"Hello, how are you today?\"}\n    ],\n    model=\"openai/gpt-4o-mini\"\n)\n\n# Print the response\nprint(response[\"data\"])\n</code></pre>"},{"location":"getting-started/#error-handling","title":"Error Handling","text":"<p>IndoxRouter provides clear error handling. Here's an example of how to handle errors:</p> <pre><code>from indoxrouter import Client, ModelNotFoundError, ProviderError, AuthenticationError\n\ntry:\n    client = Client(api_key=\"your_api_key\")\n    response = client.chat(\n        messages=[{\"role\": \"user\", \"content\": \"Hello\"}],\n        model=\"nonexistent-provider/nonexistent-model\"\n    )\nexcept AuthenticationError as e:\n    print(f\"Authentication failed: {e}\")\nexcept ModelNotFoundError as e:\n    print(f\"Model not found: {e}\")\nexcept ProviderError as e:\n    print(f\"Provider error: {e}\")\nexcept Exception as e:\n    print(f\"Unexpected error: {e}\")\n</code></pre>"},{"location":"getting-started/#best-practices","title":"Best Practices","text":"<ul> <li>Set appropriate timeouts for your use case</li> <li>Handle errors appropriately for your application</li> <li>Consider using environment variables for API keys rather than hardcoding them</li> </ul>"},{"location":"getting-started/#next-steps","title":"Next Steps","text":"<p>Now that you're set up, check out the Usage Guide for more detailed information on working with the different API capabilities:</p> <ul> <li>Basic Usage</li> <li>Chat Completions</li> <li>Text Completions</li> <li>Embeddings</li> <li>Image Generation</li> </ul>"},{"location":"api/client/","title":"Client API Reference","text":"<p>The <code>Client</code> class is the main entry point for interacting with the IndoxRouter API. This page documents all the methods and functionality provided by the client.</p>"},{"location":"api/client/#initialization","title":"Initialization","text":"<pre><code>from indoxrouter import Client\n\nclient = Client(\n    api_key=\"your_api_key\",\n    timeout=30\n)\n</code></pre>"},{"location":"api/client/#parameters","title":"Parameters","text":"<ul> <li><code>api_key</code> (<code>str</code>, optional): Your API key for authentication. If not provided, the client will look for the <code>INDOX_ROUTER_API_KEY</code> environment variable.</li> <li><code>timeout</code> (<code>int</code>, optional): Request timeout in seconds. Defaults to 30.</li> </ul>"},{"location":"api/client/#methods","title":"Methods","text":""},{"location":"api/client/#authentication","title":"Authentication","text":"<pre><code>def _authenticate(self):\n    \"\"\"\n    Authenticate with the server and get JWT tokens.\n    This uses the /auth/token endpoint to get JWT tokens using the API key.\n    \"\"\"\n</code></pre> <p>This method is called automatically during initialization. It exchanges the API key for JWT tokens that are used for subsequent requests.</p>"},{"location":"api/client/#chat-completions","title":"Chat Completions","text":"<pre><code>def chat(\n    self,\n    messages: List[Dict[str, str]],\n    model: str = DEFAULT_MODEL,\n    temperature: float = 0.7,\n    max_tokens: Optional[int] = None,\n    stream: bool = False,\n    **kwargs,\n) -&gt; Dict[str, Any]:\n    \"\"\"\n    Generate a chat completion.\n\n    Args:\n        messages: A list of message objects with role and content keys.\n        model: The model to use in format \"provider/model_name\".\n        temperature: Controls randomness. Higher values (e.g., 0.8) make output more random,\n                     lower values (e.g., 0.2) make it more deterministic.\n        max_tokens: Maximum number of tokens to generate.\n        stream: Whether to stream the response.\n        **kwargs: Additional keyword arguments to pass to the API.\n\n    Returns:\n        A dictionary containing the API response.\n    \"\"\"\n</code></pre>"},{"location":"api/client/#text-completions","title":"Text Completions","text":"<pre><code>def completion(\n    self,\n    prompt: str,\n    model: str = DEFAULT_MODEL,\n    temperature: float = 0.7,\n    max_tokens: Optional[int] = None,\n    stream: bool = False,\n    **kwargs,\n) -&gt; Dict[str, Any]:\n    \"\"\"\n    Generate a text completion.\n\n    Args:\n        prompt: The prompt to complete.\n        model: The model to use in format \"provider/model_name\".\n        temperature: Controls randomness. Higher values make output more random,\n                     lower values make it more deterministic.\n        max_tokens: Maximum number of tokens to generate.\n        stream: Whether to stream the response.\n        **kwargs: Additional keyword arguments to pass to the API.\n\n    Returns:\n        A dictionary containing the API response.\n    \"\"\"\n</code></pre>"},{"location":"api/client/#embeddings","title":"Embeddings","text":"<pre><code>def embeddings(\n    self,\n    text: Union[str, List[str]],\n    model: str = DEFAULT_EMBEDDING_MODEL,\n    **kwargs,\n) -&gt; Dict[str, Any]:\n    \"\"\"\n    Generate embeddings for the given text.\n\n    Args:\n        text: The text to embed. Can be a string or a list of strings.\n        model: The model to use in format \"provider/model_name\".\n        **kwargs: Additional keyword arguments to pass to the API.\n\n    Returns:\n        A dictionary containing the API response.\n    \"\"\"\n</code></pre>"},{"location":"api/client/#image-generation","title":"Image Generation","text":"<pre><code>def images(\n    self,\n    prompt: str,\n    model: str = DEFAULT_IMAGE_MODEL,\n    size: str = \"1024x1024\",\n    n: int = 1,\n    quality: str = \"standard\",\n    style: str = \"vivid\",\n    **kwargs,\n) -&gt; Dict[str, Any]:\n    \"\"\"\n    Generate images from a text prompt.\n\n    Args:\n        prompt: The prompt to use for generating the image.\n        model: The model to use in format \"provider/model_name\".\n        size: The size of the image in format \"widthxheight\".\n        n: The number of images to generate.\n        quality: The quality of the image (\"standard\" or \"hd\").\n        style: The style of the image (\"vivid\" or \"natural\").\n        **kwargs: Additional keyword arguments to pass to the API.\n\n    Returns:\n        A dictionary containing the API response.\n    \"\"\"\n</code></pre>"},{"location":"api/client/#model-information","title":"Model Information","text":"<pre><code>def models(self, provider: Optional[str] = None) -&gt; Dict[str, Any]:\n    \"\"\"\n    Get information about available models.\n\n    Args:\n        provider: Optional provider ID to filter by.\n\n    Returns:\n        A dictionary containing information about available models.\n    \"\"\"\n\ndef get_model_info(self, provider: str, model: str) -&gt; Dict[str, Any]:\n    \"\"\"\n    Get information about a specific model.\n\n    Args:\n        provider: The provider ID.\n        model: The model ID.\n\n    Returns:\n        A dictionary containing information about the model.\n    \"\"\"\n</code></pre>"},{"location":"api/client/#usage-information","title":"Usage Information","text":"<pre><code>def get_usage(self) -&gt; Dict[str, Any]:\n    \"\"\"\n    Get usage information for the current user.\n\n    Returns:\n        A dictionary containing usage information.\n    \"\"\"\n</code></pre>"},{"location":"api/client/#testing-and-diagnostics","title":"Testing and Diagnostics","text":"<pre><code>def test_connection(self) -&gt; Dict[str, Any]:\n    \"\"\"\n    Test the connection to the server.\n\n    Returns:\n        A dictionary containing information about the connection.\n    \"\"\"\n\ndef diagnose_request(self, endpoint: str, data: Dict[str, Any]) -&gt; Dict[str, Any]:\n    \"\"\"\n    Diagnose a request without sending it.\n\n    Args:\n        endpoint: The API endpoint (e.g., \"chat/completions\").\n        data: The request data.\n\n    Returns:\n        A dictionary containing diagnostic information.\n    \"\"\"\n\ndef enable_debug(self, level=logging.DEBUG):\n    \"\"\"\n    Enable debug logging.\n\n    Args:\n        level: The logging level to use.\n    \"\"\"\n</code></pre>"},{"location":"api/client/#resource-management","title":"Resource Management","text":"<pre><code>def close(self):\n    \"\"\"\n    Close the client session and free up resources.\n    \"\"\"\n\ndef __enter__(self):\n    \"\"\"\n    Enter the context manager.\n    \"\"\"\n    return self\n\ndef __exit__(self, exc_type, exc_val, exc_tb):\n    \"\"\"\n    Exit the context manager and clean up resources.\n    \"\"\"\n    self.close()\n</code></pre>"},{"location":"api/client/#configuration","title":"Configuration","text":"<pre><code>def set_base_url(self, base_url: str) -&gt; None:\n    \"\"\"\n    Set the base URL for the API.\n\n    Args:\n        base_url: The base URL to use.\n    \"\"\"\n</code></pre>"},{"location":"api/exceptions/","title":"Exceptions","text":"<p>IndoxRouter provides specific exception classes to help you handle different types of errors gracefully.</p>"},{"location":"api/exceptions/#exception-hierarchy","title":"Exception Hierarchy","text":"<pre><code>IndoxRouterError (base exception)\n\u251c\u2500\u2500 AuthenticationError\n\u251c\u2500\u2500 RateLimitError\n\u251c\u2500\u2500 APIError\n\u251c\u2500\u2500 NetworkError\n\u2514\u2500\u2500 ValidationError\n</code></pre>"},{"location":"api/exceptions/#base-exception","title":"Base Exception","text":""},{"location":"api/exceptions/#indoxroutererror","title":"IndoxRouterError","text":"<p>The base exception class for all IndoxRouter-related errors.</p> <pre><code>from indoxrouter import IndoxRouterError\n\ntry:\n    response = client.chat(messages=[...], model=\"invalid/model\")\nexcept IndoxRouterError as e:\n    print(f\"An error occurred: {e}\")\n    print(f\"Error type: {type(e).__name__}\")\n</code></pre>"},{"location":"api/exceptions/#specific-exceptions","title":"Specific Exceptions","text":""},{"location":"api/exceptions/#authenticationerror","title":"AuthenticationError","text":"<p>Raised when API key is invalid or missing.</p> <pre><code>from indoxrouter import Client, AuthenticationError\n\ntry:\n    client = Client(api_key=\"invalid_key\")\n    response = client.chat(messages=[...])\nexcept AuthenticationError as e:\n    print(\"Authentication failed. Please check your API key.\")\n    print(f\"Error details: {e}\")\n</code></pre>"},{"location":"api/exceptions/#ratelimiterror","title":"RateLimitError","text":"<p>Raised when you exceed the rate limits.</p> <pre><code>from indoxrouter import RateLimitError\nimport time\n\ntry:\n    response = client.chat(messages=[...])\nexcept RateLimitError as e:\n    print(\"Rate limit exceeded. Waiting before retry...\")\n    time.sleep(60)  # Wait 1 minute\n    # Retry the request\n</code></pre>"},{"location":"api/exceptions/#apierror","title":"APIError","text":"<p>Raised for general API errors from the provider.</p> <pre><code>from indoxrouter import APIError\n\ntry:\n    response = client.chat(messages=[...])\nexcept APIError as e:\n    print(f\"API error occurred: {e}\")\n    print(f\"Status code: {e.status_code}\")\n    print(f\"Error message: {e.message}\")\n</code></pre>"},{"location":"api/exceptions/#networkerror","title":"NetworkError","text":"<p>Raised for network-related issues.</p> <pre><code>from indoxrouter import NetworkError\n\ntry:\n    response = client.chat(messages=[...])\nexcept NetworkError as e:\n    print(\"Network error occurred. Please check your connection.\")\n    print(f\"Error details: {e}\")\n</code></pre>"},{"location":"api/exceptions/#validationerror","title":"ValidationError","text":"<p>Raised when request parameters are invalid.</p> <pre><code>from indoxrouter import ValidationError\n\ntry:\n    response = client.chat(\n        messages=[],  # Empty messages list\n        model=\"openai/gpt-4o-mini\"\n    )\nexcept ValidationError as e:\n    print(\"Invalid request parameters:\")\n    print(f\"Error details: {e}\")\n</code></pre>"},{"location":"api/exceptions/#error-handling-best-practices","title":"Error Handling Best Practices","text":""},{"location":"api/exceptions/#comprehensive-error-handling","title":"Comprehensive Error Handling","text":"<pre><code>from indoxrouter import (\n    Client,\n    AuthenticationError,\n    RateLimitError,\n    APIError,\n    NetworkError,\n    ValidationError,\n    IndoxRouterError\n)\nimport time\n\ndef robust_chat(client, messages, model, max_retries=3):\n    for attempt in range(max_retries):\n        try:\n            response = client.chat(messages=messages, model=model)\n            return response\n\n        except AuthenticationError:\n            print(\"Authentication failed. Please check your API key.\")\n            return None\n\n        except ValidationError as e:\n            print(f\"Invalid request parameters: {e}\")\n            return None\n\n        except RateLimitError:\n            if attempt &lt; max_retries - 1:\n                wait_time = 2 ** attempt\n                print(f\"Rate limit hit. Waiting {wait_time} seconds...\")\n                time.sleep(wait_time)\n                continue\n            else:\n                print(\"Rate limit exceeded after all retries.\")\n                return None\n\n        except NetworkError:\n            if attempt &lt; max_retries - 1:\n                wait_time = 2 ** attempt\n                print(f\"Network error. Retrying in {wait_time} seconds...\")\n                time.sleep(wait_time)\n                continue\n            else:\n                print(\"Network error persists after all retries.\")\n                return None\n\n        except APIError as e:\n            print(f\"API error: {e}\")\n            if e.status_code &gt;= 500 and attempt &lt; max_retries - 1:\n                # Retry on server errors\n                wait_time = 2 ** attempt\n                print(f\"Server error. Retrying in {wait_time} seconds...\")\n                time.sleep(wait_time)\n                continue\n            else:\n                return None\n\n        except IndoxRouterError as e:\n            print(f\"Unexpected IndoxRouter error: {e}\")\n            return None\n\n    return None\n\n# Usage\nclient = Client(api_key=\"your_api_key\")\nresponse = robust_chat(\n    client,\n    [{\"role\": \"user\", \"content\": \"Hello!\"}],\n    \"openai/gpt-4o-mini\"\n)\n\nif response:\n    print(response['choices'][0]['message']['content'])\nelse:\n    print(\"Failed to get response after all attempts.\")\n</code></pre>"},{"location":"api/exceptions/#logging-errors","title":"Logging Errors","text":"<pre><code>import logging\nfrom indoxrouter import Client, IndoxRouterError\n\n# Set up logging\nlogging.basicConfig(level=logging.INFO)\nlogger = logging.getLogger(__name__)\n\ndef chat_with_logging(client, messages, model):\n    try:\n        response = client.chat(messages=messages, model=model)\n        logger.info(f\"Successfully generated response using {model}\")\n        return response\n\n    except IndoxRouterError as e:\n        logger.error(f\"IndoxRouter error: {type(e).__name__}: {e}\")\n        raise\n\n    except Exception as e:\n        logger.error(f\"Unexpected error: {type(e).__name__}: {e}\")\n        raise\n\n# Usage\nclient = Client(api_key=\"your_api_key\")\ntry:\n    response = chat_with_logging(\n        client,\n        [{\"role\": \"user\", \"content\": \"Hello!\"}],\n        \"openai/gpt-4o-mini\"\n    )\nexcept IndoxRouterError:\n    print(\"Failed to generate response due to IndoxRouter error.\")\n</code></pre>"},{"location":"api/exceptions/#error-response-format","title":"Error Response Format","text":"<p>When an exception occurs, you can access additional information:</p> <pre><code>try:\n    response = client.chat(messages=[...])\nexcept APIError as e:\n    print(f\"Status Code: {e.status_code}\")\n    print(f\"Error Type: {e.error_type}\")\n    print(f\"Message: {e.message}\")\n    print(f\"Request ID: {e.request_id}\")  # For debugging with support\n</code></pre>"},{"location":"api/responses/","title":"Response Schemas","text":"<p>This page documents the response formats for all IndoxRouter API endpoints.</p>"},{"location":"api/responses/#chat-completion-response","title":"Chat Completion Response","text":"<pre><code>{\n    \"id\": \"chatcmpl-123\",\n    \"object\": \"chat.completion\",\n    \"created\": 1677652288,\n    \"model\": \"gpt-4\",\n    \"provider\": \"openai\",\n    \"choices\": [\n        {\n            \"index\": 0,\n            \"message\": {\n                \"role\": \"assistant\",\n                \"content\": \"Hello! How can I help you today?\"\n            },\n            \"finish_reason\": \"stop\"\n        }\n    ],\n    \"usage\": {\n        \"prompt_tokens\": 12,\n        \"completion_tokens\": 8,\n        \"total_tokens\": 20,\n        \"cache_read_tokens\": 0,\n        \"cache_write_tokens\": 0,\n        \"reasoning_tokens\": 0,\n        \"web_search_count\": 0,\n        \"request_count\": 1,\n        \"cost_breakdown\": {\n            \"prompt_cost\": 0.00036,\n            \"completion_cost\": 0.00024,\n            \"cache_read_cost\": 0.0,\n            \"cache_write_cost\": 0.0,\n            \"reasoning_cost\": 0.0,\n            \"web_search_cost\": 0.0,\n            \"total_cost\": 0.0006\n        }\n    }\n}\n</code></pre>"},{"location":"api/responses/#text-completion-response","title":"Text Completion Response","text":"<pre><code>{\n    \"id\": \"cmpl-123\",\n    \"object\": \"text_completion\",\n    \"created\": 1677652288,\n    \"model\": \"gpt-3.5-turbo-instruct\",\n    \"provider\": \"openai\",\n    \"choices\": [\n        {\n            \"text\": \"This is a sample completion text.\",\n            \"index\": 0,\n            \"finish_reason\": \"stop\"\n        }\n    ],\n    \"usage\": {\n        \"prompt_tokens\": 5,\n        \"completion_tokens\": 7,\n        \"total_tokens\": 12,\n        \"cache_read_tokens\": 0,\n        \"cache_write_tokens\": 0,\n        \"reasoning_tokens\": 0,\n        \"web_search_count\": 0,\n        \"request_count\": 1,\n        \"cost_breakdown\": {\n            \"prompt_cost\": 0.000075,\n            \"completion_cost\": 0.000105,\n            \"cache_read_cost\": 0.0,\n            \"cache_write_cost\": 0.0,\n            \"reasoning_cost\": 0.0,\n            \"web_search_cost\": 0.0,\n            \"total_cost\": 0.00018\n        }\n    }\n}\n</code></pre>"},{"location":"api/responses/#embedding-response","title":"Embedding Response","text":"<pre><code>{\n    \"object\": \"list\",\n    \"data\": [\n        {\n            \"object\": \"embedding\",\n            \"embedding\": [0.0023064255, -0.009327292, ...],\n            \"index\": 0\n        }\n    ],\n    \"model\": \"text-embedding-ada-002\",\n    \"provider\": \"openai\",\n    \"usage\": {\n        \"prompt_tokens\": 8,\n        \"total_tokens\": 8,\n        \"request_count\": 1,\n        \"cost_breakdown\": {\n            \"total_cost\": 0.0000032\n        }\n    }\n}\n</code></pre>"},{"location":"api/responses/#image-generation-response","title":"Image Generation Response","text":""},{"location":"api/responses/#url-based-models-dall-e-2-dall-e-3","title":"URL-based Models (DALL-E 2, DALL-E 3)","text":"<pre><code>{\n    \"created\": 1677652288,\n    \"data\": [\n        {\n            \"url\": \"https://oaidalleapiprodscus.blob.core.windows.net/private/...\"\n        }\n    ],\n    \"provider\": \"openai\",\n    \"model\": \"dall-e-3\",\n    \"usage\": {\n        \"request_count\": 1,\n        \"cost_breakdown\": {\n            \"total_cost\": 0.04\n        }\n    }\n}\n</code></pre>"},{"location":"api/responses/#base64-based-models-gpt-image-1","title":"Base64-based Models (GPT-Image-1)","text":"<pre><code>{\n    \"created\": 1677652288,\n    \"data\": [\n        {\n            \"b64_json\": \"iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAADUlEQVR42mP8/5+hHgAHggJ/PchI7wAAAABJRU5ErkJggg==\"\n        }\n    ],\n    \"provider\": \"openai\",\n    \"model\": \"gpt-image-1\",\n    \"usage\": {\n        \"request_count\": 1,\n        \"cost_breakdown\": {\n            \"total_cost\": 0.02\n        }\n    }\n}\n</code></pre>"},{"location":"api/responses/#model-information-response","title":"Model Information Response","text":"<pre><code>{\n    \"models\": [\n        {\n            \"id\": \"gpt-4\",\n            \"object\": \"model\",\n            \"created\": 1677610602,\n            \"owned_by\": \"openai\",\n            \"provider\": \"openai\",\n            \"capabilities\": [\"chat\", \"completion\"],\n            \"context_length\": 8192,\n            \"pricing\": {\n                \"prompt\": 0.03,\n                \"completion\": 0.06,\n                \"unit\": \"1K tokens\"\n            }\n        }\n    ]\n}\n</code></pre>"},{"location":"api/responses/#usage-statistics-response","title":"Usage Statistics Response","text":"<pre><code>{\n    \"total_requests\": 1250,\n    \"total_tokens\": 45000,\n    \"total_cost\": 12.50,\n    \"current_period\": {\n        \"requests\": 150,\n        \"tokens\": 5500,\n        \"cost\": 1.75,\n        \"period_start\": \"2024-01-01T00:00:00Z\",\n        \"period_end\": \"2024-01-31T23:59:59Z\"\n    },\n    \"by_provider\": {\n        \"openai\": {\n            \"requests\": 800,\n            \"tokens\": 28000,\n            \"cost\": 8.40\n        },\n        \"anthropic\": {\n            \"requests\": 300,\n            \"tokens\": 12000,\n            \"cost\": 2.88\n        }\n    },\n    \"by_model\": {\n        \"gpt-4\": {\n            \"requests\": 400,\n            \"tokens\": 15000,\n            \"cost\": 4.50\n        },\n        \"claude-3-sonnet\": {\n            \"requests\": 300,\n            \"tokens\": 12000,\n            \"cost\": 2.88\n        }\n    }\n}\n</code></pre>"},{"location":"api/responses/#error-response","title":"Error Response","text":"<pre><code>{\n    \"error\": {\n        \"message\": \"Invalid API key provided\",\n        \"type\": \"invalid_request_error\",\n        \"param\": null,\n        \"code\": \"invalid_api_key\"\n    }\n}\n</code></pre>"},{"location":"api/responses/#response-fields","title":"Response Fields","text":""},{"location":"api/responses/#usage-object","title":"Usage Object","text":"<p>All API responses include a <code>usage</code> object with the following fields:</p> <ul> <li><code>prompt_tokens</code> (integer): Number of tokens in the prompt</li> <li><code>completion_tokens</code> (integer): Number of tokens in the completion</li> <li><code>total_tokens</code> (integer): Total tokens used (prompt + completion)</li> <li><code>cache_read_tokens</code> (integer): Tokens read from cache</li> <li><code>cache_write_tokens</code> (integer): Tokens written to cache</li> <li><code>reasoning_tokens</code> (integer): Tokens used for reasoning (reasoning models)</li> <li><code>web_search_count</code> (integer): Number of web searches performed</li> <li><code>request_count</code> (integer): Number of API requests made</li> <li><code>cost_breakdown</code> (object): Detailed cost information</li> </ul>"},{"location":"api/responses/#cost-breakdown-object","title":"Cost Breakdown Object","text":"<p>The <code>cost_breakdown</code> object provides detailed pricing:</p> <ul> <li><code>prompt_cost</code> (float): Cost for prompt tokens</li> <li><code>completion_cost</code> (float): Cost for completion tokens</li> <li><code>cache_read_cost</code> (float): Cost for cache read operations</li> <li><code>cache_write_cost</code> (float): Cost for cache write operations</li> <li><code>reasoning_cost</code> (float): Cost for reasoning tokens</li> <li><code>web_search_cost</code> (float): Cost for web search operations</li> <li><code>total_cost</code> (float): Total cost for the request</li> </ul>"},{"location":"api/responses/#finish-reasons","title":"Finish Reasons","text":"<p>Possible values for <code>finish_reason</code>:</p> <ul> <li><code>stop</code>: Natural stopping point or provided stop sequence</li> <li><code>length</code>: Maximum token limit reached</li> <li><code>content_filter</code>: Content filtered due to policy violations</li> <li><code>tool_calls</code>: Model called a function/tool</li> <li><code>function_call</code>: Model called a function (deprecated)</li> </ul>"},{"location":"api/responses/#http-status-codes","title":"HTTP Status Codes","text":"<ul> <li>200: Success</li> <li>400: Bad Request - Invalid parameters</li> <li>401: Unauthorized - Invalid API key</li> <li>403: Forbidden - Insufficient permissions</li> <li>429: Too Many Requests - Rate limit exceeded</li> <li>500: Internal Server Error</li> <li>502: Bad Gateway - Provider error</li> <li>503: Service Unavailable - Temporary outage</li> </ul>"},{"location":"examples/advanced/","title":"Advanced Examples","text":"<p>This section covers advanced usage patterns and real-world applications of IndoxRouter.</p>"},{"location":"examples/advanced/#streaming-responses","title":"Streaming Responses","text":"<p>Handle real-time streaming responses for better user experience:</p> <pre><code>from indoxrouter import Client\n\nclient = Client(api_key=\"your_api_key\")\n\n# Stream chat responses\nfor chunk in client.chat_stream(\n    messages=[\n        {\"role\": \"user\", \"content\": \"Write a short story about AI\"}\n    ],\n    model=\"openai/gpt-4o-mini\"\n):\n    if chunk.choices[0].delta.content:\n        print(chunk.choices[0].delta.content, end=\"\", flush=True)\n</code></pre>"},{"location":"examples/advanced/#batch-processing","title":"Batch Processing","text":"<p>Process multiple requests efficiently:</p> <pre><code>import asyncio\nfrom indoxrouter import AsyncClient\n\nasync def process_batch():\n    client = AsyncClient(api_key=\"your_api_key\")\n\n    prompts = [\n        \"Explain quantum computing\",\n        \"What is machine learning?\",\n        \"How does blockchain work?\"\n    ]\n\n    tasks = []\n    for prompt in prompts:\n        task = client.chat(\n            messages=[{\"role\": \"user\", \"content\": prompt}],\n            model=\"openai/gpt-4o-mini\"\n        )\n        tasks.append(task)\n\n    responses = await asyncio.gather(*tasks)\n\n    for i, response in enumerate(responses):\n        print(f\"Question {i+1}: {prompts[i]}\")\n        print(f\"Answer: {response['choices'][0]['message']['content']}\")\n        print(\"---\")\n\n# Run the batch processing\nasyncio.run(process_batch())\n</code></pre>"},{"location":"examples/advanced/#error-handling-and-retries","title":"Error Handling and Retries","text":"<p>Implement robust error handling:</p> <pre><code>import time\nfrom indoxrouter import Client, IndoxRouterError\n\ndef chat_with_retry(client, messages, model, max_retries=3):\n    for attempt in range(max_retries):\n        try:\n            response = client.chat(\n                messages=messages,\n                model=model\n            )\n            return response\n        except IndoxRouterError as e:\n            if attempt == max_retries - 1:\n                raise e\n\n            print(f\"Attempt {attempt + 1} failed: {e}\")\n            time.sleep(2 ** attempt)  # Exponential backoff\n\n    return None\n\n# Usage\nclient = Client(api_key=\"your_api_key\")\n\ntry:\n    response = chat_with_retry(\n        client,\n        [{\"role\": \"user\", \"content\": \"Hello!\"}],\n        \"openai/gpt-4o-mini\"\n    )\n    print(response['choices'][0]['message']['content'])\nexcept IndoxRouterError as e:\n    print(f\"Failed after all retries: {e}\")\n</code></pre>"},{"location":"examples/advanced/#custom-model-routing","title":"Custom Model Routing","text":"<p>Route requests to different models based on content:</p> <pre><code>from indoxrouter import Client\n\nclass SmartRouter:\n    def __init__(self, api_key):\n        self.client = Client(api_key=api_key)\n\n    def route_request(self, message):\n        # Analyze the request to choose the best model\n        content = message.lower()\n\n        if any(word in content for word in ['code', 'programming', 'function']):\n            return \"openai/gpt-4o\"  # Better for coding\n        elif any(word in content for word in ['creative', 'story', 'poem']):\n            return \"anthropic/claude-3-opus\"  # Better for creativity\n        elif len(content) &lt; 50:\n            return \"openai/gpt-3.5-turbo\"  # Fast for simple queries\n        else:\n            return \"openai/gpt-4o-mini\"  # Default choice\n\n    def chat(self, message):\n        model = self.route_request(message)\n\n        response = self.client.chat(\n            messages=[{\"role\": \"user\", \"content\": message}],\n            model=model\n        )\n\n        return {\n            \"model_used\": model,\n            \"response\": response['choices'][0]['message']['content']\n        }\n\n# Usage\nrouter = SmartRouter(api_key=\"your_api_key\")\n\nresult = router.chat(\"Write a Python function to sort a list\")\nprint(f\"Model used: {result['model_used']}\")\nprint(f\"Response: {result['response']}\")\n</code></pre>"},{"location":"examples/advanced/#function-calling","title":"Function Calling","text":"<p>Use function calling for structured outputs:</p> <pre><code>from indoxrouter import Client\nimport json\n\nclient = Client(api_key=\"your_api_key\")\n\n# Define available functions\nfunctions = [\n    {\n        \"name\": \"get_weather\",\n        \"description\": \"Get weather information for a location\",\n        \"parameters\": {\n            \"type\": \"object\",\n            \"properties\": {\n                \"location\": {\n                    \"type\": \"string\",\n                    \"description\": \"The city and state, e.g. San Francisco, CA\"\n                },\n                \"unit\": {\n                    \"type\": \"string\",\n                    \"enum\": [\"celsius\", \"fahrenheit\"],\n                    \"description\": \"Temperature unit\"\n                }\n            },\n            \"required\": [\"location\"]\n        }\n    }\n]\n\nresponse = client.chat(\n    messages=[\n        {\"role\": \"user\", \"content\": \"What's the weather like in New York?\"}\n    ],\n    model=\"openai/gpt-4o-mini\",\n    functions=functions,\n    function_call=\"auto\"\n)\n\n# Handle function call\nif response['choices'][0]['message'].get('function_call'):\n    function_call = response['choices'][0]['message']['function_call']\n    function_name = function_call['name']\n    function_args = json.loads(function_call['arguments'])\n\n    print(f\"Function called: {function_name}\")\n    print(f\"Arguments: {function_args}\")\n\n    # In a real application, you would call the actual function here\n    weather_result = f\"The weather in {function_args['location']} is sunny, 72\u00b0F\"\n\n    # Send the function result back to the model\n    follow_up = client.chat(\n        messages=[\n            {\"role\": \"user\", \"content\": \"What's the weather like in New York?\"},\n            response['choices'][0]['message'],\n            {\"role\": \"function\", \"name\": function_name, \"content\": weather_result}\n        ],\n        model=\"openai/gpt-4o-mini\"\n    )\n\n    print(follow_up['choices'][0]['message']['content'])\n</code></pre>"},{"location":"examples/basic/","title":"Basic Examples","text":"<p>This page provides some basic examples of using the IndoxRouter Client for common tasks.</p>"},{"location":"examples/basic/#chat-completion-example","title":"Chat Completion Example","text":"<pre><code>from indoxrouter import Client\n\n# Initialize the client\nwith Client(api_key=\"your_api_key\") as client:\n    # Generate a chat completion\n    response = client.chat(\n        messages=[\n            {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n            {\"role\": \"user\", \"content\": \"What is machine learning and why is it important?\"}\n        ],\n        model=\"openai/gpt-4o-mini\",\n        temperature=0.7\n    )\n\n    # Print the response\n    print(response[\"choices\"][0][\"message\"][\"content\"])\n</code></pre>"},{"location":"examples/basic/#text-completion-example","title":"Text Completion Example","text":"<pre><code>from indoxrouter import Client\n\n# Initialize the client\nwith Client(api_key=\"your_api_key\") as client:\n    # Generate a text completion\n    response = client.completion(\n        prompt=\"Write a short poem about artificial intelligence:\",\n        model=\"openai/gpt-4o-mini\",\n        max_tokens=100\n    )\n\n    # Print the response\n    print(response[\"choices\"][0][\"text\"])\n</code></pre>"},{"location":"examples/basic/#embedding-example","title":"Embedding Example","text":"<pre><code>from indoxrouter import Client\n\n# Initialize the client\nwith Client(api_key=\"your_api_key\") as client:\n    # Generate embeddings for multiple texts\n    response = client.embeddings(\n        text=[\n            \"Machine learning is a branch of artificial intelligence.\",\n            \"Natural language processing helps computers understand human language.\"\n        ],\n        model=\"openai/text-embedding-3-small\"\n    )\n\n    # Print the first few dimensions of each embedding\n    for i, embedding_data in enumerate(response[\"data\"]):\n        embedding = embedding_data[\"embedding\"]\n        print(f\"Embedding {i+1} (first 5 dimensions): {embedding[:5]}\")\n        print(f\"Embedding {i+1} dimensions: {len(embedding)}\")\n</code></pre>"},{"location":"examples/basic/#image-generation-example","title":"Image Generation Example","text":"<pre><code>from indoxrouter import Client\n\n# Initialize the client\nwith Client(api_key=\"your_api_key\") as client:\n    # Generate an image\n    response = client.images(\n        prompt=\"A futuristic city with flying cars and towering skyscrapers at sunset\",\n        model=\"openai/dall-e-3\",\n        size=\"1024x1024\"\n    )\n\n    # Print the image URL\n    print(f\"Generated image URL: {response['data'][0]['url']}\")\n</code></pre>"},{"location":"examples/basic/#model-information-example","title":"Model Information Example","text":"<pre><code>from indoxrouter import Client\nimport json\n\n# Initialize the client\nwith Client(api_key=\"your_api_key\") as client:\n    # Get information about all available models\n    providers = client.models()\n\n    # Print information about each provider\n    for provider in providers:\n        print(f\"Provider: {provider['id']} ({provider['name']})\")\n        print(f\"Description: {provider.get('description', 'No description')}\")\n        print(f\"Models available: {len(provider['models'])}\")\n        print(\"Model IDs:\")\n        for model in provider['models']:\n            print(f\"  - {model['id']}\")\n        print()\n\n    # Get details about a specific model\n    model_info = client.get_model_info(\"openai\", \"gpt-4o-mini\")\n    print(f\"Model: {model_info['id']}\")\n    print(f\"Description: {model_info.get('description', 'No description')}\")\n    print(f\"Capabilities: {', '.join(model_info.get('capabilities', []))}\")\n    print(f\"Max tokens: {model_info.get('max_tokens', 'Unknown')}\")\n</code></pre>"},{"location":"examples/openai-sdk/","title":"Using OpenAI SDK with IndoxRouter","text":"<p>You can use the familiar OpenAI SDK with IndoxRouter to access all supported providers through the OpenAI-compatible API. This is perfect if you're already using OpenAI SDK in your codebase.</p>"},{"location":"examples/openai-sdk/#setup","title":"Setup","text":"<p>Install the OpenAI SDK and configure it to use IndoxRouter:</p> <pre><code>pip install openai\n</code></pre> <pre><code>from openai import OpenAI\n\n# Configure OpenAI client to use IndoxRouter\nclient = OpenAI(\n    api_key=\"your_indoxrouter_api_key\",  # Your IndoxRouter API key\n    base_url=\"https://api.indoxrouter.com\"  # IndoxRouter base URL\n)\n</code></pre>"},{"location":"examples/openai-sdk/#chat-completions","title":"Chat Completions","text":"<p>Use any provider's models through the OpenAI SDK interface:</p>"},{"location":"examples/openai-sdk/#openai-models","title":"OpenAI Models","text":"<pre><code># GPT-4o\nresponse = client.chat.completions.create(\n    model=\"openai/gpt-4o\",\n    messages=[\n        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n        {\"role\": \"user\", \"content\": \"Explain quantum computing\"}\n    ],\n    temperature=0.7,\n    max_tokens=500\n)\n\nprint(response.choices[0].message.content)\n</code></pre>"},{"location":"examples/openai-sdk/#anthropic-models","title":"Anthropic Models","text":"<pre><code># Claude 3 Opus\nresponse = client.chat.completions.create(\n    model=\"anthropic/claude-3-opus-20240229\",\n    messages=[\n        {\"role\": \"user\", \"content\": \"Write a creative story about a time traveler\"}\n    ],\n    temperature=0.8,\n    max_tokens=800\n)\n\nprint(response.choices[0].message.content)\n</code></pre>"},{"location":"examples/openai-sdk/#google-models","title":"Google Models","text":"<pre><code># Gemini Pro\nresponse = client.chat.completions.create(\n    model=\"google/gemini-1.5-pro\",\n    messages=[\n        {\"role\": \"user\", \"content\": \"Analyze the impact of AI on healthcare\"}\n    ],\n    temperature=0.3,\n    max_tokens=1000\n)\n\nprint(response.choices[0].message.content)\n</code></pre>"},{"location":"examples/openai-sdk/#deepseek-models","title":"DeepSeek Models","text":"<pre><code># DeepSeek for coding\nresponse = client.chat.completions.create(\n    model=\"deepseek/deepseek-coder\",\n    messages=[\n        {\"role\": \"user\", \"content\": \"Write a Python function to calculate Fibonacci numbers\"}\n    ],\n    temperature=0,\n    max_tokens=300\n)\n\nprint(response.choices[0].message.content)\n</code></pre>"},{"location":"examples/openai-sdk/#streaming-responses","title":"Streaming Responses","text":"<p>Stream responses from any provider:</p> <pre><code># Streaming with Claude\nstream = client.chat.completions.create(\n    model=\"anthropic/claude-3-sonnet-20240229\",\n    messages=[\n        {\"role\": \"user\", \"content\": \"Tell me a long story about space exploration\"}\n    ],\n    stream=True,\n    max_tokens=1500\n)\n\nprint(\"Story: \", end=\"\")\nfor chunk in stream:\n    if chunk.choices[0].delta.content:\n        print(chunk.choices[0].delta.content, end=\"\", flush=True)\nprint()\n</code></pre>"},{"location":"examples/openai-sdk/#text-completions","title":"Text Completions","text":"<p>Use text completion models:</p> <pre><code># GPT-3.5 Turbo Instruct\nresponse = client.completions.create(\n    model=\"openai/gpt-3.5-turbo-instruct\",\n    prompt=\"The future of artificial intelligence is\",\n    max_tokens=200,\n    temperature=0.7\n)\n\nprint(response.choices[0].text)\n</code></pre>"},{"location":"examples/openai-sdk/#embeddings","title":"Embeddings","text":"<p>Generate embeddings using different providers:</p> <pre><code># OpenAI embeddings\nresponse = client.embeddings.create(\n    model=\"openai/text-embedding-3-small\",\n    input=\"Hello, world!\"\n)\n\nembedding = response.data[0].embedding\nprint(f\"Embedding dimensions: {len(embedding)}\")\nprint(f\"First 5 values: {embedding[:5]}\")\n\n# Multiple texts\nresponse = client.embeddings.create(\n    model=\"openai/text-embedding-3-large\",\n    input=[\n        \"Document 1: Introduction to machine learning\",\n        \"Document 2: Deep learning fundamentals\",\n        \"Document 3: Natural language processing\"\n    ]\n)\n\nfor i, embedding_obj in enumerate(response.data):\n    print(f\"Document {i+1} embedding dimensions: {len(embedding_obj.embedding)}\")\n</code></pre>"},{"location":"examples/openai-sdk/#image-generation","title":"Image Generation","text":"<p>Generate images using DALL-E or other providers:</p> <pre><code># DALL-E 3\nresponse = client.images.generate(\n    model=\"openai/dall-e-3\",\n    prompt=\"A futuristic cityscape with flying cars at sunset\",\n    size=\"1024x1024\",\n    quality=\"hd\",\n    style=\"vivid\",\n    n=1\n)\n\nimage_url = response.data[0].url\nprint(f\"Generated image: {image_url}\")\n\n# Get revised prompt\nif hasattr(response.data[0], 'revised_prompt'):\n    print(f\"Revised prompt: {response.data[0].revised_prompt}\")\n</code></pre>"},{"location":"examples/openai-sdk/#error-handling","title":"Error Handling","text":"<p>Handle errors using OpenAI SDK patterns:</p> <pre><code>from openai import OpenAI, RateLimitError, AuthenticationError\n\nclient = OpenAI(\n    api_key=\"your_indoxrouter_api_key\",\n    base_url=\"https://api.indoxrouter.com\"\n)\n\ntry:\n    response = client.chat.completions.create(\n        model=\"openai/gpt-4o\",\n        messages=[{\"role\": \"user\", \"content\": \"Hello\"}]\n    )\n    print(response.choices[0].message.content)\n\nexcept RateLimitError as e:\n    print(f\"Rate limit exceeded: {e}\")\n\nexcept AuthenticationError as e:\n    print(f\"Authentication failed: {e}\")\n\nexcept Exception as e:\n    print(f\"Request failed: {e}\")\n</code></pre>"},{"location":"examples/openai-sdk/#advanced-usage","title":"Advanced Usage","text":""},{"location":"examples/openai-sdk/#function-calling","title":"Function Calling","text":"<p>Use function calling with supported models:</p> <pre><code>import json\n\n# Define a function\ntools = [\n    {\n        \"type\": \"function\",\n        \"function\": {\n            \"name\": \"get_weather\",\n            \"description\": \"Get current weather for a location\",\n            \"parameters\": {\n                \"type\": \"object\",\n                \"properties\": {\n                    \"location\": {\n                        \"type\": \"string\",\n                        \"description\": \"The city and state, e.g. San Francisco, CA\"\n                    }\n                },\n                \"required\": [\"location\"]\n            }\n        }\n    }\n]\n\nresponse = client.chat.completions.create(\n    model=\"openai/gpt-4o\",\n    messages=[\n        {\"role\": \"user\", \"content\": \"What's the weather like in San Francisco?\"}\n    ],\n    tools=tools,\n    tool_choice=\"auto\"\n)\n\n# Check if the model wants to call a function\nif response.choices[0].message.tool_calls:\n    tool_call = response.choices[0].message.tool_calls[0]\n    function_name = tool_call.function.name\n    function_args = json.loads(tool_call.function.arguments)\n\n    print(f\"Model wants to call: {function_name}\")\n    print(f\"With arguments: {function_args}\")\n\n    # Simulate function execution\n    weather_result = {\"temperature\": \"72\u00b0F\", \"condition\": \"sunny\"}\n\n    # Send function result back\n    messages = [\n        {\"role\": \"user\", \"content\": \"What's the weather like in San Francisco?\"},\n        response.choices[0].message,\n        {\n            \"role\": \"tool\",\n            \"tool_call_id\": tool_call.id,\n            \"content\": json.dumps(weather_result)\n        }\n    ]\n\n    final_response = client.chat.completions.create(\n        model=\"openai/gpt-4o\",\n        messages=messages\n    )\n\n    print(final_response.choices[0].message.content)\n</code></pre>"},{"location":"examples/openai-sdk/#json-mode","title":"JSON Mode","text":"<p>Request structured JSON responses:</p> <pre><code>response = client.chat.completions.create(\n    model=\"openai/gpt-4o\",\n    messages=[\n        {\"role\": \"system\", \"content\": \"You are a helpful assistant designed to output JSON.\"},\n        {\"role\": \"user\", \"content\": \"Generate a JSON object with information about Paris, France\"}\n    ],\n    response_format={\"type\": \"json_object\"}\n)\n\n# Parse the JSON response\nimport json\ncity_info = json.loads(response.choices[0].message.content)\nprint(json.dumps(city_info, indent=2))\n</code></pre>"},{"location":"examples/openai-sdk/#reproducible-outputs","title":"Reproducible Outputs","text":"<p>Use seed for reproducible outputs (when supported):</p> <pre><code># Same prompt with same seed should give same result\nresponse1 = client.chat.completions.create(\n    model=\"openai/gpt-4o\",\n    messages=[{\"role\": \"user\", \"content\": \"Generate a random number\"}],\n    seed=12345,\n    temperature=0\n)\n\nresponse2 = client.chat.completions.create(\n    model=\"openai/gpt-4o\",\n    messages=[{\"role\": \"user\", \"content\": \"Generate a random number\"}],\n    seed=12345,\n    temperature=0\n)\n\nprint(f\"Response 1: {response1.choices[0].message.content}\")\nprint(f\"Response 2: {response2.choices[0].message.content}\")\nprint(f\"Same result: {response1.choices[0].message.content == response2.choices[0].message.content}\")\n</code></pre>"},{"location":"examples/openai-sdk/#model-comparison","title":"Model Comparison","text":"<p>Easily compare responses from different providers:</p> <pre><code>def compare_models(prompt, models):\n    \"\"\"Compare responses from different models.\"\"\"\n\n    results = {}\n\n    for model in models:\n        try:\n            response = client.chat.completions.create(\n                model=model,\n                messages=[{\"role\": \"user\", \"content\": prompt}],\n                temperature=0.7,\n                max_tokens=300\n            )\n\n            results[model] = {\n                'response': response.choices[0].message.content,\n                'finish_reason': response.choices[0].finish_reason,\n                'usage': response.usage._asdict() if response.usage else None\n            }\n\n        except Exception as e:\n            results[model] = {'error': str(e)}\n\n    return results\n\n# Compare different models\nprompt = \"Explain the concept of artificial general intelligence in simple terms.\"\nmodels = [\n    \"openai/gpt-4o-mini\",\n    \"anthropic/claude-3-haiku-20240307\",\n    \"google/gemini-1.5-flash\",\n    \"deepseek/deepseek-chat\"\n]\n\ncomparison = compare_models(prompt, models)\n\nfor model, result in comparison.items():\n    print(f\"\\n{'='*50}\")\n    print(f\"Model: {model}\")\n    print(f\"{'='*50}\")\n\n    if 'error' in result:\n        print(f\"Error: {result['error']}\")\n    else:\n        print(f\"Response: {result['response'][:200]}...\")\n        if result['usage']:\n            print(f\"Tokens: {result['usage']['total_tokens']}\")\n</code></pre>"},{"location":"examples/openai-sdk/#provider-specific-features","title":"Provider-Specific Features","text":""},{"location":"examples/openai-sdk/#anthropic-claude-features","title":"Anthropic Claude Features","text":"<pre><code># Claude with system message in messages (Anthropic style)\nresponse = client.chat.completions.create(\n    model=\"anthropic/claude-3-opus-20240229\",\n    messages=[\n        {\"role\": \"system\", \"content\": \"You are Claude, an AI assistant created by Anthropic.\"},\n        {\"role\": \"user\", \"content\": \"What are your capabilities?\"}\n    ],\n    max_tokens=500\n)\n</code></pre>"},{"location":"examples/openai-sdk/#google-gemini-features","title":"Google Gemini Features","text":"<pre><code># Gemini with longer context\nresponse = client.chat.completions.create(\n    model=\"google/gemini-1.5-pro\",\n    messages=[\n        {\"role\": \"user\", \"content\": \"Analyze this very long document...\" + \"x\" * 10000}\n    ],\n    max_tokens=1000\n)\n</code></pre>"},{"location":"examples/openai-sdk/#batch-processing","title":"Batch Processing","text":"<p>Process multiple requests efficiently:</p> <pre><code>import asyncio\nfrom openai import AsyncOpenAI\n\n# Use async client for better performance\nasync_client = AsyncOpenAI(\n    api_key=\"your_indoxrouter_api_key\",\n    base_url=\"https://api.indoxrouter.com\"\n)\n\nasync def process_batch(prompts, model=\"openai/gpt-4o-mini\"):\n    \"\"\"Process multiple prompts concurrently.\"\"\"\n\n    async def single_request(prompt):\n        try:\n            response = await async_client.chat.completions.create(\n                model=model,\n                messages=[{\"role\": \"user\", \"content\": prompt}],\n                temperature=0.7,\n                max_tokens=200\n            )\n            return {\n                'prompt': prompt,\n                'response': response.choices[0].message.content,\n                'success': True\n            }\n        except Exception as e:\n            return {\n                'prompt': prompt,\n                'error': str(e),\n                'success': False\n            }\n\n    # Process all prompts concurrently\n    tasks = [single_request(prompt) for prompt in prompts]\n    results = await asyncio.gather(*tasks)\n\n    return results\n\n# Example usage\nprompts = [\n    \"What is machine learning?\",\n    \"Explain quantum computing\",\n    \"How does blockchain work?\",\n    \"What is artificial intelligence?\"\n]\n\n# Run batch processing\nresults = asyncio.run(process_batch(prompts))\n\n# Display results\nfor result in results:\n    if result['success']:\n        print(f\"Q: {result['prompt']}\")\n        print(f\"A: {result['response'][:100]}...\")\n        print()\n    else:\n        print(f\"Failed: {result['prompt']} - {result['error']}\")\n</code></pre>"},{"location":"examples/openai-sdk/#migration-from-openai","title":"Migration from OpenAI","text":"<p>If you're migrating from direct OpenAI usage to IndoxRouter:</p>"},{"location":"examples/openai-sdk/#before-direct-openai","title":"Before (Direct OpenAI)","text":"<pre><code>from openai import OpenAI\n\nclient = OpenAI(api_key=\"sk-openai-key...\")\n\nresponse = client.chat.completions.create(\n    model=\"gpt-4\",\n    messages=[{\"role\": \"user\", \"content\": \"Hello\"}]\n)\n</code></pre>"},{"location":"examples/openai-sdk/#after-indoxrouter","title":"After (IndoxRouter)","text":"<pre><code>from openai import OpenAI\n\n# Only change: API key and base URL\nclient = OpenAI(\n    api_key=\"your_indoxrouter_api_key\",  # IndoxRouter API key\n    base_url=\"https://api.indoxrouter.com\"  # IndoxRouter base URL\n)\n\nresponse = client.chat.completions.create(\n    model=\"openai/gpt-4o\",  # Specify provider/model\n    messages=[{\"role\": \"user\", \"content\": \"Hello\"}]\n)\n\n# Access to other providers with same code!\nresponse = client.chat.completions.create(\n    model=\"anthropic/claude-3-opus-20240229\",  # Switch to Anthropic\n    messages=[{\"role\": \"user\", \"content\": \"Hello\"}]\n)\n</code></pre>"},{"location":"examples/openai-sdk/#best-practices","title":"Best Practices","text":""},{"location":"examples/openai-sdk/#1-handle-provider-specific-differences","title":"1. Handle Provider-Specific Differences","text":"<pre><code>def robust_chat_completion(model, messages, **kwargs):\n    \"\"\"Make chat completion with provider-specific handling.\"\"\"\n\n    try:\n        response = client.chat.completions.create(\n            model=model,\n            messages=messages,\n            **kwargs\n        )\n        return response\n\n    except Exception as e:\n        error_msg = str(e).lower()\n\n        # Handle common provider-specific errors\n        if \"context length\" in error_msg:\n            # Reduce max_tokens or message length\n            kwargs['max_tokens'] = min(kwargs.get('max_tokens', 4000), 2000)\n            return client.chat.completions.create(model=model, messages=messages, **kwargs)\n\n        elif \"rate limit\" in error_msg:\n            # Implement retry with backoff\n            import time\n            time.sleep(60)\n            return client.chat.completions.create(model=model, messages=messages, **kwargs)\n\n        else:\n            raise\n</code></pre>"},{"location":"examples/openai-sdk/#2-cost-tracking","title":"2. Cost Tracking","text":"<pre><code>def track_usage(response):\n    \"\"\"Track token usage and costs.\"\"\"\n\n    if hasattr(response, 'usage') and response.usage:\n        usage = response.usage\n\n        # Estimate cost (you'd get actual cost from IndoxRouter response headers)\n        print(f\"Tokens used: {usage.total_tokens}\")\n        print(f\"  Prompt: {usage.prompt_tokens}\")\n        print(f\"  Completion: {usage.completion_tokens}\")\n\n        # Note: Actual costs would be in IndoxRouter's response format\n        # when using the native client, not available in OpenAI SDK format\n\n# Usage\nresponse = client.chat.completions.create(\n    model=\"openai/gpt-4o-mini\",\n    messages=[{\"role\": \"user\", \"content\": \"Hello\"}]\n)\n\ntrack_usage(response)\n</code></pre>"},{"location":"examples/openai-sdk/#3-model-fallback","title":"3. Model Fallback","text":"<pre><code>def chat_with_fallback(messages, preferred_models, **kwargs):\n    \"\"\"Try multiple models as fallbacks.\"\"\"\n\n    for model in preferred_models:\n        try:\n            response = client.chat.completions.create(\n                model=model,\n                messages=messages,\n                **kwargs\n            )\n            print(f\"\u2705 Success with {model}\")\n            return response\n\n        except Exception as e:\n            print(f\"\u274c Failed with {model}: {e}\")\n            continue\n\n    raise Exception(\"All fallback models failed\")\n\n# Example usage\nfallback_models = [\n    \"openai/gpt-4o\",                      # Try premium first\n    \"openai/gpt-4o-mini\",                 # Fallback to cheaper\n    \"anthropic/claude-3-sonnet-20240229\", # Different provider\n    \"deepseek/deepseek-chat\"              # Most economical\n]\n\nresponse = chat_with_fallback(\n    messages=[{\"role\": \"user\", \"content\": \"Complex analysis task\"}],\n    preferred_models=fallback_models,\n    temperature=0.3,\n    max_tokens=1000\n)\n</code></pre> <p>This approach lets you use the familiar OpenAI SDK while getting access to all IndoxRouter providers and their cost tracking features!</p>"},{"location":"usage/basic-usage/","title":"Basic Usage","text":"<p>This page covers the fundamental patterns and concepts for using IndoxRouter effectively.</p>"},{"location":"usage/basic-usage/#client-initialization","title":"Client Initialization","text":"<pre><code>from indoxrouter import Client\n\n# Initialize with API key\nclient = Client(api_key=\"your_api_key\")\n</code></pre>"},{"location":"usage/basic-usage/#model-specification","title":"Model Specification","text":"<p>IndoxRouter uses a consistent format for specifying models: <code>provider/model_name</code>. This allows you to easily switch between providers while keeping your code structure the same.</p> <p>Examples:</p> <ul> <li><code>openai/gpt-4o-mini</code></li> <li><code>anthropic/claude-3-sonnet-20240229</code></li> <li><code>mistral/mistral-large-latest</code></li> <li><code>google/gemini-1.5-pro</code></li> </ul>"},{"location":"usage/basic-usage/#common-parameters","title":"Common Parameters","text":"<p>All API methods accept a set of common parameters:</p> <ul> <li><code>model</code>: The model to use in format <code>provider/model_name</code></li> <li><code>temperature</code>: Controls randomness (0-1). Lower values = more deterministic. Default is 0.7</li> <li><code>max_tokens</code>: Maximum number of tokens to generate</li> </ul> <p>Additional parameters specific to each provider can be passed as keyword arguments.</p>"},{"location":"usage/basic-usage/#response-structure","title":"Response Structure","text":"<p>Responses from the API closely follow the OpenAI API response format, with some additions for consistency across providers:</p> <pre><code>{'request_id': 'b881942c-e21d-4f9d-ad82-47344945c642',\n 'created_at': '2025-06-15T09:53:26.130868',\n 'duration_ms': 1737.612247467041,\n 'provider': 'openai',\n 'model': 'gpt-4o-mini',\n 'success': True,\n 'message': '',\n 'usage': {'tokens_prompt': 24,\n  'tokens_completion': 7,\n  'tokens_total': 31,\n  'cost': 7.8e-06,\n  'latency': 1.629077672958374,\n  'timestamp': '2025-06-15T09:53:26.114626',\n  'cache_read_tokens': 0,\n  'cache_write_tokens': 0,\n  'reasoning_tokens': 0,\n  'web_search_count': 0,\n  'request_count': 1,\n  'cost_breakdown': {'input_tokens': 3.6e-06,\n   'output_tokens': 4.2e-06,\n   'cache_read': 0.0,\n   'cache_write': 0.0,\n   'reasoning': 0.0,\n   'web_search': 0.0,\n   'request': 0.0}},\n 'raw_response': None,\n 'data': 'The capital of France is Paris.',\n 'finish_reason': None}\n</code></pre>"},{"location":"usage/basic-usage/#error-handling","title":"Error Handling","text":"<p>The client provides a set of specific exception classes for different error types:</p> <ul> <li><code>AuthenticationError</code>: Issues with API key or authentication</li> <li><code>ProviderNotFoundError</code>: The requested provider doesn't exist</li> <li><code>ModelNotFoundError</code>: The requested model doesn't exist</li> <li><code>InvalidParametersError</code>: The provided parameters are invalid</li> <li><code>RateLimitError</code>: The rate limit has been exceeded</li> <li><code>ProviderError</code>: An error occurred with the provider's service</li> <li><code>InsufficientCreditsError</code>: Not enough credits to complete the request</li> <li><code>NetworkError</code>: Network connectivity issues</li> </ul> <p>Example error handling:</p> <pre><code>from indoxrouter import Client, ModelNotFoundError, ProviderError\n\ntry:\n    client = Client(api_key=\"your_api_key\")\n    response = client.chat(\n        messages=[{\"role\": \"user\", \"content\": \"Hello\"}],\n        model=\"nonexistent-provider/nonexistent-model\"\n    )\nexcept ModelNotFoundError as e:\n    print(f\"Model not found: {e}\")\nexcept ProviderError as e:\n    print(f\"Provider error: {e}\")\n</code></pre>"},{"location":"usage/basic-usage/#debugging","title":"Debugging","text":"<p>If you're experiencing issues, you can enable debug logging:</p> <pre><code>import logging\nfrom indoxrouter import Client\n\n# Enable debug logging\nlogging.basicConfig(level=logging.DEBUG)\n\nclient = Client(api_key=\"your_api_key\")\nclient.enable_debug()  # This enables additional debugging information\n\n# Your code here\n</code></pre> <p>You can also use the <code>diagnose_request</code> method to get detailed information about a request without actually sending it:</p> <pre><code>diagnostic_info = client.diagnose_request(\n    \"chat/completions\",\n    {\n        \"messages\": [{\"role\": \"user\", \"content\": \"Hello\"}],\n        \"model\": \"openai/gpt-4o-mini\"\n    }\n)\nprint(diagnostic_info)\n</code></pre>"},{"location":"usage/basic-usage/#next-steps","title":"Next Steps","text":"<p>Now that you understand the basics, check out the detailed guides for each capability:</p> <ul> <li>Chat Completions</li> <li>Text Completions</li> <li>Embeddings</li> <li>Image Generation</li> </ul>"},{"location":"usage/chat/","title":"Chat Completions","text":"<p>Chat completions are the primary way to interact with conversational AI models like GPT-4, Claude, and Gemini. This guide covers how to use the chat completions feature of the IndoxRouter Client.</p>"},{"location":"usage/chat/#basic-usage","title":"Basic Usage","text":"<p>The simplest way to use chat completions is with the <code>chat()</code> method:</p> <pre><code>from indoxrouter import Client\n\nclient = Client(api_key=\"your_api_key\")\n\nresponse = client.chat(\n    messages=[\n        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n        {\"role\": \"user\", \"content\": \"Tell me a joke about programming.\"}\n    ],\n    model=\"openai/gpt-4o-mini\"\n)\n\nprint(response[\"data\"])\n</code></pre>"},{"location":"usage/chat/#message-format","title":"Message Format","text":"<p>The <code>messages</code> parameter is a list of dictionaries, each with <code>role</code> and <code>content</code> keys:</p> <ul> <li><code>role</code>: Can be one of \"system\", \"user\", \"assistant\", or \"function\"</li> <li><code>content</code>: The content of the message</li> </ul> <p>Example message formats:</p> <pre><code># System message (instructions to the AI)\n{\"role\": \"system\", \"content\": \"You are a helpful assistant.\"}\n\n# User message (the user's input)\n{\"role\": \"user\", \"content\": \"What's the weather like today?\"}\n\n# Assistant message (previous responses from the assistant)\n{\"role\": \"assistant\", \"content\": \"I don't have access to current weather information.\"}\n\n# Function message (for function calling, when available)\n{\"role\": \"function\", \"name\": \"get_weather\", \"content\": '{\"temperature\": 72, \"condition\": \"sunny\"}'}\n</code></pre>"},{"location":"usage/chat/#model-selection","title":"Model Selection","text":"<p>You can specify different models using the <code>provider/model_name</code> format:</p> <pre><code># OpenAI\nresponse = client.chat(\n    messages=[{\"role\": \"user\", \"content\": \"Hello\"}],\n    model=\"openai/gpt-4o-mini\"\n)\n\n# Anthropic\nresponse = client.chat(\n    messages=[{\"role\": \"user\", \"content\": \"Hello\"}],\n    model=\"anthropic/claude-3-sonnet-20240229\"\n)\n\n# Google\nresponse = client.chat(\n    messages=[{\"role\": \"user\", \"content\": \"Hello\"}],\n    model=\"google/gemini-1.5-pro\"\n)\n\n# Mistral\nresponse = client.chat(\n    messages=[{\"role\": \"user\", \"content\": \"Hello\"}],\n    model=\"mistral/mistral-large-latest\"\n)\n</code></pre>"},{"location":"usage/chat/#common-parameters","title":"Common Parameters","text":"<p>The chat method accepts several parameters to control the generation:</p> <pre><code>response = client.chat(\n    messages=[{\"role\": \"user\", \"content\": \"Write a poem about AI.\"}],\n    model=\"openai/gpt-4o-mini\",\n    temperature=0.7,  # Controls randomness (0-1)\n    max_tokens=100,   # Maximum number of tokens to generate\n    stream=False,     # Whether to stream the response\n)\n</code></pre>"},{"location":"usage/chat/#streaming-responses","title":"Streaming Responses","text":"<p>For long responses, you might want to stream the response to get it piece by piece:</p> <pre><code>print(\"Streaming response:\")\nfor chunk in client.chat(\n    messages=[\n        {\"role\": \"user\", \"content\": \"Tell me a story about a robot in 5 sentences.\"}\n    ],\n    model=\"mistral/ministral-8b-latest\",\n    stream=True\n):\n    if isinstance(chunk, dict) and \"data\" in chunk:\n        print(chunk[\"data\"], end=\"\", flush=True)\n    else:\n        print(chunk, end=\"\", flush=True)\nprint(\"\\nStreaming complete!\")\n</code></pre>"},{"location":"usage/chat/#managing-conversations","title":"Managing Conversations","text":"<p>For multi-turn conversations, you'll need to keep track of the message history:</p> <pre><code># Initialize the conversation with a system message\nmessages = [\n    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"}\n]\n\n# First user message\nmessages.append({\"role\": \"user\", \"content\": \"Hello, who are you?\"})\nresponse = client.chat(messages=messages, model=\"openai/gpt-4o-mini\")\nassistant_response = response[\"choices\"][0][\"message\"][\"content\"]\nmessages.append({\"role\": \"assistant\", \"content\": assistant_response})\nprint(f\"Assistant: {assistant_response}\")\n\n# Second user message\nmessages.append({\"role\": \"user\", \"content\": \"What can you help me with?\"})\nresponse = client.chat(messages=messages, model=\"openai/gpt-4o-mini\")\nassistant_response = response[\"choices\"][0][\"message\"][\"content\"]\nmessages.append({\"role\": \"assistant\", \"content\": assistant_response})\nprint(f\"Assistant: {assistant_response}\")\n</code></pre>"},{"location":"usage/chat/#response-format","title":"Response Format","text":"<p>The response from the chat method follows this structure:</p> <pre><code>{'request_id': 'b881942c-e21d-4f9d-ad82-47344945c642',\n 'created_at': '2025-06-15T09:53:26.130868',\n 'duration_ms': 1737.612247467041,\n 'provider': 'openai',\n 'model': 'gpt-4o-mini',\n 'success': True,\n 'message': '',\n 'usage': {'tokens_prompt': 24,\n  'tokens_completion': 7,\n  'tokens_total': 31,\n  'cost': 7.8e-06,\n  'latency': 1.629077672958374,\n  'timestamp': '2025-06-15T09:53:26.114626',\n  'cache_read_tokens': 0,\n  'cache_write_tokens': 0,\n  'reasoning_tokens': 0,\n  'web_search_count': 0,\n  'request_count': 1,\n  'cost_breakdown': {'input_tokens': 3.6e-06,\n   'output_tokens': 4.2e-06,\n   'cache_read': 0.0,\n   'cache_write': 0.0,\n   'reasoning': 0.0,\n   'web_search': 0.0,\n   'request': 0.0}},\n 'raw_response': None,\n 'data': 'The capital of France is Paris.',\n 'finish_reason': None}\n</code></pre>"},{"location":"usage/completions/","title":"Text Completions","text":"<p>IndoxRouter supports text completion endpoints for generating text based on prompts.</p>"},{"location":"usage/completions/#basic-text-completion","title":"Basic Text Completion","text":"<pre><code>from indoxrouter import Client\n\nclient = Client(api_key=\"your_api_key\")\n\n# Generate text completion\nresponse = client.completions(\n    prompt=\"The future of artificial intelligence is\",\n    model=\"openai/gpt-3.5-turbo-instruct\",\n    max_tokens=100,\n    temperature=0.7\n)\n\nprint(\"Response:\", response[\"data\"])\n</code></pre>"},{"location":"usage/completions/#parameters","title":"Parameters","text":"<ul> <li><code>prompt</code>: The text prompt to complete</li> <li><code>model</code>: The model to use for completion</li> <li><code>max_tokens</code>: Maximum number of tokens to generate</li> <li><code>temperature</code>: Controls randomness (0.0 to 2.0)</li> <li><code>top_p</code>: Controls diversity via nucleus sampling</li> <li><code>frequency_penalty</code>: Penalizes frequent tokens</li> <li><code>presence_penalty</code>: Penalizes new tokens</li> </ul>"},{"location":"usage/completions/#example-use-cases","title":"Example Use Cases","text":""},{"location":"usage/completions/#creative-writing","title":"Creative Writing","text":"<pre><code>response = client.completions(\n    prompt=\"Once upon a time in a magical forest,\",\n    model=\"openai/gpt-3.5-turbo-instruct\",\n    max_tokens=200,\n    temperature=0.9\n)\n</code></pre>"},{"location":"usage/completions/#code-generation","title":"Code Generation","text":"<pre><code>response = client.completions(\n    prompt=\"# Python function to calculate fibonacci numbers\\ndef fibonacci(n):\",\n    model=\"openai/gpt-3.5-turbo-instruct\",\n    max_tokens=150,\n    temperature=0.2\n)\n</code></pre>"},{"location":"usage/embeddings/","title":"Embeddings","text":"<p>Embeddings are vector representations of text that capture semantic meaning, making them useful for similarity search, clustering, classification, and retrieval applications. This guide covers how to use the embeddings feature of IndoxRouter.</p>"},{"location":"usage/embeddings/#basic-usage","title":"Basic Usage","text":"<p>The simplest way to generate embeddings is with the <code>embeddings()</code> method:</p> <pre><code>from indoxrouter import Client\n\nclient = Client(api_key=\"your_api_key\")\n\n# Generate embeddings for a single text\nresponse = client.embeddings(\n    text=\"Machine learning is a field of study that gives computers the ability to learn without being explicitly programmed.\",\n    model=\"openai/text-embedding-3-small\"\n)\n\n# Print the embedding dimensions\nprint(f\"Embedding dimensions: {len(response['data'][0]['embedding'])}\")\nprint(f\"First few dimensions: {response['data'][0]['embedding'][:5]}\")\n</code></pre>"},{"location":"usage/embeddings/#processing-multiple-texts","title":"Processing Multiple Texts","text":"<p>You can generate embeddings for multiple texts in a single request by passing a list of strings:</p> <pre><code>from indoxrouter import Client\n\nclient = Client(api_key=\"your_api_key\")\n\n# Generate embeddings for multiple texts\nresponse = client.embeddings(\n    text=[\n        \"Artificial intelligence is revolutionizing industries worldwide.\",\n        \"Natural language processing helps computers understand human language.\",\n        \"Machine learning algorithms improve with more training data.\"\n    ],\n    model=\"openai/text-embedding-3-small\"\n)\n\n# Print information about the embeddings\nfor i, item in enumerate(response[\"data\"]):\n    embedding = item[\"embedding\"]\n    print(f\"Text {i+1}: Dimensions: {len(embedding)}\")\n</code></pre>"},{"location":"usage/embeddings/#model-selection","title":"Model Selection","text":"<p>You can select different embedding models from various providers:</p> <pre><code># OpenAI\nopenai_response = client.embeddings(\n    text=\"Example text for embedding\",\n    model=\"openai/text-embedding-3-small\"\n)\n\n# OpenAI larger model\nopenai_large_response = client.embeddings(\n    text=\"Example text for embedding\",\n    model=\"openai/text-embedding-3-large\"\n)\n\n# Google\ngoogle_response = client.embeddings(\n    text=\"Example text for embedding\",\n    model=\"google/text-embedding-gecko\"\n)\n\n# Mistral\nmistral_response = client.embeddings(\n    text=\"Example text for embedding\",\n    model=\"mistral/mistral-embed\"\n)\n</code></pre>"},{"location":"usage/embeddings/#response-format","title":"Response Format","text":"<p>The response from the embeddings method follows this structure:</p> <pre><code>{\n    \"id\": \"embd-123456789\",\n    \"object\": \"embedding\",\n    \"created\": 1684936116,\n    \"model\": \"openai/text-embedding-3-small\",\n    \"data\": [\n        {\n            \"embedding\": [0.002345, -0.012345, 0.123456, ...],  # Vector of n dimensions\n            \"index\": 0\n        },\n        # More items if multiple texts were provided\n    ],\n    \"usage\": {\n        \"prompt_tokens\": 10,\n        \"total_tokens\": 10\n    }\n}\n</code></pre>"},{"location":"usage/embeddings/#working-with-embeddings","title":"Working with Embeddings","text":""},{"location":"usage/embeddings/#calculating-similarity","title":"Calculating Similarity","text":"<p>Once you have embeddings, you can calculate similarity between them using cosine similarity:</p> <pre><code>import numpy as np\nfrom scipy.spatial.distance import cosine\n\ndef cosine_similarity(a, b):\n    return 1 - cosine(a, b)\n\n# Get embeddings for two texts\nresponse = client.embeddings(\n    text=[\n        \"The weather is quite nice today.\",\n        \"Today's weather is pleasant.\"\n    ],\n    model=\"openai/text-embedding-3-small\"\n)\n\n# Extract the embedding vectors\nembedding1 = response[\"data\"][0][\"embedding\"]\nembedding2 = response[\"data\"][1][\"embedding\"]\n\n# Calculate similarity\nsimilarity = cosine_similarity(embedding1, embedding2)\nprint(f\"Similarity: {similarity:.4f}\")  # Higher value means more similar\n</code></pre>"},{"location":"usage/embeddings/#building-a-simple-rag-system","title":"Building a Simple RAG System","text":"<p>Here's a basic example of using embeddings for a simple retrieval-augmented generation (RAG) system:</p> <pre><code>import numpy as np\nfrom sklearn.metrics.pairwise import cosine_similarity\n\n# Sample knowledge base\ndocuments = [\n    \"Machine learning is a field of study that gives computers the ability to learn without being explicitly programmed.\",\n    \"Natural language processing is a subfield of linguistics, computer science, and artificial intelligence.\",\n    \"Neural networks are computing systems inspired by the biological neural networks in animal brains.\",\n    \"Deep learning is part of a broader family of machine learning methods based on artificial neural networks.\",\n    \"Reinforcement learning is the training of machine learning models to make a sequence of decisions.\"\n]\n\nclient = Client(api_key=\"your_api_key\")\n\n# Step 1: Generate embeddings for our knowledge base\ndocs_response = client.embeddings(\n    text=documents,\n    model=\"openai/text-embedding-3-small\"\n)\n\n# Extract embeddings\ndoc_embeddings = np.array([item[\"embedding\"] for item in docs_response[\"data\"]])\n\n# Step 2: Process a query\nquery = \"How do computers learn without explicit programming?\"\n\n# Generate embedding for the query\nquery_response = client.embeddings(\n    text=query,\n    model=\"openai/text-embedding-3-small\"\n)\nquery_embedding = np.array(query_response[\"data\"][0][\"embedding\"])\n\n# Step 3: Find the most similar document\nsimilarities = cosine_similarity([query_embedding], doc_embeddings)[0]\nmost_similar_index = np.argmax(similarities)\nmost_similar_doc = documents[most_similar_index]\n\nprint(f\"Query: {query}\")\nprint(f\"Most relevant document: {most_similar_doc}\")\nprint(f\"Similarity score: {similarities[most_similar_index]:.4f}\")\n\n# Step 4: Generate an answer using the most relevant document as context\nresponse = client.chat(\n    messages=[\n        {\"role\": \"system\", \"content\": \"You are a helpful assistant. Use the provided context to answer the question.\"},\n        {\"role\": \"user\", \"content\": f\"Context: {most_similar_doc}\\n\\nQuestion: {query}\"}\n    ],\n    model=\"openai/gpt-4o-mini\"\n)\n\nprint(\"\\nGenerated Answer:\")\nprint(response[\"choices\"][0][\"message\"][\"content\"])\n</code></pre>"},{"location":"usage/embeddings/#advanced-usage","title":"Advanced Usage","text":""},{"location":"usage/embeddings/#chunking-large-documents","title":"Chunking Large Documents","text":"<p>For practical applications, you'll often need to chunk large documents before creating embeddings:</p> <pre><code>def chunk_text(text, chunk_size=1000, overlap=100):\n    \"\"\"Split text into overlapping chunks.\"\"\"\n    chunks = []\n    for i in range(0, len(text), chunk_size - overlap):\n        chunk = text[i:i + chunk_size]\n        if len(chunk) &lt; 100:  # Skip very small chunks at the end\n            continue\n        chunks.append(chunk)\n    return chunks\n\n# Example usage\nlarge_document = \"\"\"\n[Your long document text here...]\n\"\"\"\n\nchunks = chunk_text(large_document)\nprint(f\"Document split into {len(chunks)} chunks\")\n\n# Generate embeddings for each chunk\nchunk_response = client.embeddings(\n    text=chunks,\n    model=\"openai/text-embedding-3-small\"\n)\n\nchunk_embeddings = [item[\"embedding\"] for item in chunk_response[\"data\"]]\n</code></pre>"},{"location":"usage/embeddings/#storing-embeddings","title":"Storing Embeddings","text":"<p>For production applications, you would typically store embeddings in a vector database:</p> <pre><code># Pseudocode for storing embeddings in a vector database\n# Replace with actual implementation for your chosen database\n\n# Generate embeddings\nresponse = client.embeddings(\n    text=documents,\n    model=\"openai/text-embedding-3-small\"\n)\n\n# Store in vector database\nfor i, doc in enumerate(documents):\n    vector = response[\"data\"][i][\"embedding\"]\n    doc_id = f\"doc_{i}\"\n    vector_db.insert(\n        id=doc_id,\n        vector=vector,\n        metadata={\"text\": doc}\n    )\n</code></pre>"},{"location":"usage/embeddings/#best-practices","title":"Best Practices","text":"<ol> <li>Choose the right model: Different embedding models have different dimensions and performance characteristics</li> <li>Normalize text: Clean and normalize text before generating embeddings</li> <li>Chunk large documents: Split large texts into smaller chunks</li> <li>Cache embeddings: Store embeddings to avoid regenerating them for the same content</li> <li>Use appropriate similarity metrics: Cosine similarity is common, but other metrics might be better for specific use cases</li> <li>Consider dimensionality reduction: For very large collections, consider techniques like PCA to reduce embedding dimensions</li> </ol>"},{"location":"usage/images/","title":"Image Generation","text":"<p>IndoxRouter provides a unified interface for generating images from text prompts across various AI providers. This guide covers how to use the image generation capabilities.</p>"},{"location":"usage/images/#basic-usage","title":"Basic Usage","text":"<p>The simplest way to generate images is with the <code>images()</code> method:</p> <pre><code>from indoxrouter import Client\n\nclient = Client(api_key=\"your_api_key\")\n\n# Generate an image\nresponse = client.images(\n    prompt=\"A serene mountain landscape with a lake at sunset\",\n    model=\"openai/dall-e-3\"\n)\n\n# Get the image URL\nimage_url = response[\"data\"][0][\"url\"]\nprint(f\"Generated image URL: {image_url}\")\n</code></pre>"},{"location":"usage/images/#model-selection","title":"Model Selection","text":"<p>You can use different image generation models from various providers:</p> <pre><code># OpenAI DALL-E 3\ndalle3_response = client.images(\n    prompt=\"A futuristic city with flying cars\",\n    model=\"openai/dall-e-3\"\n)\n\n# OpenAI DALL-E 2\ndalle2_response = client.images(\n    prompt=\"A futuristic city with flying cars\",\n    model=\"openai/dall-e-2\"\n)\n\n# Stability AI\nstability_response = client.images(\n    prompt=\"A futuristic city with flying cars\",\n    model=\"stability/stable-diffusion-xl\"\n)\n</code></pre>"},{"location":"usage/images/#image-parameters","title":"Image Parameters","text":"<p>The image generation method accepts several parameters to control the output:</p> <pre><code>response = client.images(\n    prompt=\"A photorealistic portrait of a cyberpunk character\",\n    model=\"openai/dall-e-3\",\n    size=\"1024x1024\",  # Image dimensions\n    n=1,               # Number of images to generate\n    quality=\"hd\",      # Image quality (standard or hd)\n    style=\"vivid\"      # Image style (vivid or natural)\n)\n</code></pre>"},{"location":"usage/images/#common-parameters","title":"Common Parameters","text":"<ul> <li><code>prompt</code>: The text description of the image to generate</li> <li><code>model</code>: The model to use in format <code>provider/model_name</code></li> <li><code>size</code>: Image dimensions in format <code>widthxheight</code> (e.g., \"1024x1024\", \"512x512\")</li> <li><code>n</code>: Number of images to generate</li> <li><code>quality</code>: Image quality level (model dependent)</li> <li><code>style</code>: Image style (model dependent)</li> </ul>"},{"location":"usage/images/#response-format","title":"Response Format","text":"<p>The response from the images method follows this structure:</p> <pre><code>{\n    \"created\": 1684939249,\n    \"data\": [\n        {\n            \"url\": \"https://example.com/generated-image-123.png\",\n            \"revised_prompt\": \"A serene mountain landscape with a crystal-clear lake reflecting the sunset colors, surrounded by pine trees and snow-capped peaks\",\n            \"index\": 0\n        }\n        # More items if n &gt; 1\n    ]\n}\n</code></pre>"},{"location":"usage/images/#saving-generated-images","title":"Saving Generated Images","text":"<p>To save the generated images locally:</p> <pre><code>import requests\nimport os\n\ndef save_image(url, filename):\n    \"\"\"Download and save an image from URL to a local file.\"\"\"\n    response = requests.get(url)\n    if response.status_code == 200:\n        with open(filename, 'wb') as f:\n            f.write(response.content)\n        print(f\"Image saved as {filename}\")\n    else:\n        print(f\"Failed to download image: {response.status_code}\")\n\n# Generate an image\nresponse = client.images(\n    prompt=\"A colorful abstract painting with geometric shapes\",\n    model=\"openai/dall-e-3\"\n)\n\n# Save each generated image\nos.makedirs(\"generated_images\", exist_ok=True)\nfor i, item in enumerate(response[\"data\"]):\n    image_url = item[\"url\"]\n    filename = f\"generated_images/image_{i}.png\"\n    save_image(image_url, filename)\n</code></pre>"},{"location":"usage/images/#advanced-usage","title":"Advanced Usage","text":""},{"location":"usage/images/#generating-variations","title":"Generating Variations","text":"<p>Some models support generating variations of existing images. This feature may be added in the future.</p>"},{"location":"usage/images/#image-to-image-generation","title":"Image-to-Image Generation","text":"<p>Some models support image-to-image generation, where an input image is transformed based on a prompt. This feature may be added in the future.</p>"},{"location":"usage/images/#examples","title":"Examples","text":""},{"location":"usage/images/#detailed-art-generation","title":"Detailed Art Generation","text":"<pre><code># Generate a detailed art piece\nart_response = client.images(\n    prompt=(\n        \"An intricate fantasy illustration of an ancient library filled with \"\n        \"magical books, glowing orbs of light floating through the air, towering \"\n        \"bookshelves reaching into a starry sky ceiling, and a wizard studying \"\n        \"at an ornate desk\"\n    ),\n    model=\"openai/dall-e-3\",\n    size=\"1024x1024\",\n    quality=\"hd\",\n    style=\"vivid\"\n)\n\nprint(f\"Image URL: {art_response['data'][0]['url']}\")\nprint(f\"Revised prompt: {art_response['data'][0].get('revised_prompt', 'Not available')}\")\n</code></pre>"},{"location":"usage/images/#product-visualization","title":"Product Visualization","text":"<pre><code># Generate a product visualization\nproduct_response = client.images(\n    prompt=(\n        \"A professional product photography shot of a minimalist smartwatch \"\n        \"with a sleek black band and circular face displaying a digital time \"\n        \"against a clean white background, studio lighting\"\n    ),\n    model=\"openai/dall-e-3\",\n    size=\"1024x1024\",\n    quality=\"hd\",\n    style=\"natural\"\n)\n\nprint(f\"Product image URL: {product_response['data'][0]['url']}\")\n</code></pre>"},{"location":"usage/images/#architectural-concept","title":"Architectural Concept","text":"<pre><code># Generate an architectural concept\narchitecture_response = client.images(\n    prompt=(\n        \"A modern, sustainable treehouse design integrated into a forest canopy, \"\n        \"featuring large windows, solar panels, natural wood materials, \"\n        \"and connected walkways between tree platforms\"\n    ),\n    model=\"openai/dall-e-3\",\n    size=\"1024x1024\"\n)\n\nprint(f\"Architecture concept URL: {architecture_response['data'][0]['url']}\")\n</code></pre>"},{"location":"usage/images/#best-practices","title":"Best Practices","text":"<ol> <li>Be detailed and specific: The more detailed your prompt, the better the results</li> <li>Consider the style: Specify the artistic style, medium, lighting, and mood</li> <li>Experiment with parameters: Try different models, sizes, and quality settings</li> <li>Use appropriate models: Choose models based on your needs and budget</li> <li>Review revised prompts: Some models provide revised prompts that can help you understand how your prompt was interpreted</li> </ol>"},{"location":"usage/images/#limitations","title":"Limitations","text":"<ul> <li>Image generation capabilities may vary depending on the provider and model</li> <li>Some providers may have content filters that restrict certain types of content</li> <li>Image quality and adherence to the prompt varies across different models</li> <li>Costs for image generation can be higher than text generation</li> </ul>"},{"location":"usage/rate-limits/","title":"Rate Limits","text":"<p>IndoxRouter implements a three-tier rate limiting system to ensure fair usage and optimal performance. Understanding these limits helps you optimize your application's AI usage.</p>"},{"location":"usage/rate-limits/#rate-limit-tiers","title":"Rate Limit Tiers","text":"<p>IndoxRouter has three subscription tiers with different rate limits:</p> <pre><code># Rate limits by tier\nRATE_LIMITS = {\n    \"free\": {\n        \"requests_per_minute\": 10,\n        \"tokens_per_hour\": 10000\n    },\n    \"standard\": {\n        \"requests_per_minute\": 60,\n        \"tokens_per_hour\": 100000\n    },\n    \"enterprise\": {\n        \"requests_per_minute\": 500,\n        \"tokens_per_hour\": 1000000\n    }\n}\n</code></pre>"},{"location":"usage/rate-limits/#tier-comparison","title":"Tier Comparison","text":"Tier Requests/Minute Tokens/Hour Best For Free 10 10,000 Testing, prototyping, learning Standard 60 100,000 Production apps, small businesses Enterprise 500 1,000,000 High-volume applications, enterprises"},{"location":"usage/rate-limits/#what-counts-toward-limits","title":"What Counts Toward Limits","text":""},{"location":"usage/rate-limits/#request-limits","title":"Request Limits","text":"<p>Every API call counts as one request:</p> <ul> <li><code>client.chat()</code> = 1 request</li> <li><code>client.completions()</code> = 1 request</li> <li><code>client.embeddings()</code> = 1 request</li> <li><code>client.images()</code> = 1 request</li> <li><code>client.models()</code> = 1 request (but doesn't count toward token limits)</li> </ul>"},{"location":"usage/rate-limits/#token-limits","title":"Token Limits","text":"<p>Tokens are counted for text-based operations:</p> <ul> <li>Chat &amp; Completions: Input tokens + output tokens</li> <li>Embeddings: Input tokens only</li> <li>Images: No tokens counted (separate limits may apply)</li> <li>Models/Info calls: No tokens counted</li> </ul>"},{"location":"usage/rate-limits/#rate-limit-headers","title":"Rate Limit Headers","text":"<p>Every response includes rate limit information in the headers and response:</p> <pre><code>response = client.chat(\n    messages=[{\"role\": \"user\", \"content\": \"Hello\"}],\n    model=\"openai/gpt-4o-mini\"\n)\n\n# Rate limit info is included in response metadata\nprint(f\"Request ID: {response['request_id']}\")\nprint(f\"Duration: {response['duration_ms']}ms\")\n\n# Check your current usage\nusage = client.get_usage()\nprint(f\"Remaining credits: ${usage['remaining_credits']}\")\n</code></pre>"},{"location":"usage/rate-limits/#handling-rate-limits","title":"Handling Rate Limits","text":""},{"location":"usage/rate-limits/#rate-limit-errors","title":"Rate Limit Errors","text":"<p>When you exceed rate limits, you'll receive an error response:</p> <pre><code>{\n    'success': False,\n    'error': 'RateLimitError',\n    'message': 'Rate limit exceeded: 10 requests per minute',\n    'status_code': 429,\n    'request_id': 'req_rate_limit_123',\n    'details': {\n        'limit_type': 'requests_per_minute',\n        'limit': 10,\n        'reset_time': '2025-05-19T10:35:00Z',\n        'retry_after': 45  # seconds\n    }\n}\n</code></pre>"},{"location":"usage/rate-limits/#error-handling-example","title":"Error Handling Example","text":"<pre><code>from indoxrouter import Client, RateLimitError\nimport time\n\nclient = Client(api_key=\"your_api_key\")\n\ndef make_request_with_retry(messages, model, max_retries=3):\n    \"\"\"Make request with automatic retry on rate limit.\"\"\"\n\n    for attempt in range(max_retries):\n        try:\n            response = client.chat(messages=messages, model=model)\n\n            if response['success']:\n                return response\n            else:\n                # Handle other errors\n                print(f\"Request failed: {response['message']}\")\n                return response\n\n        except RateLimitError as e:\n            print(f\"Rate limit hit (attempt {attempt + 1}/{max_retries})\")\n\n            if attempt &lt; max_retries - 1:\n                # Extract retry delay from error details\n                retry_after = getattr(e, 'retry_after', 60)\n                print(f\"Waiting {retry_after} seconds before retry...\")\n                time.sleep(retry_after)\n            else:\n                print(\"Max retries exceeded\")\n                raise\n\n        except Exception as e:\n            print(f\"Request failed with error: {e}\")\n            raise\n\n    return None\n\n# Usage\nresponse = make_request_with_retry(\n    messages=[{\"role\": \"user\", \"content\": \"Hello\"}],\n    model=\"openai/gpt-4o-mini\"\n)\n</code></pre>"},{"location":"usage/rate-limits/#rate-limit-management","title":"Rate Limit Management","text":""},{"location":"usage/rate-limits/#request-batching","title":"Request Batching","text":"<p>For high-volume applications, batch your requests efficiently:</p> <pre><code>import time\nfrom datetime import datetime, timedelta\n\nclass RateLimitManager:\n    \"\"\"Manage requests within rate limits.\"\"\"\n\n    def __init__(self, client, requests_per_minute=60, tokens_per_hour=100000):\n        self.client = client\n        self.requests_per_minute = requests_per_minute\n        self.tokens_per_hour = tokens_per_hour\n\n        # Tracking\n        self.request_timestamps = []\n        self.token_usage_hourly = []\n\n    def can_make_request(self, estimated_tokens=100):\n        \"\"\"Check if we can make a request within limits.\"\"\"\n        now = datetime.now()\n\n        # Clean old request timestamps (older than 1 minute)\n        minute_ago = now - timedelta(minutes=1)\n        self.request_timestamps = [ts for ts in self.request_timestamps if ts &gt; minute_ago]\n\n        # Clean old token usage (older than 1 hour)\n        hour_ago = now - timedelta(hours=1)\n        self.token_usage_hourly = [(ts, tokens) for ts, tokens in self.token_usage_hourly if ts &gt; hour_ago]\n\n        # Check request limit\n        if len(self.request_timestamps) &gt;= self.requests_per_minute:\n            return False, \"Request rate limit would be exceeded\"\n\n        # Check token limit\n        current_hourly_tokens = sum(tokens for _, tokens in self.token_usage_hourly)\n        if current_hourly_tokens + estimated_tokens &gt; self.tokens_per_hour:\n            return False, \"Token rate limit would be exceeded\"\n\n        return True, \"OK\"\n\n    def make_request(self, request_func, estimated_tokens=100, **kwargs):\n        \"\"\"Make request with rate limit checking.\"\"\"\n        can_request, reason = self.can_make_request(estimated_tokens)\n\n        if not can_request:\n            # Calculate wait time\n            if \"request\" in reason.lower():\n                wait_time = 60 - (datetime.now() - min(self.request_timestamps)).total_seconds()\n            else:  # token limit\n                wait_time = 3600 - (datetime.now() - min(ts for ts, _ in self.token_usage_hourly)).total_seconds()\n\n            print(f\"Rate limit hit: {reason}\")\n            print(f\"Estimated wait time: {wait_time:.0f} seconds\")\n            return None\n\n        # Make the request\n        try:\n            response = request_func(**kwargs)\n\n            # Track the request\n            now = datetime.now()\n            self.request_timestamps.append(now)\n\n            # Track token usage if successful\n            if response.get('success') and 'usage' in response:\n                actual_tokens = response['usage']['tokens_total']\n                self.token_usage_hourly.append((now, actual_tokens))\n\n            return response\n\n        except Exception as e:\n            print(f\"Request failed: {e}\")\n            return None\n\n    def wait_for_rate_limit_reset(self):\n        \"\"\"Wait for rate limits to reset.\"\"\"\n        now = datetime.now()\n\n        # Find when we can make the next request\n        if self.request_timestamps:\n            next_request_time = min(self.request_timestamps) + timedelta(minutes=1)\n            if next_request_time &gt; now:\n                wait_seconds = (next_request_time - now).total_seconds()\n                print(f\"Waiting {wait_seconds:.0f} seconds for request limit reset...\")\n                time.sleep(wait_seconds)\n\n        # Find when we have token capacity\n        if self.token_usage_hourly:\n            next_token_time = min(ts for ts, _ in self.token_usage_hourly) + timedelta(hours=1)\n            if next_token_time &gt; now:\n                wait_seconds = (next_token_time - now).total_seconds()\n                print(f\"Waiting {wait_seconds:.0f} seconds for token limit reset...\")\n                time.sleep(wait_seconds)\n\n# Usage example\nrate_manager = RateLimitManager(client, requests_per_minute=60, tokens_per_hour=100000)\n\n# Make a managed request\nresponse = rate_manager.make_request(\n    client.chat,\n    estimated_tokens=150,\n    messages=[{\"role\": \"user\", \"content\": \"Hello\"}],\n    model=\"openai/gpt-4o-mini\"\n)\n\nif response:\n    print(f\"Response: {response['data']}\")\n    print(f\"Actual tokens used: {response['usage']['tokens_total']}\")\n</code></pre>"},{"location":"usage/rate-limits/#batch-processing","title":"Batch Processing","text":"<p>For processing multiple items efficiently:</p> <pre><code>def process_batch_with_rate_limits(client, items, batch_size=10):\n    \"\"\"Process items in batches respecting rate limits.\"\"\"\n\n    rate_manager = RateLimitManager(client)\n    results = []\n\n    for i in range(0, len(items), batch_size):\n        batch = items[i:i + batch_size]\n        batch_results = []\n\n        print(f\"Processing batch {i//batch_size + 1}/{(len(items)-1)//batch_size + 1}\")\n\n        for item in batch:\n            # Check if we can make request\n            can_request, reason = rate_manager.can_make_request(estimated_tokens=100)\n\n            if not can_request:\n                print(f\"Rate limit hit, waiting...\")\n                rate_manager.wait_for_rate_limit_reset()\n\n            # Make the request\n            response = rate_manager.make_request(\n                client.chat,\n                estimated_tokens=100,\n                messages=[{\"role\": \"user\", \"content\": f\"Process this item: {item}\"}],\n                model=\"openai/gpt-4o-mini\"\n            )\n\n            if response and response['success']:\n                batch_results.append({\n                    'item': item,\n                    'result': response['data'],\n                    'cost': response['usage']['cost'],\n                    'tokens': response['usage']['tokens_total']\n                })\n            else:\n                batch_results.append({\n                    'item': item,\n                    'result': None,\n                    'error': response.get('message', 'Unknown error') if response else 'Request failed'\n                })\n\n        results.extend(batch_results)\n\n        # Brief pause between batches\n        time.sleep(1)\n\n    return results\n\n# Process a list of items\nitems_to_process = [\n    \"Translate this to French: Hello world\",\n    \"Summarize: The quick brown fox...\",\n    \"Generate a haiku about rain\",\n    # ... more items\n]\n\nresults = process_batch_with_rate_limits(client, items_to_process)\n\n# Analyze results\nsuccessful = [r for r in results if r['result'] is not None]\nfailed = [r for r in results if r['result'] is None]\n\nprint(f\"Successfully processed: {len(successful)}/{len(results)}\")\nprint(f\"Total cost: ${sum(r.get('cost', 0) for r in successful):.4f}\")\nprint(f\"Total tokens: {sum(r.get('tokens', 0) for r in successful):,}\")\n</code></pre>"},{"location":"usage/rate-limits/#optimization-strategies","title":"Optimization Strategies","text":""},{"location":"usage/rate-limits/#1-token-efficient-prompts","title":"1. Token-Efficient Prompts","text":"<p>Reduce token usage to stay within hourly limits:</p> <pre><code>def optimize_prompt_tokens(original_prompt, target_tokens=100):\n    \"\"\"Optimize prompts to use fewer tokens.\"\"\"\n\n    # Estimate tokens (rough approximation: 1 token \u2248 4 characters)\n    estimated_tokens = len(original_prompt) // 4\n\n    if estimated_tokens &lt;= target_tokens:\n        return original_prompt\n\n    # Optimization strategies\n    optimized_prompt = original_prompt\n\n    # Remove extra whitespace\n    optimized_prompt = ' '.join(optimized_prompt.split())\n\n    # Use abbreviations\n    replacements = {\n        'please': 'pls',\n        'because': 'bc',\n        'without': 'w/o',\n        'with': 'w/',\n        'and': '&amp;',\n        'you are': \"you're\",\n        'do not': \"don't\",\n        'cannot': \"can't\"\n    }\n\n    for old, new in replacements.items():\n        optimized_prompt = optimized_prompt.replace(old, new)\n\n    # If still too long, truncate with ellipsis\n    target_chars = target_tokens * 4\n    if len(optimized_prompt) &gt; target_chars:\n        optimized_prompt = optimized_prompt[:target_chars-3] + \"...\"\n\n    return optimized_prompt\n\n# Example usage\nlong_prompt = \"\"\"\nPlease analyze this text very carefully and provide a comprehensive summary\nthat includes all the key points, main arguments, supporting evidence, and\nconclusions. Make sure to maintain the original tone and style.\n\"\"\"\n\noptimized = optimize_prompt_tokens(long_prompt, target_tokens=50)\nprint(f\"Original: {len(long_prompt)} chars\")\nprint(f\"Optimized: {len(optimized)} chars\")\nprint(f\"Optimized prompt: {optimized}\")\n</code></pre>"},{"location":"usage/rate-limits/#2-smart-model-selection","title":"2. Smart Model Selection","text":"<p>Choose models based on rate limits and requirements:</p> <pre><code>def select_optimal_model(task_complexity, urgency=\"normal\", tier=\"standard\"):\n    \"\"\"Select the best model based on constraints.\"\"\"\n\n    # Model efficiency ratings (cost per token)\n    model_efficiency = {\n        \"openai/gpt-3.5-turbo\": {\"cost_per_token\": 0.000001, \"quality\": 7},\n        \"openai/gpt-4o-mini\": {\"cost_per_token\": 0.000002, \"quality\": 8},\n        \"openai/gpt-4o\": {\"cost_per_token\": 0.000030, \"quality\": 10},\n        \"anthropic/claude-3-haiku-20240307\": {\"cost_per_token\": 0.000002, \"quality\": 8},\n        \"anthropic/claude-3-sonnet-20240229\": {\"cost_per_token\": 0.000008, \"quality\": 9},\n        \"anthropic/claude-3-opus-20240229\": {\"cost_per_token\": 0.000020, \"quality\": 10},\n        \"deepseek/deepseek-chat\": {\"cost_per_token\": 0.0000005, \"quality\": 7}\n    }\n\n    # Filter models based on tier token limits\n    tier_limits = {\n        \"free\": 10000,      # 10K tokens/hour\n        \"standard\": 100000,  # 100K tokens/hour\n        \"enterprise\": 1000000 # 1M tokens/hour\n    }\n\n    hourly_limit = tier_limits.get(tier, 100000)\n\n    if task_complexity == \"simple\" and hourly_limit &lt; 50000:\n        # Use most efficient models for high-volume, low-complexity tasks\n        recommended = [\"deepseek/deepseek-chat\", \"openai/gpt-3.5-turbo\"]\n    elif task_complexity == \"moderate\":\n        recommended = [\"openai/gpt-4o-mini\", \"anthropic/claude-3-haiku-20240307\"]\n    else:  # complex\n        if urgency == \"high\" or hourly_limit &gt; 500000:\n            recommended = [\"openai/gpt-4o\", \"anthropic/claude-3-opus-20240229\"]\n        else:\n            recommended = [\"anthropic/claude-3-sonnet-20240229\", \"openai/gpt-4o-mini\"]\n\n    # Return best option with reasoning\n    best_model = recommended[0]\n    model_info = model_efficiency[best_model]\n\n    return {\n        'model': best_model,\n        'reasoning': f\"Selected for {task_complexity} task with {tier} tier limits\",\n        'cost_per_token': model_info['cost_per_token'],\n        'quality_rating': model_info['quality'],\n        'alternatives': recommended[1:] if len(recommended) &gt; 1 else []\n    }\n\n# Example usage\nselection = select_optimal_model(\"simple\", \"normal\", \"standard\")\nprint(f\"Recommended model: {selection['model']}\")\nprint(f\"Reasoning: {selection['reasoning']}\")\nprint(f\"Cost per token: ${selection['cost_per_token']:.7f}\")\n</code></pre>"},{"location":"usage/rate-limits/#3-request-scheduling","title":"3. Request Scheduling","text":"<p>Distribute requests to avoid rate limit peaks:</p> <pre><code>import time\nfrom datetime import datetime, timedelta\nimport random\n\nclass RequestScheduler:\n    \"\"\"Schedule requests to optimize rate limit usage.\"\"\"\n\n    def __init__(self, client, requests_per_minute=60):\n        self.client = client\n        self.requests_per_minute = requests_per_minute\n        self.request_queue = []\n\n    def add_request(self, request_func, priority=1, **kwargs):\n        \"\"\"Add request to queue with priority.\"\"\"\n        self.request_queue.append({\n            'func': request_func,\n            'kwargs': kwargs,\n            'priority': priority,\n            'added_at': datetime.now()\n        })\n\n        # Sort by priority (higher number = higher priority)\n        self.request_queue.sort(key=lambda x: x['priority'], reverse=True)\n\n    def process_queue(self, max_concurrent=5):\n        \"\"\"Process queued requests respecting rate limits.\"\"\"\n\n        # Calculate optimal delay between requests\n        min_delay = 60 / self.requests_per_minute  # seconds between requests\n\n        results = []\n        processed = 0\n\n        while self.request_queue and processed &lt; max_concurrent:\n            request = self.request_queue.pop(0)\n\n            try:\n                # Add small random delay to avoid synchronized requests\n                jitter = random.uniform(0, min_delay * 0.1)\n                time.sleep(min_delay + jitter)\n\n                print(f\"Processing request {processed + 1}/{min(len(self.request_queue) + 1, max_concurrent)}\")\n\n                response = request['func'](**request['kwargs'])\n                results.append({\n                    'request': request,\n                    'response': response,\n                    'processed_at': datetime.now()\n                })\n\n                processed += 1\n\n            except Exception as e:\n                print(f\"Request failed: {e}\")\n                results.append({\n                    'request': request,\n                    'response': None,\n                    'error': str(e),\n                    'processed_at': datetime.now()\n                })\n\n        return results\n\n# Usage example\nscheduler = RequestScheduler(client, requests_per_minute=60)\n\n# Add requests with different priorities\nscheduler.add_request(\n    client.chat,\n    priority=3,  # High priority\n    messages=[{\"role\": \"user\", \"content\": \"Urgent: Translate 'Hello' to French\"}],\n    model=\"openai/gpt-4o-mini\"\n)\n\nscheduler.add_request(\n    client.chat,\n    priority=1,  # Low priority\n    messages=[{\"role\": \"user\", \"content\": \"Generate a fun fact about cats\"}],\n    model=\"openai/gpt-3.5-turbo\"\n)\n\nscheduler.add_request(\n    client.chat,\n    priority=2,  # Medium priority\n    messages=[{\"role\": \"user\", \"content\": \"Summarize the latest news\"}],\n    model=\"openai/gpt-4o-mini\"\n)\n\n# Process the queue\nresults = scheduler.process_queue(max_concurrent=3)\n\nprint(f\"Processed {len(results)} requests\")\nfor i, result in enumerate(results, 1):\n    if result['response'] and result['response']['success']:\n        print(f\"{i}. Success: {result['response']['data'][:50]}...\")\n    else:\n        print(f\"{i}. Failed: {result.get('error', 'Unknown error')}\")\n</code></pre>"},{"location":"usage/rate-limits/#monitoring-rate-limits","title":"Monitoring Rate Limits","text":""},{"location":"usage/rate-limits/#real-time-rate-limit-tracking","title":"Real-time Rate Limit Tracking","text":"<pre><code>class RateLimitMonitor:\n    \"\"\"Monitor rate limit usage in real-time.\"\"\"\n\n    def __init__(self, requests_per_minute=60, tokens_per_hour=100000):\n        self.requests_per_minute = requests_per_minute\n        self.tokens_per_hour = tokens_per_hour\n        self.request_timestamps = []\n        self.token_usage = []\n\n    def log_request(self, response):\n        \"\"\"Log a request and its token usage.\"\"\"\n        now = datetime.now()\n        self.request_timestamps.append(now)\n\n        if response and response.get('success') and 'usage' in response:\n            tokens = response['usage']['tokens_total']\n            self.token_usage.append((now, tokens))\n\n    def get_current_usage(self):\n        \"\"\"Get current rate limit usage.\"\"\"\n        now = datetime.now()\n\n        # Clean old data\n        minute_ago = now - timedelta(minutes=1)\n        hour_ago = now - timedelta(hours=1)\n\n        self.request_timestamps = [ts for ts in self.request_timestamps if ts &gt; minute_ago]\n        self.token_usage = [(ts, tokens) for ts, tokens in self.token_usage if ts &gt; hour_ago]\n\n        # Calculate current usage\n        current_requests = len(self.request_timestamps)\n        current_tokens = sum(tokens for _, tokens in self.token_usage)\n\n        return {\n            'requests': {\n                'current': current_requests,\n                'limit': self.requests_per_minute,\n                'percentage': (current_requests / self.requests_per_minute * 100) if self.requests_per_minute &gt; 0 else 0,\n                'remaining': max(0, self.requests_per_minute - current_requests)\n            },\n            'tokens': {\n                'current': current_tokens,\n                'limit': self.tokens_per_hour,\n                'percentage': (current_tokens / self.tokens_per_hour * 100) if self.tokens_per_hour &gt; 0 else 0,\n                'remaining': max(0, self.tokens_per_hour - current_tokens)\n            }\n        }\n\n    def print_status(self):\n        \"\"\"Print current rate limit status.\"\"\"\n        usage = self.get_current_usage()\n\n        print(f\"\ud83d\udcca Rate Limit Status\")\n        print(f\"   Requests: {usage['requests']['current']}/{usage['requests']['limit']} ({usage['requests']['percentage']:.1f}%)\")\n        print(f\"   Tokens: {usage['tokens']['current']:,}/{usage['tokens']['limit']:,} ({usage['tokens']['percentage']:.1f}%)\")\n\n        if usage['requests']['percentage'] &gt; 80:\n            print(f\"   \u26a0\ufe0f  High request usage\")\n        if usage['tokens']['percentage'] &gt; 80:\n            print(f\"   \u26a0\ufe0f  High token usage\")\n\n# Usage\nmonitor = RateLimitMonitor(requests_per_minute=60, tokens_per_hour=100000)\n\n# Make some requests\nfor i in range(5):\n    response = client.chat(\n        messages=[{\"role\": \"user\", \"content\": f\"Request {i+1}\"}],\n        model=\"openai/gpt-4o-mini\"\n    )\n    monitor.log_request(response)\n\n    # Show status every few requests\n    if (i + 1) % 2 == 0:\n        monitor.print_status()\n        print()\n</code></pre>"},{"location":"usage/rate-limits/#best-practices","title":"Best Practices","text":""},{"location":"usage/rate-limits/#1-respect-rate-limits","title":"1. Respect Rate Limits","text":"<pre><code># Always implement retry logic with exponential backoff\ndef make_request_with_backoff(client, request_func, max_retries=3, **kwargs):\n    \"\"\"Make request with exponential backoff on rate limits.\"\"\"\n\n    for attempt in range(max_retries):\n        try:\n            response = request_func(**kwargs)\n            return response\n\n        except RateLimitError as e:\n            if attempt &lt; max_retries - 1:\n                wait_time = (2 ** attempt) * 60  # Exponential backoff: 1min, 2min, 4min\n                print(f\"Rate limited, waiting {wait_time} seconds...\")\n                time.sleep(wait_time)\n            else:\n                raise\n\n    return None\n</code></pre>"},{"location":"usage/rate-limits/#2-optimize-token-usage","title":"2. Optimize Token Usage","text":"<pre><code># Use efficient prompting techniques\ndef create_efficient_prompt(task, context=\"\", max_tokens=100):\n    \"\"\"Create token-efficient prompts.\"\"\"\n\n    # Use concise language\n    efficient_prompt = f\"Task: {task}\"\n\n    if context:\n        # Limit context to essential information\n        context_limit = max_tokens // 2\n        if len(context) &gt; context_limit * 4:  # Rough token estimation\n            context = context[:context_limit * 4] + \"...\"\n        efficient_prompt += f\"\\nContext: {context}\"\n\n    efficient_prompt += \"\\nResponse:\"\n\n    return efficient_prompt\n\n# Example\ntask = \"Summarize the main points\"\ncontext = \"Long document text here...\"\nprompt = create_efficient_prompt(task, context, max_tokens=150)\n</code></pre>"},{"location":"usage/rate-limits/#3-monitor-and-alert","title":"3. Monitor and Alert","text":"<pre><code># Set up monitoring for your application\ndef setup_rate_limit_alerts(client, alert_threshold=0.8):\n    \"\"\"Set up alerts for rate limit usage.\"\"\"\n\n    monitor = RateLimitMonitor()\n\n    def check_and_alert():\n        usage = monitor.get_current_usage()\n\n        if usage['requests']['percentage'] &gt; alert_threshold * 100:\n            print(f\"\ud83d\udea8 Request rate limit alert: {usage['requests']['percentage']:.1f}% used\")\n\n        if usage['tokens']['percentage'] &gt; alert_threshold * 100:\n            print(f\"\ud83d\udea8 Token rate limit alert: {usage['tokens']['percentage']:.1f}% used\")\n\n    return monitor, check_and_alert\n\n# Use monitoring\nmonitor, alert_check = setup_rate_limit_alerts(client)\n\n# In your application loop\nresponse = client.chat(messages=[...], model=\"openai/gpt-4o-mini\")\nmonitor.log_request(response)\nalert_check()\n</code></pre>"},{"location":"usage/responses/","title":"Response Format","text":"<p>IndoxRouter provides detailed response information for every API call, including usage statistics, costs, and performance metrics. This helps you monitor and optimize your AI application usage.</p>"},{"location":"usage/responses/#standard-response-structure","title":"Standard Response Structure","text":"<p>Every IndoxRouter response follows this consistent format:</p> <pre><code>{\n    'request_id': 'c08cc108-6b0d-48bd-a660-546143f1b9fa',\n    'created_at': '2025-05-19T06:07:38.077269',\n    'duration_ms': 9664.651870727539,\n    'provider': 'deepseek',\n    'model': 'deepseek-chat',\n    'success': True,\n    'message': '',\n    'usage': {\n        'tokens_prompt': 15,\n        'tokens_completion': 107,\n        'tokens_total': 122,\n        'cost': 0.000229,\n        'latency': 9.487398862838745,\n        'timestamp': '2025-05-19T06:07:38.065330',\n        'cache_read_tokens': 0,\n        'cache_write_tokens': 0,\n        'reasoning_tokens': 0,\n        'web_search_count': 0,\n        'request_count': 1,\n        'cost_breakdown': {\n            'input_tokens': 0.000025,\n            'output_tokens': 0.000204,\n            'cache_read': 0.0,\n            'cache_write': 0.0,\n            'reasoning': 0.0,\n            'web_search': 0.0,\n            'request': 0.0\n        }\n    },\n    'raw_response': None,\n    'data': 'Your AI response content here...',\n    'finish_reason': None\n}\n</code></pre>"},{"location":"usage/responses/#response-fields","title":"Response Fields","text":""},{"location":"usage/responses/#metadata-fields","title":"Metadata Fields","text":"Field Type Description <code>request_id</code> string Unique identifier for this request <code>created_at</code> string ISO timestamp when request was processed <code>duration_ms</code> float Total request duration in milliseconds <code>provider</code> string AI provider used (openai, anthropic, etc.) <code>model</code> string Specific model used <code>success</code> boolean Whether the request succeeded <code>message</code> string Success message or additional info"},{"location":"usage/responses/#usage-statistics","title":"Usage Statistics","text":"<p>The <code>usage</code> object contains detailed usage and cost information:</p> Field Type Description <code>tokens_prompt</code> integer Tokens used in the input/prompt <code>tokens_completion</code> integer Tokens generated in the response <code>tokens_total</code> integer Total tokens used (prompt + completion) <code>cost</code> float Total cost in USD for this request <code>latency</code> float Provider response time in seconds <code>timestamp</code> string ISO timestamp of the request <code>cache_read_tokens</code> integer Tokens read from cache <code>cache_write_tokens</code> integer Tokens written to cache <code>reasoning_tokens</code> integer Tokens used for internal reasoning <code>web_search_count</code> integer Number of web searches performed <code>request_count</code> integer Number of requests made (usually 1) <code>cost_breakdown</code> object Detailed cost breakdown by component"},{"location":"usage/responses/#cost-breakdown","title":"Cost Breakdown","text":"<p>The <code>cost_breakdown</code> object provides detailed cost information:</p> Field Type Description <code>input_tokens</code> float Cost for input/prompt tokens <code>output_tokens</code> float Cost for output/completion tokens <code>cache_read</code> float Cost for cache read operations <code>cache_write</code> float Cost for cache write operations <code>reasoning</code> float Cost for reasoning tokens <code>web_search</code> float Cost for web search operations <code>request</code> float Base request cost"},{"location":"usage/responses/#content-fields","title":"Content Fields","text":"Field Type Description <code>data</code> string/array The actual AI response content <code>finish_reason</code> string Why the response ended (stop, length, etc.) <code>raw_response</code> object Original provider response (optional)"},{"location":"usage/responses/#response-examples-by-operation","title":"Response Examples by Operation","text":""},{"location":"usage/responses/#chat-completion-response","title":"Chat Completion Response","text":"<pre><code>response = client.chat(\n    messages=[{\"role\": \"user\", \"content\": \"What is the capital of France?\"}],\n    model=\"openai/gpt-4o-mini\"\n)\n\n# Response structure:\n{\n    'request_id': 'b881942c-e21d-4f9d-ad82-47344945c642',\n    'created_at': '2025-06-15T09:53:26.130868',\n    'duration_ms': 1737.612247467041,\n    'provider': 'openai',\n    'model': 'gpt-4o-mini',\n    'success': True,\n    'message': '',\n    'usage': {\n        'tokens_prompt': 24,\n        'tokens_completion': 7,\n        'tokens_total': 31,\n        'cost': 7.8e-06,\n        'latency': 1.629077672958374,\n        'timestamp': '2025-06-15T09:53:26.114626',\n        'cache_read_tokens': 0,\n        'cache_write_tokens': 0,\n        'reasoning_tokens': 0,\n        'web_search_count': 0,\n        'request_count': 1,\n        'cost_breakdown': {\n            'input_tokens': 3.6e-06,\n            'output_tokens': 4.2e-06,\n            'cache_read': 0.0,\n            'cache_write': 0.0,\n            'reasoning': 0.0,\n            'web_search': 0.0,\n            'request': 0.0\n        }\n    },\n    'raw_response': None,\n    'data': 'The capital of France is Paris.',\n    'finish_reason': None\n}\n</code></pre>"},{"location":"usage/responses/#text-completion-response","title":"Text Completion Response","text":"<pre><code>response = client.completions(\n    prompt=\"Tell me a story\",\n    model=\"openai/gpt-4o-mini\",\n    max_tokens=500\n)\n\n# Response structure:\n{\n    'request_id': '0fecd9af-0ba8-47a4-852f-029b3a5bfa18',\n    'created_at': '2025-06-15T09:54:51.393591',\n    'duration_ms': 6939.460754394531,\n    'provider': 'openai',\n    'model': 'gpt-4o-mini',\n    'success': True,\n    'message': '',\n    'usage': {\n        'tokens_prompt': 11,\n        'tokens_completion': 530,\n        'tokens_total': 541,\n        'cost': 0.00031965,\n        'latency': 6.794795513153076,\n        'timestamp': '2025-06-15T09:54:51.362423',\n        'cache_read_tokens': 0,\n        'cache_write_tokens': 0,\n        'reasoning_tokens': 0,\n        'web_search_count': 0,\n        'request_count': 1,\n        'cost_breakdown': {\n            'input_tokens': 1.6499999999999999e-06,\n            'output_tokens': 0.000318,\n            'cache_read': 0.0,\n            'cache_write': 0.0,\n            'reasoning': 0.0,\n            'web_search': 0.0,\n            'request': 0.0\n        }\n    },\n    'raw_response': None,\n    'data': 'Once upon a time, in a small village nestled between rolling hills and a sparkling river, there lived a young girl named Elara. She was known throughout the village for her kindness and her love for nature...',\n    'finish_reason': None\n}\n</code></pre>"},{"location":"usage/responses/#embedding-response","title":"Embedding Response","text":"<pre><code>response = client.embeddings(\n    text=\"Hello world\",\n    model=\"openai/text-embedding-3-small\"\n)\n\n# Response structure:\n{\n    'request_id': 'req_ghi789',\n    'created_at': '2025-05-19T10:40:15.456789',\n    'duration_ms': 456.78,\n    'provider': 'openai',\n    'model': 'text-embedding-3-small',\n    'success': True,\n    'message': '',\n    'usage': {\n        'tokens_prompt': 2,\n        'tokens_completion': 0,\n        'tokens_total': 2,\n        'cost': 0.000001,\n        'latency': 0.3,\n        'timestamp': '2025-05-19T10:40:15.400000'\n    },\n    'data': [\n        [0.123, -0.456, 0.789, ...]  # 1536-dimensional vector\n    ],\n    'dimensions': 1536\n}\n</code></pre>"},{"location":"usage/responses/#image-generation-response","title":"Image Generation Response","text":""},{"location":"usage/responses/#url-based-models-dall-e-2-dall-e-3","title":"URL-Based Models (DALL-E 2, DALL-E 3)","text":"<pre><code>response = client.images(\n    prompt=\"A beautiful sunset over the ocean\",\n    model=\"openai/dall-e-2\",\n    size=\"1024x1024\"\n)\n\n# Response structure:\n{\n    'request_id': '0bc89954-f5cc-4efc-a055-4e5624aa2a81',\n    'created_at': '2025-05-29T11:39:24.621706',\n    'duration_ms': 12340.412378311157,\n    'provider': 'openai',\n    'model': 'dall-e-2',\n    'success': True,\n    'message': '',\n    'usage': {\n        'tokens_prompt': 0,\n        'tokens_completion': 0,\n        'tokens_total': 0,\n        'cost': 0.016,\n        'latency': 12.240789651870728,\n        'timestamp': '2025-05-29T11:39:24.612377',\n        'cache_read_tokens': 0,\n        'cache_write_tokens': 0,\n        'reasoning_tokens': 0,\n        'web_search_count': 0,\n        'request_count': 1,\n        'cost_breakdown': None\n    },\n    'raw_response': None,\n    'data': [\n        {\n            'url': 'https://dalle-images.openai.com/...',\n            'revised_prompt': 'A beautiful sunset over the ocean with golden clouds...'\n        }\n    ]\n}\n</code></pre>"},{"location":"usage/responses/#base64-based-models-gpt-image-1","title":"Base64-Based Models (GPT-Image-1)","text":"<pre><code>response = client.images(\n    prompt=\"A beautiful sunset over the ocean\",\n    model=\"openai/gpt-image-1\",\n    size=\"1024x1024\"\n)\n\n# Response structure:\n{\n    'request_id': 'b4ece4dd-b41d-4e57-952a-7cc4e7d60be4',\n    'created_at': '2025-06-15T10:00:04.021541',\n    'duration_ms': 36015.32602310181,\n    'provider': 'openai',\n    'model': 'gpt-image-1',\n    'success': True,\n    'message': '',\n    'usage': {\n        'tokens_prompt': 12,\n        'tokens_completion': 4160,\n        'tokens_total': 4172,\n        'cost': 0.17746,\n        'latency': 35.91205406188965,\n        'timestamp': '2025-06-15T10:00:04.010734',\n        'cache_read_tokens': 0,\n        'cache_write_tokens': 0,\n        'reasoning_tokens': 0,\n        'web_search_count': 0,\n        'request_count': 1,\n        'cost_breakdown': None\n    },\n    'raw_response': None,\n    'data': [\n        {\n            'b64_json': 'iVBORw0KGgoAAAANSUhEUgAAB...(base64 encoded image data)'\n        }\n    ]\n}\n</code></pre>"},{"location":"usage/responses/#working-with-responses","title":"Working with Responses","text":""},{"location":"usage/responses/#accessing-response-data","title":"Accessing Response Data","text":"<pre><code>response = client.chat(\n    messages=[{\"role\": \"user\", \"content\": \"Hello\"}],\n    model=\"openai/gpt-4o-mini\"\n)\n\n# Get the AI response text\ncontent = response['data']\nprint(content)\n\n# Get usage information\nusage = response['usage']\nprint(f\"Tokens used: {usage['tokens_total']}\")\nprint(f\"Cost: ${usage['cost']:.6f}\")\nprint(f\"Latency: {usage['latency']:.2f}s\")\n\n# Get detailed cost breakdown\nif usage['cost_breakdown']:\n    breakdown = usage['cost_breakdown']\n    print(f\"Input token cost: ${breakdown['input_tokens']:.6f}\")\n    print(f\"Output token cost: ${breakdown['output_tokens']:.6f}\")\n    print(f\"Cache read cost: ${breakdown['cache_read']:.6f}\")\n\n# Get metadata\nprint(f\"Provider: {response['provider']}\")\nprint(f\"Model: {response['model']}\")\nprint(f\"Request ID: {response['request_id']}\")\n</code></pre>"},{"location":"usage/responses/#handling-image-responses","title":"Handling Image Responses","text":""},{"location":"usage/responses/#url-based-images-dall-e-2-dall-e-3","title":"URL-Based Images (DALL-E 2, DALL-E 3)","text":"<pre><code>response = client.images(\n    prompt=\"A beautiful sunset\",\n    model=\"openai/dall-e-2\",\n    size=\"1024x1024\"\n)\n\n# Get image URL\nif response['data']:\n    image_url = response['data'][0]['url']\n    print(f\"Image URL: {image_url}\")\n\n    # Download and display the image\n    import requests\n    from PIL import Image\n    from io import BytesIO\n\n    img_response = requests.get(image_url)\n    img = Image.open(BytesIO(img_response.content))\n    img.show()  # Or save: img.save(\"generated_image.png\")\n</code></pre>"},{"location":"usage/responses/#base64-based-images-gpt-image-1","title":"Base64-Based Images (GPT-Image-1)","text":"<pre><code>response = client.images(\n    prompt=\"A beautiful sunset\",\n    model=\"openai/gpt-image-1\",\n    size=\"1024x1024\"\n)\n\n# Handle base64 encoded image\nif response['data'] and 'b64_json' in response['data'][0]:\n    import base64\n    from PIL import Image\n    from io import BytesIO\n\n    # Decode base64 image data\n    b64_data = response['data'][0]['b64_json']\n    image_data = base64.b64decode(b64_data)\n\n    # Convert to PIL Image\n    img = Image.open(BytesIO(image_data))\n\n    # Save the image\n    img.save(\"generated_image.png\")\n    print(\"Image saved as 'generated_image.png'\")\n\n    # Display in Jupyter notebook\n    from IPython.display import Image as IPImage, display\n    display(IPImage(data=image_data))\n</code></pre>"},{"location":"usage/responses/#cost-tracking","title":"Cost Tracking","text":"<pre><code>def track_costs(response):\n    \"\"\"Extract and log cost information from response.\"\"\"\n    usage = response['usage']\n\n    print(f\"Request Cost Breakdown:\")\n    print(f\"  Model: {response['provider']}/{response['model']}\")\n    print(f\"  Prompt tokens: {usage['tokens_prompt']}\")\n    print(f\"  Completion tokens: {usage['tokens_completion']}\")\n    print(f\"  Total tokens: {usage['tokens_total']}\")\n    print(f\"  Cost: ${usage['cost']:.6f}\")\n    print(f\"  Latency: {usage['latency']:.2f}s\")\n\n    return usage['cost']\n\n# Use with any request\nresponse = client.chat(messages=[...], model=\"openai/gpt-4o\")\ncost = track_costs(response)\n</code></pre>"},{"location":"usage/responses/#performance-monitoring","title":"Performance Monitoring","text":"<pre><code>def analyze_performance(response):\n    \"\"\"Analyze request performance metrics.\"\"\"\n    duration = response['duration_ms']\n    latency = response['usage']['latency'] * 1000  # Convert to ms\n\n    # Network + processing overhead\n    overhead = duration - latency\n\n    print(f\"Performance Analysis:\")\n    print(f\"  Total duration: {duration:.2f}ms\")\n    print(f\"  Provider latency: {latency:.2f}ms\")\n    print(f\"  Network overhead: {overhead:.2f}ms\")\n\n    if overhead &gt; 1000:  # &gt; 1 second overhead\n        print(\"  \u26a0\ufe0f  High network overhead detected\")\n\n    return {\n        'total_duration': duration,\n        'provider_latency': latency,\n        'overhead': overhead\n    }\n</code></pre>"},{"location":"usage/responses/#error-responses","title":"Error Responses","text":"<p>When an error occurs, the response format changes:</p> <pre><code>{\n    'success': False,\n    'error': 'ModelNotFoundError',\n    'message': 'Model \"gpt-5\" not found for provider \"openai\"',\n    'status_code': 404,\n    'request_id': 'req_error123',\n    'details': {\n        'provider': 'openai',\n        'requested_model': 'gpt-5',\n        'available_models': ['gpt-4o', 'gpt-4o-mini', 'gpt-3.5-turbo']\n    }\n}\n</code></pre>"},{"location":"usage/responses/#handling-error-responses","title":"Handling Error Responses","text":"<pre><code>try:\n    response = client.chat(\n        messages=[{\"role\": \"user\", \"content\": \"Hello\"}],\n        model=\"invalid/model\"\n    )\n\n    if response['success']:\n        print(response['data'])\n    else:\n        print(f\"Error: {response['error']}\")\n        print(f\"Message: {response['message']}\")\n\n        # Get suggested alternatives\n        if 'details' in response and 'available_models' in response['details']:\n            print(\"Available models:\", response['details']['available_models'])\n\nexcept Exception as e:\n    print(f\"Request failed: {e}\")\n</code></pre>"},{"location":"usage/responses/#streaming-responses","title":"Streaming Responses","text":"<p>For streaming requests, responses come in chunks:</p> <pre><code>response_stream = client.chat(\n    messages=[{\"role\": \"user\", \"content\": \"Tell me a story\"}],\n    model=\"openai/gpt-4o-mini\",\n    stream=True\n)\n\nfull_response = \"\"\ntotal_cost = 0\n\nfor chunk in response_stream:\n    if chunk['success']:\n        # Accumulate the response\n        full_response += chunk['data']\n\n        # Track costs (final chunk has complete usage info)\n        if 'usage' in chunk:\n            total_cost = chunk['usage']['cost']\n\n        print(chunk['data'], end='', flush=True)\n\nprint(f\"\\n\\nTotal cost: ${total_cost:.6f}\")\n</code></pre>"},{"location":"usage/responses/#response-validation","title":"Response Validation","text":"<pre><code>def validate_response(response):\n    \"\"\"Validate IndoxRouter response format.\"\"\"\n    required_fields = ['request_id', 'success', 'provider', 'model']\n\n    for field in required_fields:\n        if field not in response:\n            raise ValueError(f\"Missing required field: {field}\")\n\n    if response['success']:\n        if 'data' not in response:\n            raise ValueError(\"Success response missing 'data' field\")\n        if 'usage' not in response:\n            raise ValueError(\"Success response missing 'usage' field\")\n    else:\n        if 'error' not in response:\n            raise ValueError(\"Error response missing 'error' field\")\n\n    return True\n\n# Use with responses\nresponse = client.chat(messages=[...], model=\"openai/gpt-4o-mini\")\nif validate_response(response):\n    print(\"Response format is valid\")\n</code></pre>"},{"location":"usage/responses/#best-practices","title":"Best Practices","text":""},{"location":"usage/responses/#1-always-check-success-status","title":"1. Always Check Success Status","text":"<pre><code>response = client.chat(messages=[...], model=\"openai/gpt-4o-mini\")\n\nif response['success']:\n    content = response['data']\n    cost = response['usage']['cost']\n    # Process successful response\nelse:\n    error = response['error']\n    message = response['message']\n    # Handle error\n</code></pre>"},{"location":"usage/responses/#2-monitor-costs","title":"2. Monitor Costs","text":"<pre><code># Set up cost alerts\ndef check_cost_threshold(response, max_cost=0.01):\n    \"\"\"Alert if single request exceeds cost threshold.\"\"\"\n    cost = response['usage']['cost']\n    if cost &gt; max_cost:\n        print(f\"\u26a0\ufe0f  High cost request: ${cost:.4f}\")\n        print(f\"   Model: {response['provider']}/{response['model']}\")\n        print(f\"   Tokens: {response['usage']['tokens_total']}\")\n\n    return cost\n\nresponse = client.chat(messages=[...], model=\"openai/gpt-4o\")\ncheck_cost_threshold(response)\n</code></pre>"},{"location":"usage/responses/#3-track-performance","title":"3. Track Performance","text":"<pre><code># Performance monitoring\ndef log_performance(response):\n    \"\"\"Log performance metrics for monitoring.\"\"\"\n    metrics = {\n        'request_id': response['request_id'],\n        'model': f\"{response['provider']}/{response['model']}\",\n        'duration_ms': response['duration_ms'],\n        'latency_ms': response['usage']['latency'] * 1000,\n        'tokens': response['usage']['tokens_total'],\n        'cost': response['usage']['cost']\n    }\n\n    # Log to your monitoring system\n    print(f\"METRICS: {metrics}\")\n\n    return metrics\n</code></pre>"},{"location":"usage/responses/#4-store-request-ids","title":"4. Store Request IDs","text":"<pre><code># For debugging and support\ndef save_request_info(response, query_description):\n    \"\"\"Save request information for debugging.\"\"\"\n    info = {\n        'timestamp': response['created_at'],\n        'request_id': response['request_id'],\n        'description': query_description,\n        'model': f\"{response['provider']}/{response['model']}\",\n        'success': response['success'],\n        'cost': response.get('usage', {}).get('cost', 0)\n    }\n\n    # Save to logs or database\n    print(f\"REQUEST_LOG: {info}\")\n\n    return info\n\nresponse = client.chat(messages=[...], model=\"openai/gpt-4o-mini\")\nsave_request_info(response, \"User greeting response\")\n</code></pre>"},{"location":"usage/tracking/","title":"Usage Tracking","text":"<p>IndoxRouter provides comprehensive usage tracking to help you monitor your API consumption, costs, and performance metrics.</p>"},{"location":"usage/tracking/#getting-usage-statistics","title":"Getting Usage Statistics","text":"<p>Use the <code>get_usage()</code> method to retrieve detailed usage information:</p> <pre><code>from indoxrouter import IndoxRouter\n\nclient = IndoxRouter(api_key=\"your-api-key\")\n\n# Get current usage statistics\nusage_stats = client.get_usage()\nprint(usage_stats)\n</code></pre>"},{"location":"usage/tracking/#usage-response-format","title":"Usage Response Format","text":"<p>The <code>get_usage()</code> method returns detailed statistics including:</p> <pre><code>{\n    \"total_requests\": 1250,\n    \"total_tokens\": 45000,\n    \"total_cost\": 12.50,\n    \"current_period\": {\n        \"requests\": 150,\n        \"tokens\": 5500,\n        \"cost\": 1.75,\n        \"period_start\": \"2024-01-01T00:00:00Z\",\n        \"period_end\": \"2024-01-31T23:59:59Z\"\n    },\n    \"by_provider\": {\n        \"openai\": {\n            \"requests\": 800,\n            \"tokens\": 28000,\n            \"cost\": 8.40\n        },\n        \"anthropic\": {\n            \"requests\": 300,\n            \"tokens\": 12000,\n            \"cost\": 2.88\n        },\n        \"google\": {\n            \"requests\": 150,\n            \"tokens\": 5000,\n            \"cost\": 1.22\n        }\n    },\n    \"by_model\": {\n        \"gpt-4\": {\n            \"requests\": 400,\n            \"tokens\": 15000,\n            \"cost\": 4.50\n        },\n        \"claude-3-sonnet\": {\n            \"requests\": 300,\n            \"tokens\": 12000,\n            \"cost\": 2.88\n        },\n        \"gemini-pro\": {\n            \"requests\": 150,\n            \"tokens\": 5000,\n            \"cost\": 1.22\n        }\n    }\n}\n</code></pre>"},{"location":"usage/tracking/#real-time-usage-in-responses","title":"Real-time Usage in Responses","text":"<p>Every API response includes detailed usage information:</p>"},{"location":"usage/tracking/#chat-completion-usage","title":"Chat Completion Usage","text":"<pre><code>response = client.chat_completions(\n    provider=\"openai\",\n    model=\"gpt-4\",\n    messages=[{\"role\": \"user\", \"content\": \"Hello!\"}]\n)\n\n# Access usage information\nusage = response.usage\nprint(f\"Prompt tokens: {usage.prompt_tokens}\")\nprint(f\"Completion tokens: {usage.completion_tokens}\")\nprint(f\"Total tokens: {usage.total_tokens}\")\nprint(f\"Cost: ${usage.cost_breakdown.total_cost}\")\n</code></pre>"},{"location":"usage/tracking/#detailed-usage-fields","title":"Detailed Usage Fields","text":"<p>Each response includes comprehensive usage tracking:</p> <ul> <li><code>prompt_tokens</code>: Tokens used in the input</li> <li><code>completion_tokens</code>: Tokens generated in the response</li> <li><code>total_tokens</code>: Sum of prompt and completion tokens</li> <li><code>cache_read_tokens</code>: Tokens read from cache (if applicable)</li> <li><code>cache_write_tokens</code>: Tokens written to cache (if applicable)</li> <li><code>reasoning_tokens</code>: Tokens used for reasoning (for reasoning models)</li> <li><code>web_search_count</code>: Number of web searches performed</li> <li><code>request_count</code>: Number of API requests made</li> <li><code>cost_breakdown</code>: Detailed cost information</li> </ul>"},{"location":"usage/tracking/#cost-breakdown","title":"Cost Breakdown","text":"<p>The <code>cost_breakdown</code> object provides detailed pricing information:</p> <pre><code>{\n    \"prompt_cost\": 0.015,\n    \"completion_cost\": 0.030,\n    \"cache_read_cost\": 0.0015,\n    \"cache_write_cost\": 0.0075,\n    \"reasoning_cost\": 0.240,\n    \"web_search_cost\": 0.001,\n    \"total_cost\": 0.2935\n}\n</code></pre>"},{"location":"usage/tracking/#monitoring-best-practices","title":"Monitoring Best Practices","text":""},{"location":"usage/tracking/#1-regular-usage-checks","title":"1. Regular Usage Checks","text":"<pre><code># Check usage before making expensive requests\nusage = client.get_usage()\nif usage[\"current_period\"][\"cost\"] &gt; 50.0:\n    print(\"Warning: High usage this period\")\n</code></pre>"},{"location":"usage/tracking/#2-provider-cost-optimization","title":"2. Provider Cost Optimization","text":"<pre><code># Compare costs across providers\nusage = client.get_usage()\nfor provider, stats in usage[\"by_provider\"].items():\n    cost_per_token = stats[\"cost\"] / stats[\"tokens\"]\n    print(f\"{provider}: ${cost_per_token:.6f} per token\")\n</code></pre>"},{"location":"usage/tracking/#3-model-performance-tracking","title":"3. Model Performance Tracking","text":"<pre><code># Track model efficiency\nusage = client.get_usage()\nfor model, stats in usage[\"by_model\"].items():\n    avg_tokens_per_request = stats[\"tokens\"] / stats[\"requests\"]\n    print(f\"{model}: {avg_tokens_per_request:.1f} tokens per request\")\n</code></pre>"},{"location":"usage/tracking/#rate-limit-monitoring","title":"Rate Limit Monitoring","text":"<p>Usage tracking also helps monitor rate limit consumption:</p> <pre><code># Check current rate limit status\nusage = client.get_usage()\ncurrent_requests = usage[\"current_period\"][\"requests\"]\nprint(f\"Requests this period: {current_requests}\")\n\n# Rate limits vary by tier:\n# - Free: 10 requests/minute, 10K tokens/hour\n# - Standard: 60 requests/minute, 100K tokens/hour\n# - Enterprise: 500 requests/minute, 1M tokens/hour\n</code></pre>"},{"location":"usage/tracking/#export-usage-data","title":"Export Usage Data","text":"<p>For detailed analysis, you can export usage data:</p> <pre><code>import json\nfrom datetime import datetime\n\n# Get usage data\nusage = client.get_usage()\n\n# Save to file with timestamp\ntimestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\nfilename = f\"usage_report_{timestamp}.json\"\n\nwith open(filename, 'w') as f:\n    json.dump(usage, f, indent=2)\n\nprint(f\"Usage report saved to {filename}\")\n</code></pre>"},{"location":"usage/tracking/#integration-with-analytics","title":"Integration with Analytics","text":"<p>Usage data can be integrated with analytics platforms:</p> <pre><code># Example: Send to analytics service\ndef track_usage_metrics(usage_data):\n    metrics = {\n        'total_requests': usage_data['total_requests'],\n        'total_cost': usage_data['total_cost'],\n        'avg_cost_per_request': usage_data['total_cost'] / usage_data['total_requests']\n    }\n\n    # Send to your analytics platform\n    # analytics_client.track('api_usage', metrics)\n\ntrack_usage_metrics(client.get_usage())\n</code></pre>"},{"location":"use-cases/chatbots/","title":"Building Chatbots with IndoxRouter","text":"<p>This guide shows how to build a simple but effective chatbot using the IndoxRouter client. By following these examples, you can create chatbots that leverage different AI models through a consistent interface.</p>"},{"location":"use-cases/chatbots/#basic-chatbot","title":"Basic Chatbot","text":"<p>Here's a simple example of a command-line chatbot:</p> <pre><code>from indoxrouter import Client\n\ndef simple_chatbot():\n    \"\"\"A simple command-line chatbot using IndoxRouter.\"\"\"\n\n    print(\"Welcome to IndoxRouter Chatbot!\")\n    print(\"Type 'exit' to end the conversation.\\n\")\n\n    # Initialize the client\n    with Client(api_key=\"your_api_key\") as client:\n        # Set up the conversation with a system message\n        messages = [\n            {\"role\": \"system\", \"content\": \"You are a helpful assistant that provides concise answers.\"}\n        ]\n\n        while True:\n            # Get user input\n            user_input = input(\"You: \")\n\n            # Check if user wants to exit\n            if user_input.lower() in (\"exit\", \"quit\", \"bye\"):\n                print(\"Assistant: Goodbye!\")\n                break\n\n            # Add user message to conversation\n            messages.append({\"role\": \"user\", \"content\": user_input})\n\n            try:\n                # Get response from the model\n                response = client.chat(\n                    messages=messages,\n                    model=\"openai/gpt-4o-mini\",\n                    temperature=0.7\n                )\n\n                # Extract and print the assistant's response\n                assistant_response = response[\"data\"]\n                print(f\"Assistant: {assistant_response}\")\n\n                # Add assistant response to conversation history\n                messages.append({\"role\": \"assistant\", \"content\": assistant_response})\n\n            except Exception as e:\n                print(f\"Error: {str(e)}\")\n                messages.pop()\n\nif __name__ == \"__main__\":\n    simple_chatbot()\n</code></pre>"},{"location":"use-cases/chatbots/#multi-provider-chatbot","title":"Multi-Provider Chatbot","text":"<p>One of the key benefits of IndoxRouter is the ability to use multiple AI providers through a consistent interface. Here's an example of a chatbot that can switch between different providers:</p> <pre><code>from indoxrouter import Client\nimport argparse\n\ndef multi_provider_chatbot():\n    \"\"\"A chatbot that can use different AI providers.\"\"\"\n\n    # Parse command line arguments\n    parser = argparse.ArgumentParser(description=\"Multi-provider chatbot\")\n    parser.add_argument(\"--provider\", type=str, default=\"openai\", help=\"Provider to use (openai, anthropic, google, mistral)\")\n    parser.add_argument(\"--model\", type=str, help=\"Specific model to use\")\n    args = parser.parse_args()\n\n    # Set up provider and model\n    provider = args.provider.lower()\n\n    # Default models for each provider\n    provider_models = {\n        \"openai\": \"gpt-4o-mini\",\n        \"anthropic\": \"claude-3-haiku-20240307\",\n        \"google\": \"gemini-1.5-pro\",\n        \"mistral\": \"mistral-small-latest\",\n        \"deepseek\": \"deepseek-chat\"\n    }\n\n    if args.model:\n        model = args.model\n    elif provider in provider_models:\n        model = provider_models[provider]\n    else:\n        print(f\"Unknown provider: {provider}. Using OpenAI as default.\")\n        provider = \"openai\"\n        model = provider_models[provider]\n\n    full_model = f\"{provider}/{model}\"\n    print(f\"Welcome to IndoxRouter Multi-Provider Chatbot!\")\n    print(f\"Using model: {full_model}\")\n    print(\"Type 'exit' to end the conversation.\")\n    print(\"Type 'switch provider model' to change the AI model.\\n\")\n\n    # Initialize the client\n    with Client(api_key=\"your_api_key\") as client:\n        # Set up the conversation with a system message\n        messages = [\n            {\"role\": \"system\", \"content\": \"You are a helpful assistant that provides concise answers.\"}\n        ]\n\n        while True:\n            # Get user input\n            user_input = input(\"You: \")\n\n            # Check if user wants to exit\n            if user_input.lower() in (\"exit\", \"quit\", \"bye\"):\n                print(\"Assistant: Goodbye!\")\n                break\n\n            # Check if user wants to switch model\n            if user_input.lower().startswith(\"switch \"):\n                try:\n                    _, new_provider, new_model = user_input.split()\n                    full_model = f\"{new_provider}/{new_model}\"\n                    print(f\"Switching to {full_model}\")\n\n                    # Add system message explaining the switch\n                    messages.append({\"role\": \"system\", \"content\": f\"The conversation will now continue using {full_model}.\"})\n                    continue\n                except ValueError:\n                    print(\"Invalid format. Use 'switch provider model'\")\n                    continue\n\n            # Add user message to conversation\n            messages.append({\"role\": \"user\", \"content\": user_input})\n\n            try:\n                # Get response from the model\n                response = client.chat(\n                    messages=messages,\n                    model=full_model,\n                    temperature=0.7\n                )\n\n                # Extract and print the assistant's response\n                assistant_response = response[\"data\"]\n                print(f\"Assistant: {assistant_response}\")\n\n                # Add assistant response to conversation history\n                messages.append({\"role\": \"assistant\", \"content\": assistant_response})\n\n            except Exception as e:\n                print(f\"Error: {str(e)}\")\n                messages.pop()  # Remove the user message from history\n\nif __name__ == \"__main__\":\n    multi_provider_chatbot()\n</code></pre>"},{"location":"use-cases/chatbots/#web-based-chatbot-with-streamlit","title":"Web-Based Chatbot with Streamlit","text":"<p>You can also create a simple web-based chatbot using Streamlit:</p> <pre><code># Save as chatbot_app.py\nimport streamlit as st\nfrom indoxrouter import Client, ModelNotFoundError, ProviderError\n\n# Set page title and configure\nst.set_page_config(page_title=\"IndoxRouter Chatbot\", page_icon=\"\ud83d\udcac\")\nst.title(\"IndoxRouter Chatbot\")\n\n# Initialize session state for conversation history\nif \"messages\" not in st.session_state:\n    st.session_state.messages = [\n        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"}\n    ]\n\nif \"current_model\" not in st.session_state:\n    st.session_state.current_model = \"openai/gpt-4o-mini\"\n\n# API key input\napi_key = st.sidebar.text_input(\"API Key\", type=\"password\")\n\n# Model selection\nprovider_options = [\"openai\", \"anthropic\", \"google\", \"mistral\", \"deepseek\"]\nselected_provider = st.sidebar.selectbox(\"Provider\", provider_options)\n\nmodel_options = {\n    \"openai\": [\"gpt-4o-mini\", \"gpt-4o\", \"gpt-3.5-turbo\"],\n    \"anthropic\": [\"claude-3-haiku-20240307\", \"claude-3-sonnet-20240229\", \"claude-3-opus-20240229\"],\n    \"google\": [\"gemini-1.5-pro\", \"gemini-1.5-flash\"],\n    \"mistral\": [\"mistral-small-latest\", \"mistral-medium-latest\", \"mistral-large-latest\"],\n    \"deepseek\": [\"deepseek-chat\", \"deepseek-coder\"]\n}\n\nselected_model = st.sidebar.selectbox(\"Model\", model_options[selected_provider])\nst.session_state.current_model = f\"{selected_provider}/{selected_model}\"\n\n# Temperature slider\ntemperature = st.sidebar.slider(\"Temperature\", 0.0, 1.0, 0.7, 0.1)\n\n# Display conversation history\nfor message in st.session_state.messages:\n    if message[\"role\"] != \"system\":\n        with st.chat_message(message[\"role\"]):\n            st.write(message[\"content\"])\n\n# User input\nuser_input = st.chat_input(\"Type your message here...\")\n\nif user_input and api_key:\n    # Add user message to conversation\n    st.session_state.messages.append({\"role\": \"user\", \"content\": user_input})\n\n    # Display user message\n    with st.chat_message(\"user\"):\n        st.write(user_input)\n\n    # Get assistant response\n    with st.chat_message(\"assistant\"):\n        message_placeholder = st.empty()\n        full_response = \"\"\n\n        try:\n            # Initialize client\n            with Client(api_key=api_key) as client:\n                # Stream the response\n                for chunk in client.chat(\n                    messages=st.session_state.messages,\n                    model=st.session_state.current_model,\n                    temperature=temperature,\n                    stream=True\n                ):\n                    if isinstance(chunk, dict) and \"data\" in chunk:\n                        content = chunk[\"data\"]\n                        full_response += content\n                        message_placeholder.write(full_response + \"\u258c\")\n\n                message_placeholder.write(full_response)\n\n            # Add assistant response to conversation\n            st.session_state.messages.append({\"role\": \"assistant\", \"content\": full_response})\n\n        except ModelNotFoundError as e:\n            st.error(f\"Model not found: {e}\")\n        except ProviderError as e:\n            st.error(f\"Provider error: {e}\")\n        except Exception as e:\n            st.error(f\"An error occurred: {str(e)}\")\n\nelif user_input and not api_key:\n    st.warning(\"Please enter your API key in the sidebar.\")\n\n# Run with: streamlit run chatbot_app.py\n</code></pre> <p>To run the Streamlit app, install Streamlit and run:</p> <pre><code>pip install streamlit\nstreamlit run chatbot_app.py\n</code></pre>"},{"location":"use-cases/chatbots/#best-practices-for-chatbots","title":"Best Practices for Chatbots","text":"<ol> <li>Maintain Conversation Context: Keep track of conversation history to provide contextual responses</li> <li>Set Clear System Instructions: Use system messages to define the persona and behavior of your chatbot</li> <li>Handle Errors Gracefully: Implement proper error handling to ensure a smooth user experience</li> <li>Optimize Token Usage: Be mindful of the conversation length to avoid exceeding token limits</li> <li>Implement Rate Limiting: Add rate limiting to prevent abuse and manage costs</li> <li>Consider Privacy: Be transparent about data usage and implement appropriate data retention policies</li> <li>Test Different Models: Experiment with different models to find the best balance of quality and cost</li> <li>Implement Fallbacks: Have fallback mechanisms when a provider is unavailable or returns errors</li> </ol>"},{"location":"use-cases/chatbots/#next-steps","title":"Next Steps","text":"<p>To further enhance your chatbot, consider:</p> <ul> <li>Implementing memory management for long conversations</li> <li>Adding message persistence using a database</li> <li>Implementing functions/tools for more interactive capabilities</li> <li>Creating a feedback mechanism to improve chatbot responses</li> <li>Fine-tuning models for specific use cases</li> </ul>"},{"location":"use-cases/content-generation/","title":"Content Generation with IndoxRouter","text":"<p>IndoxRouter provides access to powerful language models that can be used for a wide range of content generation tasks. This guide demonstrates various content generation use cases and how to implement them.</p>"},{"location":"use-cases/content-generation/#text-generation-basics","title":"Text Generation Basics","text":"<p>At its core, content generation involves prompting a language model to produce text that meets specific requirements. Here's a simple example:</p> <pre><code>from indoxrouter import Client\n\nclient = Client(api_key=\"your_api_key\")\n\n# Generate a short blog post\nresponse = client.chat(\n    messages=[\n        {\"role\": \"system\", \"content\": \"You are a professional blog writer with expertise in technology.\"},\n        {\"role\": \"user\", \"content\": \"Write a 300-word blog post about the impact of AI on content creation.\"}\n    ],\n    model=\"openai/gpt-4o\"\n)\n\nblog_post = response[\"data\"]\nprint(blog_post)\n</code></pre>"},{"location":"use-cases/content-generation/#creative-writing","title":"Creative Writing","text":"<p>Language models can generate creative content like stories, poems, and scripts:</p> <pre><code># Generate a short story\nstory_response = client.chat(\n    messages=[\n        {\"role\": \"system\", \"content\": \"You are a creative fiction writer with a talent for engaging narratives.\"},\n        {\"role\": \"user\", \"content\": (\n            \"Write a short story (around 500 words) about a scientist who discovers \"\n            \"a way to communicate with plants. The story should have a surprising twist ending.\"\n        )}\n    ],\n    model=\"anthropic/claude-3-opus-20240229\",\n    temperature=0.8  # Higher temperature for more creativity\n)\n\nstory = story_response[\"data\"]\n</code></pre>"},{"location":"use-cases/content-generation/#seo-content-creation","title":"SEO Content Creation","text":"<p>Generate SEO-optimized content for websites and marketing:</p> <pre><code># Generate SEO-optimized article\nseo_response = client.chat(\n    messages=[\n        {\"role\": \"system\", \"content\": (\n            \"You are an SEO content expert who writes engaging, informative content \"\n            \"that ranks well in search engines. Include appropriate headings (H2, H3), \"\n            \"bullet points where relevant, and a conclusion.\"\n        )},\n        {\"role\": \"user\", \"content\": (\n            \"Write a comprehensive SEO article (800-1000 words) about 'Best Practices for Machine Learning Deployment' \"\n            \"targeting developers and data scientists. Include these keywords naturally: machine learning operations, \"\n            \"MLOps, model monitoring, deployment pipeline, and containerization.\"\n        )}\n    ],\n    model=\"openai/gpt-4o\",\n    temperature=0.7\n)\n\nseo_article = seo_response[\"data\"]\n</code></pre>"},{"location":"use-cases/content-generation/#product-descriptions","title":"Product Descriptions","text":"<p>Create compelling product descriptions for e-commerce:</p> <pre><code>def generate_product_description(product_name, features, target_audience, word_count=200):\n    \"\"\"Generate a product description based on product details.\"\"\"\n    features_text = \"\\n\".join([f\"- {feature}\" for feature in features])\n\n    prompt = f\"\"\"\n    Product: {product_name}\n    Features:\n    {features_text}\n    Target Audience: {target_audience}\n    Word Count: {word_count}\n\n    Write a compelling product description that highlights the features and benefits.\n    \"\"\"\n\n    response = client.chat(\n        messages=[\n            {\"role\": \"system\", \"content\": \"You are a marketing copywriter who creates compelling product descriptions.\"},\n            {\"role\": \"user\", \"content\": prompt}\n        ],\n        model=\"mistral/mistral-large-latest\",\n        temperature=0.7\n    )\n\n    return response[\"data\"]\n\n# Example usage\nproduct_description = generate_product_description(\n    product_name=\"EcoCharge Solar Power Bank\",\n    features=[\n        \"25,000mAh capacity\",\n        \"Solar charging capability\",\n        \"Fast-charging USB-C port\",\n        \"Built-in LED flashlight\",\n        \"Waterproof (IP67 rating)\"\n    ],\n    target_audience=\"Outdoor enthusiasts and travelers\",\n    word_count=150\n)\n</code></pre>"},{"location":"use-cases/content-generation/#email-marketing-campaigns","title":"Email Marketing Campaigns","text":"<p>Generate email marketing content with different styles:</p> <pre><code>def generate_email_campaign(campaign_type, product_info, audience, call_to_action):\n    \"\"\"Generate email marketing content based on campaign type.\"\"\"\n\n    campaign_prompts = {\n        \"welcome\": \"Write a friendly welcome email for new subscribers.\",\n        \"promotional\": \"Write a promotional email announcing a new product or special offer.\",\n        \"newsletter\": \"Write a newsletter email with updates and valuable content.\",\n        \"abandonment\": \"Write a cart abandonment email to remind customers of items left in their cart.\",\n        \"follow_up\": \"Write a follow-up email after a purchase to thank the customer and suggest next steps.\"\n    }\n\n    if campaign_type not in campaign_prompts:\n        raise ValueError(f\"Campaign type must be one of: {', '.join(campaign_prompts.keys())}\")\n\n    prompt = f\"\"\"\n    Campaign Type: {campaign_type} email\n    Product/Service Information: {product_info}\n    Target Audience: {audience}\n    Call to Action: {call_to_action}\n\n    {campaign_prompts[campaign_type]}\n    Include a subject line, greeting, body, and sign-off.\n    \"\"\"\n\n    response = client.chat(\n        messages=[\n            {\"role\": \"system\", \"content\": \"You are an email marketing specialist who writes compelling emails that convert.\"},\n            {\"role\": \"user\", \"content\": prompt}\n        ],\n        model=\"openai/gpt-4o-mini\",\n        temperature=0.7\n    )\n\n    return response[\"data\"]\n\n# Example usage\nwelcome_email = generate_email_campaign(\n    campaign_type=\"welcome\",\n    product_info=\"TechNest - A productivity app for managing tasks, notes, and projects\",\n    audience=\"New app subscribers, primarily professionals aged 25-45\",\n    call_to_action=\"Download the app and complete the onboarding tour\"\n)\n</code></pre>"},{"location":"use-cases/content-generation/#social-media-content","title":"Social Media Content","text":"<p>Generate content for different social media platforms:</p> <pre><code>def generate_social_media_post(platform, topic, tone, hashtags=None, include_emoji=True):\n    \"\"\"Generate a social media post tailored to a specific platform.\"\"\"\n\n    platform_guidelines = {\n        \"twitter\": \"Write a concise post under 280 characters.\",\n        \"instagram\": \"Write an engaging caption that works well with a visual. Include line breaks for readability.\",\n        \"linkedin\": \"Write a professional post that provides value to a business audience. Can be longer format.\",\n        \"facebook\": \"Write a conversational post that encourages engagement and interaction.\",\n        \"tiktok\": \"Write a catchy, trend-aware caption that would work well with a short video.\"\n    }\n\n    if platform not in platform_guidelines:\n        raise ValueError(f\"Platform must be one of: {', '.join(platform_guidelines.keys())}\")\n\n    hashtag_text = \"\"\n    if hashtags:\n        hashtag_text = f\"\\nSuggested hashtags: {', '.join(hashtags)}\"\n\n    emoji_instruction = \"Include appropriate emojis to increase engagement.\" if include_emoji else \"Don't use emojis.\"\n\n    prompt = f\"\"\"\n    Platform: {platform}\n    Topic: {topic}\n    Tone: {tone}\n    {hashtag_text}\n\n    {platform_guidelines[platform]}\n    {emoji_instruction}\n    \"\"\"\n\n    response = client.chat(\n        messages=[\n            {\"role\": \"system\", \"content\": \"You are a social media content creator who crafts engaging posts.\"},\n            {\"role\": \"user\", \"content\": prompt}\n        ],\n        model=\"anthropic/claude-3-haiku-20240307\",\n        temperature=0.7\n    )\n\n    return response[\"data\"]\n\n# Example usage\nlinkedin_post = generate_social_media_post(\n    platform=\"linkedin\",\n    topic=\"How AI is transforming data analysis in finance\",\n    tone=\"professional and informative\",\n    hashtags=[\"AIinFinance\", \"DataAnalytics\", \"FinTech\", \"MachineLearning\"],\n    include_emoji=True\n)\n</code></pre>"},{"location":"use-cases/content-generation/#content-repurposing","title":"Content Repurposing","text":"<p>Take existing content and repurpose it for different formats:</p> <pre><code>def repurpose_content(original_content, original_format, target_format, target_length=None):\n    \"\"\"Repurpose content from one format to another.\"\"\"\n\n    length_instruction = f\"The target length should be approximately {target_length} words.\" if target_length else \"\"\n\n    prompt = f\"\"\"\n    Original Content ({original_format}):\n\n    {original_content}\n\n    Please repurpose this content into a {target_format} format.\n    {length_instruction}\n    Maintain the key points and message while adapting to the new format's requirements.\n    \"\"\"\n\n    response = client.chat(\n        messages=[\n            {\"role\": \"system\", \"content\": \"You are a content repurposing specialist who can transform content between different formats while preserving the core message.\"},\n            {\"role\": \"user\", \"content\": prompt}\n        ],\n        model=\"openai/gpt-4o\",\n        temperature=0.7\n    )\n\n    return response[\"data\"]\n\n# Example usage\nblog_post = \"\"\"\n# The Future of Remote Work\nRemote work has transformed how businesses operate in the digital age. Since the COVID-19 pandemic, companies have discovered both challenges and benefits of distributed teams. Studies show productivity often increases, while office costs decrease. However, maintaining company culture and collaboration requires intentional strategies and tools. The future likely holds a hybrid model, combining in-person collaboration with remote flexibility.\n\"\"\"\n\n# Repurpose to different formats\ntwitter_thread = repurpose_content(blog_post, \"blog post\", \"Twitter thread (5-7 tweets)\")\nvideo_script = repurpose_content(blog_post, \"blog post\", \"video script\", 300)\nnewsletter = repurpose_content(blog_post, \"blog post\", \"email newsletter\", 250)\n</code></pre>"},{"location":"use-cases/content-generation/#data-driven-content","title":"Data-Driven Content","text":"<p>Generate content based on data and analysis:</p> <pre><code>def generate_data_report(data_summary, key_findings, audience, report_type=\"executive_summary\"):\n    \"\"\"Generate a data-driven report based on findings.\"\"\"\n\n    report_types = {\n        \"executive_summary\": \"Write a concise executive summary highlighting the most important insights.\",\n        \"detailed_analysis\": \"Write a detailed analysis explaining all findings and their implications.\",\n        \"recommendation\": \"Write recommendations based on the data findings.\",\n        \"press_release\": \"Write a press release announcing the key findings.\"\n    }\n\n    if report_type not in report_types:\n        raise ValueError(f\"Report type must be one of: {', '.join(report_types.keys())}\")\n\n    prompt = f\"\"\"\n    Data Summary: {data_summary}\n    Key Findings:\n    {key_findings}\n    Target Audience: {audience}\n\n    {report_types[report_type]}\n    Use a professional tone and focus on actionable insights.\n    \"\"\"\n\n    response = client.chat(\n        messages=[\n            {\"role\": \"system\", \"content\": \"You are a data analyst who creates clear, insightful reports from complex data.\"},\n            {\"role\": \"user\", \"content\": prompt}\n        ],\n        model=\"openai/gpt-4o\",\n        temperature=0.3  # Lower temperature for more factual content\n    )\n\n    return response[\"data\"]\n\n# Example usage\nexecutive_summary = generate_data_report(\n    data_summary=\"Survey of 1,000 consumers about shopping habits\",\n    key_findings=\"\"\"\n    - 67% of consumers prefer shopping online for electronics\n    - 82% read at least 3 reviews before making purchases over $100\n    - Mobile shopping increased by 34% compared to last year\n    - 45% of customers abandon carts due to high shipping costs\n    - Loyalty programs influence 58% of repeat purchases\n    \"\"\",\n    audience=\"E-commerce business executives\",\n    report_type=\"executive_summary\"\n)\n</code></pre>"},{"location":"use-cases/content-generation/#content-translation-and-localization","title":"Content Translation and Localization","text":"<p>Translate and adapt content for different regions:</p> <pre><code>def translate_and_localize(content, source_language, target_language, target_region=None, content_type=\"general\"):\n    \"\"\"Translate and localize content for a specific language and region.\"\"\"\n\n    region_instruction = f\"Adapt for {target_region} region specifically.\" if target_region else \"\"\n\n    content_type_instructions = {\n        \"general\": \"Translate the content while maintaining the original meaning.\",\n        \"marketing\": \"Translate and adapt marketing content to resonate with the target culture.\",\n        \"technical\": \"Translate technical content with precision, maintaining all technical details.\",\n        \"legal\": \"Translate legal content accurately, using appropriate legal terminology.\"\n    }\n\n    if content_type not in content_type_instructions:\n        raise ValueError(f\"Content type must be one of: {', '.join(content_type_instructions.keys())}\")\n\n    prompt = f\"\"\"\n    Original Content ({source_language}):\n\n    {content}\n\n    Please translate this content into {target_language}.\n    {region_instruction}\n\n    Content Type: {content_type}\n    {content_type_instructions[content_type]}\n\n    Note any cultural adaptations made in your translation.\n    \"\"\"\n\n    response = client.chat(\n        messages=[\n            {\"role\": \"system\", \"content\": f\"You are a professional translator fluent in {source_language} and {target_language} with expertise in cultural localization.\"},\n            {\"role\": \"user\", \"content\": prompt}\n        ],\n        model=\"openai/gpt-4o\",\n        temperature=0.4\n    )\n\n    return response[\"data\"]\n\n# Example usage\nenglish_content = \"\"\"\nOur Premium Membership gives you access to all courses, workshops, and resources.\nSign up today and get a 20% discount for the first three months!\n\"\"\"\n\nspanish_translation = translate_and_localize(\n    content=english_content,\n    source_language=\"English\",\n    target_language=\"Spanish\",\n    target_region=\"Mexico\",\n    content_type=\"marketing\"\n)\n</code></pre>"},{"location":"use-cases/content-generation/#best-practices-for-content-generation","title":"Best Practices for Content Generation","text":"<ol> <li>Iterative Refinement: Generate a draft, then refine it with additional prompts</li> <li>Specific Instructions: Provide clear, detailed instructions about tone, style, and format</li> <li>Model Selection: Choose the appropriate model based on the content complexity</li> <li>Temperature Control: Use higher temperature for creative content, lower for factual content</li> <li>Content Verification: Always review AI-generated content for accuracy and appropriateness</li> <li>A/B Testing: Generate multiple versions and test their effectiveness</li> <li>Human Touch: Add human editing to enhance and personalize the content</li> </ol>"},{"location":"use-cases/document-processing/","title":"Document Processing with IndoxRouter","text":"<p>IndoxRouter provides powerful capabilities for processing and analyzing documents using various language models. This guide covers common document processing tasks and patterns.</p>"},{"location":"use-cases/document-processing/#text-extraction-and-summarization","title":"Text Extraction and Summarization","text":""},{"location":"use-cases/document-processing/#document-summarization","title":"Document Summarization","text":"<pre><code>from indoxrouter import Client\n\nclient = Client(api_key=\"your_api_key\")\n\ndef summarize_document(text, max_length=200):\n    \"\"\"Summarize a long document into key points.\"\"\"\n\n    response = client.chat(\n        messages=[\n            {\n                \"role\": \"system\",\n                \"content\": f\"Summarize the following document in {max_length} words or less. Focus on the main points and key information.\"\n            },\n            {\"role\": \"user\", \"content\": text}\n        ],\n        model=\"openai/gpt-4o-mini\",\n        max_tokens=max_length * 2  # Rough estimate for token count\n    )\n\n    return response[\"data\"]\n\n# Example usage\ndocument = \"\"\"\nYour long document text here...\nThis could be a research paper, article, report, etc.\n\"\"\"\n\nsummary = summarize_document(document)\nprint(\"Summary:\", summary)\n</code></pre>"},{"location":"use-cases/document-processing/#extractive-vs-abstractive-summarization","title":"Extractive vs Abstractive Summarization","text":"<pre><code>def extractive_summary(text, num_sentences=3):\n    \"\"\"Extract key sentences from the document.\"\"\"\n\n    response = client.chat(\n        messages=[\n            {\n                \"role\": \"system\",\n                \"content\": f\"Extract the {num_sentences} most important sentences from the following text. Return only the sentences, separated by newlines.\"\n            },\n            {\"role\": \"user\", \"content\": text}\n        ],\n        model=\"openai/gpt-4o-mini\"\n    )\n\n    return response[\"data\"]\n\ndef abstractive_summary(text, style=\"professional\"):\n    \"\"\"Generate a new summary in the specified style.\"\"\"\n\n    response = client.chat(\n        messages=[\n            {\n                \"role\": \"system\",\n                \"content\": f\"Create a {style} summary of the following document. Write it in your own words, capturing the essence and main ideas.\"\n            },\n            {\"role\": \"user\", \"content\": text}\n        ],\n        model=\"openai/gpt-4o-mini\"\n    )\n\n    return response[\"data\"]\n</code></pre>"},{"location":"use-cases/document-processing/#document-classification","title":"Document Classification","text":""},{"location":"use-cases/document-processing/#topic-classification","title":"Topic Classification","text":"<pre><code>def classify_document(text, categories):\n    \"\"\"Classify a document into predefined categories.\"\"\"\n\n    categories_str = \", \".join(categories)\n\n    response = client.chat(\n        messages=[\n            {\n                \"role\": \"system\",\n                \"content\": f\"Classify the following document into one of these categories: {categories_str}. Respond with only the category name and a confidence score (0-1).\"\n            },\n            {\"role\": \"user\", \"content\": text}\n        ],\n        model=\"openai/gpt-4o-mini\"\n    )\n\n    return response[\"data\"]\n\n# Example usage\ncategories = [\"Technology\", \"Finance\", \"Healthcare\", \"Education\", \"Sports\"]\nclassification = classify_document(document, categories)\nprint(\"Classification:\", classification)\n</code></pre>"},{"location":"use-cases/document-processing/#sentiment-analysis","title":"Sentiment Analysis","text":"<pre><code>def analyze_sentiment(text):\n    \"\"\"Analyze the sentiment of a document.\"\"\"\n\n    response = client.chat(\n        messages=[\n            {\n                \"role\": \"system\",\n                \"content\": \"Analyze the sentiment of the following text. Provide: 1) Overall sentiment (positive/negative/neutral), 2) Confidence score (0-1), 3) Key phrases that indicate the sentiment.\"\n            },\n            {\"role\": \"user\", \"content\": text}\n        ],\n        model=\"openai/gpt-4o-mini\"\n    )\n\n    return response[\"data\"]\n</code></pre>"},{"location":"use-cases/document-processing/#information-extraction","title":"Information Extraction","text":""},{"location":"use-cases/document-processing/#named-entity-recognition","title":"Named Entity Recognition","text":"<pre><code>def extract_entities(text, entity_types=None):\n    \"\"\"Extract named entities from text.\"\"\"\n\n    if entity_types:\n        entity_prompt = f\"Focus on these entity types: {', '.join(entity_types)}\"\n    else:\n        entity_prompt = \"Extract all relevant entities\"\n\n    response = client.chat(\n        messages=[\n            {\n                \"role\": \"system\",\n                \"content\": f\"Extract named entities from the following text. {entity_prompt}. Format as JSON with entity type and value.\"\n            },\n            {\"role\": \"user\", \"content\": text}\n        ],\n        model=\"openai/gpt-4o-mini\"\n    )\n\n    return response[\"data\"]\n\n# Example usage\nentities = extract_entities(\n    \"Apple Inc. was founded by Steve Jobs in Cupertino, California in 1976.\",\n    entity_types=[\"PERSON\", \"ORGANIZATION\", \"LOCATION\", \"DATE\"]\n)\nprint(\"Entities:\", entities)\n</code></pre>"},{"location":"use-cases/document-processing/#key-information-extraction","title":"Key Information Extraction","text":"<pre><code>def extract_key_info(text, fields):\n    \"\"\"Extract specific fields from a document.\"\"\"\n\n    fields_str = \", \".join(fields)\n\n    response = client.chat(\n        messages=[\n            {\n                \"role\": \"system\",\n                \"content\": f\"Extract the following information from the document: {fields_str}. Format as JSON. If a field is not found, mark it as null.\"\n            },\n            {\"role\": \"user\", \"content\": text}\n        ],\n        model=\"openai/gpt-4o-mini\"\n    )\n\n    return response[\"data\"]\n\n# Example for invoice processing\ninvoice_fields = [\"invoice_number\", \"date\", \"total_amount\", \"vendor_name\", \"items\"]\nextracted_info = extract_key_info(invoice_text, invoice_fields)\n</code></pre>"},{"location":"use-cases/document-processing/#document-comparison-and-analysis","title":"Document Comparison and Analysis","text":""},{"location":"use-cases/document-processing/#document-similarity","title":"Document Similarity","text":"<pre><code>def compare_documents(doc1, doc2):\n    \"\"\"Compare two documents for similarity and differences.\"\"\"\n\n    response = client.chat(\n        messages=[\n            {\n                \"role\": \"system\",\n                \"content\": \"Compare these two documents. Provide: 1) Similarity score (0-1), 2) Main similarities, 3) Key differences, 4) Summary of comparison.\"\n            },\n            {\n                \"role\": \"user\",\n                \"content\": f\"Document 1:\\n{doc1}\\n\\nDocument 2:\\n{doc2}\"\n            }\n        ],\n        model=\"openai/gpt-4o-mini\"\n    )\n\n    return response[\"data\"]\n</code></pre>"},{"location":"use-cases/document-processing/#change-detection","title":"Change Detection","text":"<pre><code>def detect_changes(original_doc, revised_doc):\n    \"\"\"Detect changes between document versions.\"\"\"\n\n    response = client.chat(\n        messages=[\n            {\n                \"role\": \"system\",\n                \"content\": \"Identify all changes between the original and revised documents. List additions, deletions, and modifications clearly.\"\n            },\n            {\n                \"role\": \"user\",\n                \"content\": f\"Original:\\n{original_doc}\\n\\nRevised:\\n{revised_doc}\"\n            }\n        ],\n        model=\"openai/gpt-4o-mini\"\n    )\n\n    return response[\"data\"]\n</code></pre>"},{"location":"use-cases/document-processing/#batch-document-processing","title":"Batch Document Processing","text":""},{"location":"use-cases/document-processing/#processing-multiple-documents","title":"Processing Multiple Documents","text":"<pre><code>import asyncio\nfrom indoxrouter import AsyncClient\n\nasync def process_documents_batch(documents, processing_function):\n    \"\"\"Process multiple documents concurrently.\"\"\"\n\n    client = AsyncClient(api_key=\"your_api_key\")\n\n    tasks = []\n    for doc in documents:\n        task = processing_function(client, doc)\n        tasks.append(task)\n\n    results = await asyncio.gather(*tasks)\n    return results\n\nasync def summarize_async(client, text):\n    \"\"\"Async version of document summarization.\"\"\"\n\n    response = await client.chat(\n        messages=[\n            {\n                \"role\": \"system\",\n                \"content\": \"Summarize this document in 100 words or less.\"\n            },\n            {\"role\": \"user\", \"content\": text}\n        ],\n        model=\"openai/gpt-4o-mini\"\n    )\n\n    return response[\"data\"]\n\n# Usage\ndocuments = [\"doc1 text...\", \"doc2 text...\", \"doc3 text...\"]\nsummaries = asyncio.run(process_documents_batch(documents, summarize_async))\n</code></pre>"},{"location":"use-cases/document-processing/#document-quality-assessment","title":"Document Quality Assessment","text":""},{"location":"use-cases/document-processing/#readability-analysis","title":"Readability Analysis","text":"<pre><code>def assess_readability(text):\n    \"\"\"Assess document readability and provide improvement suggestions.\"\"\"\n\n    response = client.chat(\n        messages=[\n            {\n                \"role\": \"system\",\n                \"content\": \"Analyze the readability of this text. Provide: 1) Reading level, 2) Clarity score (1-10), 3) Specific suggestions for improvement, 4) Complex sentences that could be simplified.\"\n            },\n            {\"role\": \"user\", \"content\": text}\n        ],\n        model=\"openai/gpt-4o-mini\"\n    )\n\n    return response[\"data\"]\n</code></pre>"},{"location":"use-cases/document-processing/#content-quality-check","title":"Content Quality Check","text":"<pre><code>def check_content_quality(text, criteria=None):\n    \"\"\"Check document quality against specific criteria.\"\"\"\n\n    if criteria:\n        criteria_str = f\"Focus on these criteria: {', '.join(criteria)}\"\n    else:\n        criteria_str = \"Use general quality standards\"\n\n    response = client.chat(\n        messages=[\n            {\n                \"role\": \"system\",\n                \"content\": f\"Evaluate the quality of this document. {criteria_str}. Provide scores and specific feedback.\"\n            },\n            {\"role\": \"user\", \"content\": text}\n        ],\n        model=\"openai/gpt-4o-mini\"\n    )\n\n    return response[\"data\"]\n\n# Example usage\nquality_criteria = [\"accuracy\", \"completeness\", \"clarity\", \"organization\"]\nquality_report = check_content_quality(document, quality_criteria)\n</code></pre>"},{"location":"use-cases/document-processing/#specialized-document-types","title":"Specialized Document Types","text":""},{"location":"use-cases/document-processing/#legal-document-processing","title":"Legal Document Processing","text":"<pre><code>def process_legal_document(text, task=\"summarize\"):\n    \"\"\"Process legal documents with domain-specific understanding.\"\"\"\n\n    tasks = {\n        \"summarize\": \"Summarize this legal document, highlighting key legal points, obligations, and important dates.\",\n        \"extract_clauses\": \"Extract and list all important clauses, terms, and conditions from this legal document.\",\n        \"risk_analysis\": \"Identify potential legal risks, ambiguities, or concerning clauses in this document.\"\n    }\n\n    prompt = tasks.get(task, tasks[\"summarize\"])\n\n    response = client.chat(\n        messages=[\n            {\n                \"role\": \"system\",\n                \"content\": f\"{prompt} Use legal terminology appropriately and be precise.\"\n            },\n            {\"role\": \"user\", \"content\": text}\n        ],\n        model=\"openai/gpt-4o\"  # Use more capable model for legal analysis\n    )\n\n    return response[\"data\"]\n</code></pre>"},{"location":"use-cases/document-processing/#scientific-paper-processing","title":"Scientific Paper Processing","text":"<pre><code>def process_research_paper(text, section=\"abstract\"):\n    \"\"\"Process scientific papers with academic focus.\"\"\"\n\n    sections = {\n        \"abstract\": \"Extract and summarize the abstract, highlighting research objectives, methods, and key findings.\",\n        \"methodology\": \"Analyze and summarize the research methodology, including data collection and analysis methods.\",\n        \"findings\": \"Extract and summarize the key findings, results, and their significance.\",\n        \"citations\": \"Extract all citations and references mentioned in this paper.\"\n    }\n\n    prompt = sections.get(section, sections[\"abstract\"])\n\n    response = client.chat(\n        messages=[\n            {\n                \"role\": \"system\",\n                \"content\": f\"{prompt} Maintain scientific accuracy and use appropriate academic language.\"\n            },\n            {\"role\": \"user\", \"content\": text}\n        ],\n        model=\"openai/gpt-4o\"\n    )\n\n    return response[\"data\"]\n</code></pre>"},{"location":"use-cases/document-processing/#document-generation-and-enhancement","title":"Document Generation and Enhancement","text":""},{"location":"use-cases/document-processing/#document-enhancement","title":"Document Enhancement","text":"<pre><code>def enhance_document(text, enhancement_type=\"improve_clarity\"):\n    \"\"\"Enhance document quality and readability.\"\"\"\n\n    enhancements = {\n        \"improve_clarity\": \"Rewrite this text to improve clarity and readability while maintaining all original information.\",\n        \"professional_tone\": \"Rewrite this text in a more professional and formal tone.\",\n        \"simplify\": \"Simplify this text for a general audience while keeping all important information.\",\n        \"expand\": \"Expand this text with additional relevant details and explanations.\"\n    }\n\n    prompt = enhancements.get(enhancement_type, enhancements[\"improve_clarity\"])\n\n    response = client.chat(\n        messages=[\n            {\n                \"role\": \"system\",\n                \"content\": prompt\n            },\n            {\"role\": \"user\", \"content\": text}\n        ],\n        model=\"openai/gpt-4o-mini\"\n    )\n\n    return response[\"data\"]\n</code></pre>"},{"location":"use-cases/document-processing/#template-based-generation","title":"Template-Based Generation","text":"<pre><code>def generate_from_template(template, data):\n    \"\"\"Generate documents from templates and data.\"\"\"\n\n    response = client.chat(\n        messages=[\n            {\n                \"role\": \"system\",\n                \"content\": \"Fill in the following template with the provided data. Maintain the template structure and format.\"\n            },\n            {\n                \"role\": \"user\",\n                \"content\": f\"Template:\\n{template}\\n\\nData:\\n{data}\"\n            }\n        ],\n        model=\"openai/gpt-4o-mini\"\n    )\n\n    return response[\"data\"]\n\n# Example usage\nemail_template = \"\"\"\nSubject: {subject}\n\nDear {recipient_name},\n\n{opening_paragraph}\n\n{main_content}\n\n{closing_paragraph}\n\nBest regards,\n{sender_name}\n\"\"\"\n\nemail_data = {\n    \"subject\": \"Project Update\",\n    \"recipient_name\": \"John Doe\",\n    \"opening_paragraph\": \"I hope this email finds you well.\",\n    \"main_content\": \"I wanted to provide you with an update on our current project status...\",\n    \"closing_paragraph\": \"Please let me know if you have any questions.\",\n    \"sender_name\": \"Jane Smith\"\n}\n\ngenerated_email = generate_from_template(email_template, str(email_data))\n</code></pre> <p>This comprehensive guide covers the main document processing capabilities available through IndoxRouter, enabling you to build sophisticated document analysis and processing systems.</p>"},{"location":"use-cases/rag-systems/","title":"Building RAG Systems with IndoxRouter","text":"<p>Retrieval-Augmented Generation (RAG) is a powerful approach that combines the benefits of retrieving relevant information from a knowledge base with the capabilities of large language models. This guide demonstrates how to build effective RAG systems using IndoxRouter.</p>"},{"location":"use-cases/rag-systems/#what-is-rag","title":"What is RAG?","text":"<p>RAG enhances language model responses by:</p> <ol> <li>Breaking down documents into smaller chunks</li> <li>Creating vector embeddings for each chunk</li> <li>Storing these embeddings in a vector database</li> <li>When a query is received, finding the most relevant chunks</li> <li>Using these chunks as context for the language model to generate an answer</li> </ol> <p>This approach helps ground model responses in specific knowledge and reduces hallucinations.</p>"},{"location":"use-cases/rag-systems/#basic-rag-implementation","title":"Basic RAG Implementation","text":"<p>Here's a simple implementation of a RAG system using IndoxRouter:</p> <pre><code>import numpy as np\nfrom sklearn.metrics.pairwise import cosine_similarity\nfrom indoxrouter import Client\n\n# Initialize client\nclient = Client(api_key=\"your_api_key\")\n\n# Sample documents - in a real application, you would load these from files\ndocuments = [\n    \"The Python programming language was created by Guido van Rossum and first released in 1991.\",\n    \"Python is known for its readability and simplicity, making it an excellent language for beginners.\",\n    \"Python supports multiple programming paradigms, including procedural, object-oriented, and functional programming.\",\n    \"The name Python comes from Monty Python, not the snake.\",\n    \"Popular Python frameworks include Django and Flask for web development, and NumPy and Pandas for data analysis.\"\n]\n\n# Step 1: Generate embeddings for the documents\ndoc_response = client.embeddings(\n    text=documents,\n    model=\"openai/text-embedding-3-small\"\n)\n\n# Convert to numpy array for easier processing\ndoc_embeddings = np.array([item[\"embedding\"] for item in doc_response[\"data\"]])\n\n# Step 2: Process a user query\nquery = \"Why is Python good for beginners?\"\n\n# Generate embedding for the query\nquery_response = client.embeddings(\n    text=query,\n    model=\"openai/text-embedding-3-small\"\n)\nquery_embedding = np.array(query_response[\"data\"][0][\"embedding\"])\n\n# Step 3: Find the most relevant documents\nsimilarities = cosine_similarity([query_embedding], doc_embeddings)[0]\n\n# Get the top 2 most relevant documents\ntop_indices = np.argsort(similarities)[-2:][::-1]\nrelevant_docs = [documents[i] for i in top_indices]\ncontext = \"\\n\".join(relevant_docs)\n\n# Step 4: Generate an answer using the relevant documents as context\nresponse = client.chat(\n    messages=[\n        {\"role\": \"system\", \"content\": \"You are a helpful assistant. Answer the question based on the provided context only.\"},\n        {\"role\": \"user\", \"content\": f\"Context:\\n{context}\\n\\nQuestion: {query}\"}\n    ],\n    model=\"openai/gpt-4o-mini\"\n)\n\n# Print the answer\nprint(f\"Question: {query}\")\nprint(f\"Answer: {response['data']\")\n</code></pre>"},{"location":"use-cases/rag-systems/#advanced-rag-implementation","title":"Advanced RAG Implementation","text":"<p>For real-world applications, you'll need a more sophisticated approach:</p> <pre><code>import os\nimport numpy as np\nfrom sklearn.metrics.pairwise import cosine_similarity\nfrom indoxrouter import Client\n\nclass RAGSystem:\n    def __init__(self, api_key, embed_model=\"openai/text-embedding-3-small\", llm_model=\"openai/gpt-4o-mini\"):\n        \"\"\"Initialize the RAG system with API key and models.\"\"\"\n        self.client = Client(api_key=api_key)\n        self.embed_model = embed_model\n        self.llm_model = llm_model\n        self.document_store = []\n        self.embeddings = []\n\n    def chunk_text(self, text, chunk_size=500, overlap=50):\n        \"\"\"Split text into overlapping chunks.\"\"\"\n        chunks = []\n        for i in range(0, len(text), chunk_size - overlap):\n            chunk = text[i:i + chunk_size]\n            if len(chunk) &lt; 100:  # Skip very small chunks\n                continue\n            chunks.append(chunk)\n        return chunks\n\n    def add_document(self, doc_id, text, metadata=None):\n        \"\"\"Process and add a document to the RAG system.\"\"\"\n        # Split document into chunks\n        chunks = self.chunk_text(text)\n\n        # Create embeddings for each chunk\n        response = self.client.embeddings(\n            text=chunks,\n            model=self.embed_model\n        )\n\n        # Store chunks and their embeddings\n        for i, chunk in enumerate(chunks):\n            chunk_id = f\"{doc_id}_chunk_{i}\"\n            embedding = response[\"data\"][i][\"embedding\"]\n            chunk_metadata = metadata.copy() if metadata else {}\n            chunk_metadata.update({\n                \"doc_id\": doc_id,\n                \"chunk_id\": chunk_id,\n                \"chunk_index\": i,\n                \"total_chunks\": len(chunks)\n            })\n\n            self.document_store.append({\n                \"id\": chunk_id,\n                \"text\": chunk,\n                \"metadata\": chunk_metadata\n            })\n            self.embeddings.append(embedding)\n\n        print(f\"Added document {doc_id} with {len(chunks)} chunks\")\n\n    def query(self, question, top_k=3):\n        \"\"\"Process a query and return the answer with supporting evidence.\"\"\"\n        # Generate embedding for the query\n        query_response = self.client.embeddings(\n            text=question,\n            model=self.embed_model\n        )\n        query_embedding = np.array(query_response[\"data\"][0][\"embedding\"])\n\n        # Calculate similarities with all document chunks\n        doc_embeddings = np.array(self.embeddings)\n        similarities = cosine_similarity([query_embedding], doc_embeddings)[0]\n\n        # Get top K most relevant chunks\n        top_indices = np.argsort(similarities)[-top_k:][::-1]\n        relevant_chunks = [self.document_store[i] for i in top_indices]\n\n        # Build context from relevant chunks\n        context_parts = []\n        for i, chunk in enumerate(relevant_chunks):\n            context_parts.append(f\"[Document {i+1}] {chunk['text']}\")\n\n        context = \"\\n\\n\".join(context_parts)\n\n        # Generate answer using context\n        system_message = (\n            \"You are a helpful assistant. Answer the user's question based ONLY on the provided context. \"\n            \"If the answer cannot be determined from the context, say 'I don't have enough information to answer that question.'\"\n        )\n\n        response = self.client.chat(\n            messages=[\n                {\"role\": \"system\", \"content\": system_message},\n                {\"role\": \"user\", \"content\": f\"Context:\\n\\n{context}\\n\\nQuestion: {question}\"}\n            ],\n            model=self.llm_model\n        )\n\n        answer = response[\"data\"]\n\n        # Return answer along with supporting evidence\n        return {\n            \"question\": question,\n            \"answer\": answer,\n            \"sources\": relevant_chunks,\n            \"similarities\": [similarities[i] for i in top_indices]\n        }\n\n# Usage example\nrag = RAGSystem(api_key=\"your_api_key\")\n\n# Add some documents\nrag.add_document(\"python_basics\", \"\"\"\nPython is a high-level, interpreted programming language with dynamic semantics.\nIts high-level built in data structures, combined with dynamic typing and dynamic binding,\nmake it very attractive for Rapid Application Development, as well as for use as a\nscripting or glue language to connect existing components together.\n\"\"\")\n\nrag.add_document(\"python_features\", \"\"\"\nPython's simple, easy to learn syntax emphasizes readability and therefore reduces\nthe cost of program maintenance. Python supports modules and packages, which encourages\nprogram modularity and code reuse. The Python interpreter and the extensive standard\nlibrary are available in source or binary form without charge for all major platforms.\n\"\"\")\n\n# Query the system\nresult = rag.query(\"What makes Python good for development?\")\nprint(f\"Question: {result['question']}\")\nprint(f\"Answer: {result['answer']}\")\nprint(\"\\nSources:\")\nfor i, source in enumerate(result['sources']):\n    print(f\"{i+1}. {source['metadata']['doc_id']} (similarity: {result['similarities'][i]:.3f})\")\n</code></pre>"},{"location":"use-cases/rag-systems/#best-practices","title":"Best Practices","text":""},{"location":"use-cases/rag-systems/#1-document-preprocessing","title":"1. Document Preprocessing","text":"<ul> <li>Clean text: Remove unnecessary formatting, headers, footers</li> <li>Normalize text: Handle different encodings, special characters</li> <li>Structure preservation: Maintain important formatting like lists, tables</li> </ul>"},{"location":"use-cases/rag-systems/#2-chunking-strategies","title":"2. Chunking Strategies","text":"<ul> <li>Fixed-size chunking: Simple but may break context</li> <li>Sentence-based chunking: Preserves semantic boundaries</li> <li>Paragraph-based chunking: Good for structured documents</li> <li>Overlapping chunks: Helps maintain context across boundaries</li> </ul>"},{"location":"use-cases/rag-systems/#3-embedding-optimization","title":"3. Embedding Optimization","text":"<ul> <li>Choose appropriate models: Balance quality vs speed</li> <li>Batch processing: Process multiple texts together for efficiency</li> <li>Caching: Store embeddings to avoid recomputation</li> </ul>"},{"location":"use-cases/rag-systems/#4-retrieval-tuning","title":"4. Retrieval Tuning","text":"<ul> <li>Adjust top_k: Find the right balance of context vs noise</li> <li>Similarity thresholds: Filter out irrelevant results</li> <li>Hybrid search: Combine semantic and keyword search</li> </ul>"},{"location":"use-cases/rag-systems/#5-response-generation","title":"5. Response Generation","text":"<ul> <li>Clear instructions: Tell the model how to use the context</li> <li>Context formatting: Structure the retrieved information clearly</li> <li>Fallback handling: Handle cases where no relevant context is found</li> </ul>"},{"location":"use-cases/rag-systems/#integration-with-vector-databases","title":"Integration with Vector Databases","text":"<p>For production systems, consider using dedicated vector databases:</p> <pre><code># Example with Pinecone\nimport pinecone\nfrom indoxrouter import Client\n\nclass ProductionRAG:\n    def __init__(self, api_key, pinecone_api_key, pinecone_env):\n        self.client = Client(api_key=api_key)\n\n        # Initialize Pinecone\n        pinecone.init(api_key=pinecone_api_key, environment=pinecone_env)\n        self.index = pinecone.Index(\"rag-index\")\n\n    def add_document(self, doc_id, text):\n        # Generate embeddings\n        response = self.client.embeddings(\n            text=text,\n            model=\"openai/text-embedding-3-small\"\n        )\n\n        # Store in Pinecone\n        self.index.upsert([(\n            doc_id,\n            response[\"data\"][0][\"embedding\"],\n            {\"text\": text}\n        )])\n\n    def query(self, question, top_k=3):\n        # Generate query embedding\n        query_response = self.client.embeddings(\n            text=question,\n            model=\"openai/text-embedding-3-small\"\n        )\n\n        # Search Pinecone\n        results = self.index.query(\n            vector=query_response[\"data\"][0][\"embedding\"],\n            top_k=top_k,\n            include_metadata=True\n        )\n\n        # Build context\n        context = \"\\n\\n\".join([match.metadata[\"text\"] for match in results.matches])\n\n        # Generate answer\n        response = self.client.chat(\n            messages=[\n                {\"role\": \"system\", \"content\": \"Answer based on the provided context.\"},\n                {\"role\": \"user\", \"content\": f\"Context:\\n{context}\\n\\nQuestion: {question}\"}\n            ],\n            model=\"openai/gpt-4o-mini\"\n        )\n\n        return response[\"data\"]\n</code></pre> <p>This approach provides scalable, production-ready RAG systems with IndoxRouter.</p>"}]}